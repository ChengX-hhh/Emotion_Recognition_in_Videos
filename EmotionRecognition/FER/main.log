nohup: ignoring input
FineTuneModel(
  (resnet34): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer2): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer3): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (4): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (5): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (layer4): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (avgpool): AvgPool2d(kernel_size=4, stride=1, padding=0)
    (fc): Linear(in_features=512, out_features=300, bias=True)
  )
  (fc): Linear(in_features=300, out_features=30, bias=True)
  (fc2): Linear(in_features=30, out_features=7, bias=True)
)
dataset init finish
dataset init finish
dataset init finish
(tensor([[[0.0549, 0.0549, 0.0549,  ..., 0.0196, 0.0196, 0.0196],
         [0.0549, 0.0549, 0.0549,  ..., 0.0196, 0.0196, 0.0196],
         [0.0549, 0.0549, 0.0549,  ..., 0.0196, 0.0196, 0.0196],
         ...,
         [0.3647, 0.3686, 0.3804,  ..., 0.4941, 0.4863, 0.4824],
         [0.3529, 0.3569, 0.3686,  ..., 0.4980, 0.4863, 0.4824],
         [0.3529, 0.3569, 0.3686,  ..., 0.4980, 0.4863, 0.4824]],

        [[0.0549, 0.0549, 0.0549,  ..., 0.0196, 0.0196, 0.0196],
         [0.0549, 0.0549, 0.0549,  ..., 0.0196, 0.0196, 0.0196],
         [0.0549, 0.0549, 0.0549,  ..., 0.0196, 0.0196, 0.0196],
         ...,
         [0.3647, 0.3686, 0.3804,  ..., 0.4941, 0.4863, 0.4824],
         [0.3529, 0.3569, 0.3686,  ..., 0.4980, 0.4863, 0.4824],
         [0.3529, 0.3569, 0.3686,  ..., 0.4980, 0.4863, 0.4824]],

        [[0.0549, 0.0549, 0.0549,  ..., 0.0196, 0.0196, 0.0196],
         [0.0549, 0.0549, 0.0549,  ..., 0.0196, 0.0196, 0.0196],
         [0.0549, 0.0549, 0.0549,  ..., 0.0196, 0.0196, 0.0196],
         ...,
         [0.3647, 0.3686, 0.3804,  ..., 0.4941, 0.4863, 0.4824],
         [0.3529, 0.3569, 0.3686,  ..., 0.4980, 0.4863, 0.4824],
         [0.3529, 0.3569, 0.3686,  ..., 0.4980, 0.4863, 0.4824]]]), 1)
train_dataloader
val_dataloader
test_dataloader
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:04<03:49,  4.16s/it]  4%|▎         | 2/56.072265625 [00:04<01:43,  1.91s/it]  5%|▌         | 3/56.072265625 [00:04<01:01,  1.15s/it]  7%|▋         | 4/56.072265625 [00:04<00:41,  1.27it/s]  9%|▉         | 5/56.072265625 [00:05<00:30,  1.67it/s] 11%|█         | 6/56.072265625 [00:05<00:23,  2.11it/s] 12%|█▏        | 7/56.072265625 [00:05<00:19,  2.49it/s] 14%|█▍        | 8/56.072265625 [00:05<00:16,  2.88it/s] 16%|█▌        | 9/56.072265625 [00:06<00:15,  3.11it/s] 18%|█▊        | 10/56.072265625 [00:06<00:13,  3.42it/s] 20%|█▉        | 11/56.072265625 [00:06<00:12,  3.54it/s] 21%|██▏       | 12/56.072265625 [00:06<00:11,  3.76it/s] 23%|██▎       | 13/56.072265625 [00:07<00:11,  3.64it/s] 25%|██▍       | 14/56.072265625 [00:07<00:10,  3.84it/s] 27%|██▋       | 15/56.072265625 [00:07<00:10,  3.85it/s] 29%|██▊       | 16/56.072265625 [00:07<00:10,  3.99it/s] 30%|███       | 17/56.072265625 [00:08<00:09,  3.94it/s] 32%|███▏      | 18/56.072265625 [00:08<00:09,  4.06it/s] 34%|███▍      | 19/56.072265625 [00:08<00:09,  4.01it/s] 36%|███▌      | 20/56.072265625 [00:08<00:08,  4.12it/s] 37%|███▋      | 21/56.072265625 [00:09<00:08,  4.05it/s] 39%|███▉      | 22/56.072265625 [00:09<00:08,  4.14it/s] 41%|████      | 23/56.072265625 [00:09<00:08,  4.06it/s] 43%|████▎     | 24/56.072265625 [00:09<00:07,  4.14it/s] 45%|████▍     | 25/56.072265625 [00:10<00:07,  4.04it/s] 46%|████▋     | 26/56.072265625 [00:10<00:07,  4.14it/s] 48%|████▊     | 27/56.072265625 [00:10<00:07,  4.05it/s] 50%|████▉     | 28/56.072265625 [00:10<00:06,  4.14it/s] 52%|█████▏    | 29/56.072265625 [00:11<00:06,  4.04it/s] 54%|█████▎    | 30/56.072265625 [00:11<00:06,  4.14it/s] 55%|█████▌    | 31/56.072265625 [00:11<00:06,  4.04it/s] 57%|█████▋    | 32/56.072265625 [00:11<00:05,  4.14it/s] 59%|█████▉    | 33/56.072265625 [00:12<00:05,  4.04it/s] 61%|██████    | 34/56.072265625 [00:12<00:05,  4.13it/s] 62%|██████▏   | 35/56.072265625 [00:12<00:05,  4.04it/s] 64%|██████▍   | 36/56.072265625 [00:12<00:04,  4.13it/s] 66%|██████▌   | 37/56.072265625 [00:13<00:04,  4.03it/s] 68%|██████▊   | 38/56.072265625 [00:13<00:04,  4.13it/s] 70%|██████▉   | 39/56.072265625 [00:13<00:04,  4.05it/s] 71%|███████▏  | 40/56.072265625 [00:13<00:03,  4.14it/s] 73%|███████▎  | 41/56.072265625 [00:14<00:03,  4.04it/s] 75%|███████▍  | 42/56.072265625 [00:14<00:03,  4.13it/s] 77%|███████▋  | 43/56.072265625 [00:14<00:03,  4.03it/s] 78%|███████▊  | 44/56.072265625 [00:14<00:02,  4.12it/s] 80%|████████  | 45/56.072265625 [00:15<00:02,  4.02it/s] 82%|████████▏ | 46/56.072265625 [00:15<00:02,  4.12it/s] 84%|████████▍ | 47/56.072265625 [00:15<00:02,  4.04it/s] 86%|████████▌ | 48/56.072265625 [00:15<00:01,  4.13it/s] 87%|████████▋ | 49/56.072265625 [00:16<00:01,  4.03it/s] 89%|████████▉ | 50/56.072265625 [00:16<00:01,  4.12it/s] 91%|█████████ | 51/56.072265625 [00:16<00:01,  4.06it/s] 93%|█████████▎| 52/56.072265625 [00:16<00:00,  4.15it/s] 95%|█████████▍| 53/56.072265625 [00:16<00:00,  4.08it/s] 96%|█████████▋| 54/56.072265625 [00:17<00:00,  4.16it/s] 98%|█████████▊| 55/56.072265625 [00:17<00:00,  4.09it/s]100%|█████████▉| 56/56.072265625 [00:17<00:00,  4.17it/s]57it [00:17,  5.01it/s]                                  57it [00:17,  3.19it/s]epoch 0 iter 0 sum correct 0.146484375 loss 1.945724368095398
epoch 0 iter 1 sum correct 0.1484375 loss 1.9452464580535889
epoch 0 iter 2 sum correct 0.15950520833333334 loss 1.9447200298309326
epoch 0 iter 3 sum correct 0.16748046875 loss 1.9443037509918213
epoch 0 iter 4 sum correct 0.183984375 loss 1.9438072443008423
epoch 0 iter 5 sum correct 0.20638020833333334 loss 1.9432352781295776
epoch 0 iter 6 sum correct 0.22572544642857142 loss 1.9427224397659302
epoch 0 iter 7 sum correct 0.240966796875 loss 1.9422730207443237
epoch 0 iter 8 sum correct 0.2630208333333333 loss 1.9417192935943604
epoch 0 iter 9 sum correct 0.27734375 loss 1.941292405128479
epoch 0 iter 10 sum correct 0.2878196022727273 loss 1.9407517910003662
epoch 0 iter 11 sum correct 0.3033854166666667 loss 1.940065622329712
epoch 0 iter 12 sum correct 0.3147536057692308 loss 1.9395067691802979
epoch 0 iter 13 sum correct 0.3247767857142857 loss 1.9389907121658325
epoch 0 iter 14 sum correct 0.33111979166666666 loss 1.9385595321655273
epoch 0 iter 15 sum correct 0.3367919921875 loss 1.9381287097930908
epoch 0 iter 16 sum correct 0.3450137867647059 loss 1.9375776052474976
epoch 0 iter 17 sum correct 0.3510199652777778 loss 1.9371914863586426
epoch 0 iter 18 sum correct 0.35711348684210525 loss 1.9367845058441162
epoch 0 iter 19 sum correct 0.36494140625 loss 1.9363291263580322
epoch 0 iter 20 sum correct 0.36941964285714285 loss 1.9359796047210693
epoch 0 iter 21 sum correct 0.3750887784090909 loss 1.935632586479187
epoch 0 iter 22 sum correct 0.38051970108695654 loss 1.9352716207504272
epoch 0 iter 23 sum correct 0.3854166666666667 loss 1.934936761856079
epoch 0 iter 24 sum correct 0.389296875 loss 1.934656023979187
epoch 0 iter 25 sum correct 0.39295372596153844 loss 1.9343969821929932
epoch 0 iter 26 sum correct 0.396484375 loss 1.9341198205947876
epoch 0 iter 27 sum correct 0.4017857142857143 loss 1.9337791204452515
epoch 0 iter 28 sum correct 0.40564385775862066 loss 1.933510184288025
epoch 0 iter 29 sum correct 0.409765625 loss 1.9332276582717896
epoch 0 iter 30 sum correct 0.41217237903225806 loss 1.933005928993225
epoch 0 iter 31 sum correct 0.41455078125 loss 1.9327811002731323
epoch 0 iter 32 sum correct 0.4169034090909091 loss 1.9325799942016602
epoch 0 iter 33 sum correct 0.42009420955882354 loss 1.932336449623108
epoch 0 iter 34 sum correct 0.4224888392857143 loss 1.9321202039718628
epoch 0 iter 35 sum correct 0.4255642361111111 loss 1.931897759437561
epoch 0 iter 36 sum correct 0.4289484797297297 loss 1.9316588640213013
epoch 0 iter 37 sum correct 0.43158922697368424 loss 1.9314697980880737
epoch 0 iter 38 sum correct 0.43379407051282054 loss 1.9313057661056519
epoch 0 iter 39 sum correct 0.437255859375 loss 1.9310880899429321
epoch 0 iter 40 sum correct 0.4386909298780488 loss 1.9309594631195068
epoch 0 iter 41 sum correct 0.44131324404761907 loss 1.9307605028152466
epoch 0 iter 42 sum correct 0.4437681686046512 loss 1.9305802583694458
epoch 0 iter 43 sum correct 0.4459783380681818 loss 1.9304031133651733
epoch 0 iter 44 sum correct 0.44765625 loss 1.930253028869629
epoch 0 iter 45 sum correct 0.4497707201086957 loss 1.930086612701416
epoch 0 iter 46 sum correct 0.45108876329787234 loss 1.929959774017334
epoch 0 iter 47 sum correct 0.453125 loss 1.9298166036605835
epoch 0 iter 48 sum correct 0.45527742346938777 loss 1.9296650886535645
epoch 0 iter 49 sum correct 0.4572265625 loss 1.9295209646224976
epoch 0 iter 50 sum correct 0.4595205269607843 loss 1.9293626546859741
epoch 0 iter 51 sum correct 0.4607496995192308 loss 1.9292546510696411
epoch 0 iter 52 sum correct 0.4625958136792453 loss 1.9291179180145264
epoch 0 iter 53 sum correct 0.4646629050925926 loss 1.9289679527282715
epoch 0 iter 54 sum correct 0.4661576704545455 loss 1.928846001625061
epoch 0 iter 55 sum correct 0.46854073660714285 loss 1.9286977052688599
epoch 0 iter 56 sum correct 0.46110882675438597 loss 1.9285513162612915
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.75it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.43it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.53it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.69it/s]8it [00:01,  5.80it/s]                                 macro  0.41902377948418135
micro  0.5463917525773195
[[185   0   1  78 102  26  75]
 [ 29   0   1   7  11   2   6]
 [ 64   0  19  57 193  92  71]
 [ 14   0   1 750  37  14  79]
 [111   0   3  58 332  11 138]
 [ 12   0   1  35  20 320  27]
 [ 58   0   1  56 127  10 355]]
              precision    recall  f1-score   support

           0       0.39      0.40      0.39       467
           1       0.00      0.00      0.00        56
           2       0.70      0.04      0.07       496
           3       0.72      0.84      0.77       895
           4       0.40      0.51      0.45       653
           5       0.67      0.77      0.72       415
           6       0.47      0.58      0.52       607

    accuracy                           0.55      3589
   macro avg       0.48      0.45      0.42      3589
weighted avg       0.56      0.55      0.51      3589

correct 0.5463917525773195 
f1 0.41902377948418135 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.72it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.38it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.79it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.04it/s]8it [00:01,  5.96it/s]                                 macro  0.4243573034443429
micro  0.556143772638618
[[209   0   4  72 110  24  72]
 [ 26   0   0  11  12   3   3]
 [ 91   0  15  63 183 101  75]
 [ 11   0   3 768  46  13  38]
 [ 74   0   7  46 338   9 120]
 [ 15   0   6  50  18 289  38]
 [ 60   0   0  45 127  17 377]]
              precision    recall  f1-score   support

           0       0.43      0.43      0.43       491
           1       0.00      0.00      0.00        55
           2       0.43      0.03      0.05       528
           3       0.73      0.87      0.79       879
           4       0.41      0.57      0.47       594
           5       0.63      0.69      0.66       416
           6       0.52      0.60      0.56       626

    accuracy                           0.56      3589
   macro avg       0.45      0.46      0.42      3589
weighted avg       0.53      0.56      0.51      3589

correct 0.556143772638618 
f1 0.4243573034443429 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:39,  1.41it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.32it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.84it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.29it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.46it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.71it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.76it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.94it/s] 16%|█▌        | 9/56.072265625 [00:02<00:11,  3.93it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.05it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  4.01it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.10it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  4.07it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.15it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.09it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.17it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.07it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.14it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.04it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.12it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  3.95it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.04it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  3.97it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.07it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.01it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.11it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.02it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.10it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.03it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.06it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.02it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.11it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  3.98it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.08it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.01it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.08it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  3.94it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.00it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  3.95it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.07it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  3.95it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.05it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  3.99it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.08it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  3.92it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.03it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  3.92it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:02,  3.97it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  3.88it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.01it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  3.96it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:01,  4.06it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.02it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.12it/s] 98%|█████████▊| 55/56.072265625 [00:14<00:00,  4.06it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.14it/s]57it [00:14,  4.96it/s]                                  57it [00:14,  3.94it/s]epoch 1 iter 0 sum correct 0.587890625 loss 1.920724630355835
epoch 1 iter 1 sum correct 0.6103515625 loss 1.9197839498519897
epoch 1 iter 2 sum correct 0.6087239583333334 loss 1.9197940826416016
epoch 1 iter 3 sum correct 0.609375 loss 1.9197618961334229
epoch 1 iter 4 sum correct 0.616015625 loss 1.9195698499679565
epoch 1 iter 5 sum correct 0.6051432291666666 loss 1.91996431350708
epoch 1 iter 6 sum correct 0.6079799107142857 loss 1.9197908639907837
epoch 1 iter 7 sum correct 0.603759765625 loss 1.91994047164917
epoch 1 iter 8 sum correct 0.6032986111111112 loss 1.919983983039856
epoch 1 iter 9 sum correct 0.6025390625 loss 1.9200025796890259
epoch 1 iter 10 sum correct 0.603515625 loss 1.9199087619781494
epoch 1 iter 11 sum correct 0.6036783854166666 loss 1.91987943649292
epoch 1 iter 12 sum correct 0.6057692307692307 loss 1.9197125434875488
epoch 1 iter 13 sum correct 0.6036551339285714 loss 1.919834852218628
epoch 1 iter 14 sum correct 0.6053385416666667 loss 1.919711709022522
epoch 1 iter 15 sum correct 0.607666015625 loss 1.919611930847168
epoch 1 iter 16 sum correct 0.6100643382352942 loss 1.9194670915603638
epoch 1 iter 17 sum correct 0.6110026041666666 loss 1.9193882942199707
epoch 1 iter 18 sum correct 0.6119449013157895 loss 1.9193241596221924
epoch 1 iter 19 sum correct 0.61328125 loss 1.9191974401474
epoch 1 iter 20 sum correct 0.6155133928571429 loss 1.9190638065338135
epoch 1 iter 21 sum correct 0.6150568181818182 loss 1.919098138809204
epoch 1 iter 22 sum correct 0.6162533967391305 loss 1.9190253019332886
epoch 1 iter 23 sum correct 0.61767578125 loss 1.9189577102661133
epoch 1 iter 24 sum correct 0.616796875 loss 1.9189709424972534
epoch 1 iter 25 sum correct 0.6171875 loss 1.918933391571045
epoch 1 iter 26 sum correct 0.6166811342592593 loss 1.9189424514770508
epoch 1 iter 27 sum correct 0.6175362723214286 loss 1.9188549518585205
epoch 1 iter 28 sum correct 0.6184671336206896 loss 1.9187897443771362
epoch 1 iter 29 sum correct 0.61796875 loss 1.9187954664230347
epoch 1 iter 30 sum correct 0.6180065524193549 loss 1.918785572052002
epoch 1 iter 31 sum correct 0.61810302734375 loss 1.9187802076339722
epoch 1 iter 32 sum correct 0.6197916666666666 loss 1.9186947345733643
epoch 1 iter 33 sum correct 0.62109375 loss 1.918602466583252
epoch 1 iter 34 sum correct 0.6221540178571429 loss 1.918520450592041
epoch 1 iter 35 sum correct 0.6227213541666666 loss 1.9184529781341553
epoch 1 iter 36 sum correct 0.6226245777027027 loss 1.9184385538101196
epoch 1 iter 37 sum correct 0.6230982730263158 loss 1.9184123277664185
epoch 1 iter 38 sum correct 0.6236979166666666 loss 1.9183419942855835
epoch 1 iter 39 sum correct 0.623681640625 loss 1.918338418006897
epoch 1 iter 40 sum correct 0.6237137957317073 loss 1.9183354377746582
epoch 1 iter 41 sum correct 0.6244419642857143 loss 1.9182791709899902
epoch 1 iter 42 sum correct 0.6250454215116279 loss 1.9182428121566772
epoch 1 iter 43 sum correct 0.6247780539772727 loss 1.9182265996932983
epoch 1 iter 44 sum correct 0.6250434027777778 loss 1.9181959629058838
epoch 1 iter 45 sum correct 0.6256368885869565 loss 1.918154239654541
epoch 1 iter 46 sum correct 0.6265375664893617 loss 1.9180909395217896
epoch 1 iter 47 sum correct 0.6262613932291666 loss 1.9180864095687866
epoch 1 iter 48 sum correct 0.6258370535714286 loss 1.9180914163589478
epoch 1 iter 49 sum correct 0.6268359375 loss 1.9180256128311157
epoch 1 iter 50 sum correct 0.6274126838235294 loss 1.9179795980453491
epoch 1 iter 51 sum correct 0.6270282451923077 loss 1.9179766178131104
epoch 1 iter 52 sum correct 0.6268425707547169 loss 1.91796875
epoch 1 iter 53 sum correct 0.6272424768518519 loss 1.917917013168335
epoch 1 iter 54 sum correct 0.62734375 loss 1.917896032333374
epoch 1 iter 55 sum correct 0.6277204241071429 loss 1.9178657531738281
epoch 1 iter 56 sum correct 0.6173930921052632 loss 1.9179233312606812
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.70it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.41it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.66it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.98it/s]8it [00:01,  5.94it/s]                                 macro  0.4762641694201127
micro  0.5848425745332961
[[159   0  39  58 137  21  53]
 [ 11   0  11  10  18   2   4]
 [ 41   0 158  37 150  66  44]
 [  7   0   9 786  42  24  27]
 [ 27   0  57  73 396  12  88]
 [  9   0  31  30  17 316  12]
 [ 13   0  45  89 153  23 284]]
              precision    recall  f1-score   support

           0       0.60      0.34      0.43       467
           1       0.00      0.00      0.00        56
           2       0.45      0.32      0.37       496
           3       0.73      0.88      0.79       895
           4       0.43      0.61      0.51       653
           5       0.68      0.76      0.72       415
           6       0.55      0.47      0.51       607

    accuracy                           0.58      3589
   macro avg       0.49      0.48      0.48      3589
weighted avg       0.57      0.58      0.57      3589

correct 0.5848425745332961 
f1 0.4762641694201127 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.84it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.54it/s] 71%|███████▏  | 5/7.009765625 [00:00<00:00,  6.04it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.25it/s]8it [00:01,  6.19it/s]                                 macro  0.4763654219803301
micro  0.5851212036779047
[[159   0  50  59 152  21  50]
 [ 16   0  10  10  13   2   4]
 [ 48   0 167  54 145  76  38]
 [  6   0  17 788  42  13  13]
 [ 13   0  70  49 371   8  83]
 [  5   0  41  40  15 301  14]
 [ 11   0  34  79 155  33 314]]
              precision    recall  f1-score   support

           0       0.62      0.32      0.42       491
           1       0.00      0.00      0.00        55
           2       0.43      0.32      0.36       528
           3       0.73      0.90      0.80       879
           4       0.42      0.62      0.50       594
           5       0.66      0.72      0.69       416
           6       0.61      0.50      0.55       626

    accuracy                           0.59      3589
   macro avg       0.49      0.48      0.48      3589
weighted avg       0.58      0.59      0.57      3589

correct 0.5851212036779047 
f1 0.4763654219803301 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:37,  1.47it/s]  4%|▎         | 2/56.072265625 [00:00<00:22,  2.37it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.89it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.32it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.51it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.76it/s] 12%|█▏        | 7/56.072265625 [00:02<00:12,  3.82it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.97it/s] 16%|█▌        | 9/56.072265625 [00:02<00:11,  3.96it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.07it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  4.02it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.11it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  4.05it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.13it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.07it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.14it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.09it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.16it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.09it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.16it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.08it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.15it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.09it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.16it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.09it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.16it/s] 48%|████▊     | 27/56.072265625 [00:06<00:07,  4.08it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.15it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.09it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.16it/s] 55%|█████▌    | 31/56.072265625 [00:07<00:06,  4.10it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.17it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.10it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.17it/s] 62%|██████▏   | 35/56.072265625 [00:08<00:05,  4.09it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.16it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.09it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.17it/s] 70%|██████▉   | 39/56.072265625 [00:09<00:04,  4.09it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.16it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.09it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.16it/s] 77%|███████▋  | 43/56.072265625 [00:10<00:03,  4.09it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.16it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.09it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.16it/s] 84%|████████▍ | 47/56.072265625 [00:11<00:02,  4.09it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.16it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.09it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.17it/s] 91%|█████████ | 51/56.072265625 [00:12<00:01,  4.12it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.19it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.13it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.19it/s] 98%|█████████▊| 55/56.072265625 [00:13<00:00,  4.13it/s]100%|█████████▉| 56/56.072265625 [00:13<00:00,  4.19it/s]57it [00:14,  5.04it/s]                                  57it [00:14,  4.02it/s]epoch 2 iter 0 sum correct 0.748046875 loss 1.911010980606079
epoch 2 iter 1 sum correct 0.7216796875 loss 1.9126896858215332
epoch 2 iter 2 sum correct 0.71875 loss 1.912549376487732
epoch 2 iter 3 sum correct 0.70751953125 loss 1.9132084846496582
epoch 2 iter 4 sum correct 0.68984375 loss 1.9141143560409546
epoch 2 iter 5 sum correct 0.6904296875 loss 1.9140328168869019
epoch 2 iter 6 sum correct 0.6978236607142857 loss 1.91371488571167
epoch 2 iter 7 sum correct 0.6962890625 loss 1.9137332439422607
epoch 2 iter 8 sum correct 0.6955295138888888 loss 1.9137121438980103
epoch 2 iter 9 sum correct 0.69296875 loss 1.913817286491394
epoch 2 iter 10 sum correct 0.6951349431818182 loss 1.9137240648269653
epoch 2 iter 11 sum correct 0.697265625 loss 1.9135963916778564
epoch 2 iter 12 sum correct 0.6984675480769231 loss 1.9135034084320068
epoch 2 iter 13 sum correct 0.6988002232142857 loss 1.9134571552276611
epoch 2 iter 14 sum correct 0.6985677083333334 loss 1.9134752750396729
epoch 2 iter 15 sum correct 0.6981201171875 loss 1.913445234298706
epoch 2 iter 16 sum correct 0.6987591911764706 loss 1.91337251663208
epoch 2 iter 17 sum correct 0.6975911458333334 loss 1.913414478302002
epoch 2 iter 18 sum correct 0.6962376644736842 loss 1.9134125709533691
epoch 2 iter 19 sum correct 0.6966796875 loss 1.9133459329605103
epoch 2 iter 20 sum correct 0.6975446428571429 loss 1.913266658782959
epoch 2 iter 21 sum correct 0.6977095170454546 loss 1.9132612943649292
epoch 2 iter 22 sum correct 0.6975203804347826 loss 1.9132639169692993
epoch 2 iter 23 sum correct 0.697265625 loss 1.9132604598999023
epoch 2 iter 24 sum correct 0.6971875 loss 1.9132534265518188
epoch 2 iter 25 sum correct 0.6975661057692307 loss 1.9132013320922852
epoch 2 iter 26 sum correct 0.6985677083333334 loss 1.9131138324737549
epoch 2 iter 27 sum correct 0.6990094866071429 loss 1.9131053686141968
epoch 2 iter 28 sum correct 0.6966594827586207 loss 1.9132033586502075
epoch 2 iter 29 sum correct 0.6970703125 loss 1.9131486415863037
epoch 2 iter 30 sum correct 0.6971396169354839 loss 1.9131131172180176
epoch 2 iter 31 sum correct 0.69854736328125 loss 1.9130401611328125
epoch 2 iter 32 sum correct 0.6982717803030303 loss 1.9130467176437378
epoch 2 iter 33 sum correct 0.6988166360294118 loss 1.9129971265792847
epoch 2 iter 34 sum correct 0.698046875 loss 1.9130151271820068
epoch 2 iter 35 sum correct 0.6984049479166666 loss 1.9129736423492432
epoch 2 iter 36 sum correct 0.6978462837837838 loss 1.912981390953064
epoch 2 iter 37 sum correct 0.6976768092105263 loss 1.9129552841186523
epoch 2 iter 38 sum correct 0.6971153846153846 loss 1.9129586219787598
epoch 2 iter 39 sum correct 0.6974609375 loss 1.9129250049591064
epoch 2 iter 40 sum correct 0.6975038109756098 loss 1.9129148721694946
epoch 2 iter 41 sum correct 0.6976841517857143 loss 1.9128916263580322
epoch 2 iter 42 sum correct 0.6984011627906976 loss 1.9128414392471313
epoch 2 iter 43 sum correct 0.6981090198863636 loss 1.9128293991088867
epoch 2 iter 44 sum correct 0.6985243055555556 loss 1.9127987623214722
epoch 2 iter 45 sum correct 0.6981997282608695 loss 1.9127919673919678
epoch 2 iter 46 sum correct 0.6986785239361702 loss 1.9127466678619385
epoch 2 iter 47 sum correct 0.6985677083333334 loss 1.9127354621887207
epoch 2 iter 48 sum correct 0.6987404336734694 loss 1.9127047061920166
epoch 2 iter 49 sum correct 0.6987109375 loss 1.9126948118209839
epoch 2 iter 50 sum correct 0.6979549632352942 loss 1.9127216339111328
epoch 2 iter 51 sum correct 0.6972280649038461 loss 1.9127438068389893
epoch 2 iter 52 sum correct 0.6969708136792453 loss 1.9127495288848877
epoch 2 iter 53 sum correct 0.697265625 loss 1.9127202033996582
epoch 2 iter 54 sum correct 0.6974076704545454 loss 1.912701964378357
epoch 2 iter 55 sum correct 0.697265625 loss 1.9126886129379272
epoch 2 iter 56 sum correct 0.6857182017543859 loss 1.9128252267837524
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.62it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.21it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.34it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.60it/s]8it [00:01,  5.63it/s]                                 macro  0.511909096609875
micro  0.6062970186681527
[[239   0  79  19  77  11  42]
 [ 21   0  14   3  13   1   4]
 [ 51   0 220  15 119  46  45]
 [ 28   0  32 725  29  15  66]
 [ 71   0  95  30 356  12  89]
 [  9   0  44  17  20 320   5]
 [ 45   0  60  38 135  13 316]]
              precision    recall  f1-score   support

           0       0.52      0.51      0.51       467
           1       0.00      0.00      0.00        56
           2       0.40      0.44      0.42       496
           3       0.86      0.81      0.83       895
           4       0.48      0.55      0.51       653
           5       0.77      0.77      0.77       415
           6       0.56      0.52      0.54       607

    accuracy                           0.61      3589
   macro avg       0.51      0.51      0.51      3589
weighted avg       0.61      0.61      0.61      3589

correct 0.6062970186681527 
f1 0.511909096609875 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.73it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.46it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.83it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.11it/s]8it [00:01,  6.03it/s]                                 macro  0.5103680478258764
micro  0.6068542769573697
[[240   0  96  24  85   3  43]
 [ 27   0   9   3  11   2   3]
 [ 68   0 243  18 112  51  36]
 [ 26   0  18 744  40  18  33]
 [ 53   0  97  29 334   6  75]
 [  7   0  67  27  13 288  14]
 [ 57   0  49  32 146  13 329]]
              precision    recall  f1-score   support

           0       0.50      0.49      0.50       491
           1       0.00      0.00      0.00        55
           2       0.42      0.46      0.44       528
           3       0.85      0.85      0.85       879
           4       0.45      0.56      0.50       594
           5       0.76      0.69      0.72       416
           6       0.62      0.53      0.57       626

    accuracy                           0.61      3589
   macro avg       0.51      0.51      0.51      3589
weighted avg       0.61      0.61      0.61      3589

correct 0.6068542769573697 
f1 0.5103680478258764 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:39,  1.38it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.26it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.81it/s]  7%|▋         | 4/56.072265625 [00:01<00:16,  3.24it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.44it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.71it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.77it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.92it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.89it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.03it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.97it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.08it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  4.03it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.12it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.04it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.12it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.06it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.14it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.07it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.15it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.08it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.15it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.08it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.16it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.09it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.16it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.09it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.16it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.09it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.16it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.08it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.16it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.09it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.16it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.09it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.16it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.09it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.16it/s] 70%|██████▉   | 39/56.072265625 [00:09<00:04,  4.09it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.16it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.09it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.16it/s] 77%|███████▋  | 43/56.072265625 [00:10<00:03,  4.09it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.16it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.09it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.16it/s] 84%|████████▍ | 47/56.072265625 [00:11<00:02,  4.09it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.16it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.09it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.16it/s] 91%|█████████ | 51/56.072265625 [00:12<00:01,  4.11it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.18it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.10it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.17it/s] 98%|█████████▊| 55/56.072265625 [00:13<00:00,  4.12it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.18it/s]57it [00:14,  4.00it/s]                                  epoch 3 iter 0 sum correct 0.75390625 loss 1.9092143774032593
epoch 3 iter 1 sum correct 0.7578125 loss 1.9090471267700195
epoch 3 iter 2 sum correct 0.7649739583333334 loss 1.9088762998580933
epoch 3 iter 3 sum correct 0.76220703125 loss 1.9088903665542603
epoch 3 iter 4 sum correct 0.756640625 loss 1.9092140197753906
epoch 3 iter 5 sum correct 0.7600911458333334 loss 1.908989429473877
epoch 3 iter 6 sum correct 0.7572544642857143 loss 1.9090938568115234
epoch 3 iter 7 sum correct 0.753662109375 loss 1.9092481136322021
epoch 3 iter 8 sum correct 0.7508680555555556 loss 1.9093540906906128
epoch 3 iter 9 sum correct 0.752734375 loss 1.9092262983322144
epoch 3 iter 10 sum correct 0.7501775568181818 loss 1.909332513809204
epoch 3 iter 11 sum correct 0.7527669270833334 loss 1.909096121788025
epoch 3 iter 12 sum correct 0.7552584134615384 loss 1.9088964462280273
epoch 3 iter 13 sum correct 0.7547433035714286 loss 1.908941626548767
epoch 3 iter 14 sum correct 0.755078125 loss 1.9088729619979858
epoch 3 iter 15 sum correct 0.7542724609375 loss 1.9088776111602783
epoch 3 iter 16 sum correct 0.7531020220588235 loss 1.9088963270187378
epoch 3 iter 17 sum correct 0.7516276041666666 loss 1.908958077430725
epoch 3 iter 18 sum correct 0.750719572368421 loss 1.9089791774749756
epoch 3 iter 19 sum correct 0.750390625 loss 1.9090055227279663
epoch 3 iter 20 sum correct 0.7509300595238095 loss 1.9089802503585815
epoch 3 iter 21 sum correct 0.7507102272727273 loss 1.9089728593826294
epoch 3 iter 22 sum correct 0.7510190217391305 loss 1.9089417457580566
epoch 3 iter 23 sum correct 0.749755859375 loss 1.9089641571044922
epoch 3 iter 24 sum correct 0.7509375 loss 1.908876657485962
epoch 3 iter 25 sum correct 0.7490985576923077 loss 1.9089634418487549
epoch 3 iter 26 sum correct 0.7481192129629629 loss 1.9089950323104858
epoch 3 iter 27 sum correct 0.7467912946428571 loss 1.9090579748153687
epoch 3 iter 28 sum correct 0.7468345905172413 loss 1.9090447425842285
epoch 3 iter 29 sum correct 0.7470703125 loss 1.9089914560317993
epoch 3 iter 30 sum correct 0.7467867943548387 loss 1.9089998006820679
epoch 3 iter 31 sum correct 0.7469482421875 loss 1.9089781045913696
epoch 3 iter 32 sum correct 0.7471590909090909 loss 1.9089616537094116
epoch 3 iter 33 sum correct 0.7471852022058824 loss 1.9089564085006714
epoch 3 iter 34 sum correct 0.7466517857142857 loss 1.9089773893356323
epoch 3 iter 35 sum correct 0.7466905381944444 loss 1.9089480638504028
epoch 3 iter 36 sum correct 0.7465688344594594 loss 1.9089326858520508
epoch 3 iter 37 sum correct 0.74609375 loss 1.9089480638504028
epoch 3 iter 38 sum correct 0.7457932692307693 loss 1.9089410305023193
epoch 3 iter 39 sum correct 0.745263671875 loss 1.9089492559432983
epoch 3 iter 40 sum correct 0.7456173780487805 loss 1.908921480178833
epoch 3 iter 41 sum correct 0.7453962053571429 loss 1.9089229106903076
epoch 3 iter 42 sum correct 0.7453670058139535 loss 1.9089069366455078
epoch 3 iter 43 sum correct 0.7456498579545454 loss 1.9088882207870483
epoch 3 iter 44 sum correct 0.7453993055555556 loss 1.908886432647705
epoch 3 iter 45 sum correct 0.7449473505434783 loss 1.9088900089263916
epoch 3 iter 46 sum correct 0.7449301861702128 loss 1.908880352973938
epoch 3 iter 47 sum correct 0.7450764973958334 loss 1.9088596105575562
epoch 3 iter 48 sum correct 0.7452965561224489 loss 1.9088300466537476
epoch 3 iter 49 sum correct 0.74546875 loss 1.9088140726089478
epoch 3 iter 50 sum correct 0.7448299632352942 loss 1.9088327884674072
epoch 3 iter 51 sum correct 0.7450045072115384 loss 1.9088181257247925
epoch 3 iter 52 sum correct 0.7445828419811321 loss 1.908826470375061
epoch 3 iter 53 sum correct 0.7450086805555556 loss 1.908786654472351
epoch 3 iter 54 sum correct 0.7443181818181818 loss 1.9088027477264404
epoch 3 iter 55 sum correct 0.744384765625 loss 1.908787727355957
epoch 3 iter 56 sum correct 0.7321820175438597 loss 1.908811092376709
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.69it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.27it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.85it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.06it/s]8it [00:01,  5.86it/s]                                 macro  0.48270530829473995
micro  0.5748119253273892
[[260   0  94  26  35  19  33]
 [ 28   0  12   4   9   1   2]
 [ 71   0 262  14  58  58  33]
 [ 36   0  41 725  19  21  53]
 [ 72   0 221  31 222  21  86]
 [  9   0  43  19   3 331  10]
 [ 54   0 139  55  60  36 263]]
              precision    recall  f1-score   support

           0       0.49      0.56      0.52       467
           1       0.00      0.00      0.00        56
           2       0.32      0.53      0.40       496
           3       0.83      0.81      0.82       895
           4       0.55      0.34      0.42       653
           5       0.68      0.80      0.73       415
           6       0.55      0.43      0.48       607

    accuracy                           0.57      3589
   macro avg       0.49      0.50      0.48      3589
weighted avg       0.59      0.57      0.57      3589

correct 0.5748119253273892 
f1 0.48270530829473995 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.62it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.25it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.68it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.97it/s]8it [00:01,  5.90it/s]                                 macro  0.49782139836810574
micro  0.5971022568960713
[[268   0  96  26  42  15  44]
 [ 29   0  15   5   3   2   1]
 [ 74   0 282  29  45  70  28]
 [ 22   0  31 760  23  17  26]
 [ 77   0 205  22 195  15  80]
 [ 10   0  56  28   2 313   7]
 [ 43   0 127  29  62  40 325]]
              precision    recall  f1-score   support

           0       0.51      0.55      0.53       491
           1       0.00      0.00      0.00        55
           2       0.35      0.53      0.42       528
           3       0.85      0.86      0.85       879
           4       0.52      0.33      0.40       594
           5       0.66      0.75      0.70       416
           6       0.64      0.52      0.57       626

    accuracy                           0.60      3589
   macro avg       0.50      0.51      0.50      3589
weighted avg       0.60      0.60      0.59      3589

correct 0.5971022568960713 
f1 0.49782139836810574 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:39,  1.41it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.32it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.84it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.29it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.48it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.72it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.77it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.91it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.91it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.03it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.98it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.03it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  3.99it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.09it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.03it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.06it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  3.98it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.07it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.00it/s] 36%|███▌      | 20/56.072265625 [00:05<00:09,  4.00it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  3.91it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  3.97it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  3.93it/s] 43%|████▎     | 24/56.072265625 [00:06<00:08,  3.99it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  3.89it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  3.95it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  3.87it/s] 50%|████▉     | 28/56.072265625 [00:07<00:07,  3.94it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  3.89it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.00it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  3.96it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.06it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.00it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.09it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.03it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.12it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  3.97it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.01it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  3.97it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:04,  3.99it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  3.90it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  3.96it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  3.85it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:03,  3.92it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  3.84it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  3.97it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  3.95it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.05it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.01it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.10it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  4.04it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.12it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.06it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.14it/s] 98%|█████████▊| 55/56.072265625 [00:14<00:00,  4.07it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.14it/s]57it [00:14,  4.99it/s]                                  57it [00:14,  3.91it/s]epoch 4 iter 0 sum correct 0.76171875 loss 1.9071906805038452
epoch 4 iter 1 sum correct 0.7666015625 loss 1.9068138599395752
epoch 4 iter 2 sum correct 0.7532552083333334 loss 1.9076274633407593
epoch 4 iter 3 sum correct 0.75634765625 loss 1.9074398279190063
epoch 4 iter 4 sum correct 0.74921875 loss 1.9077885150909424
epoch 4 iter 5 sum correct 0.748046875 loss 1.9078730344772339
epoch 4 iter 6 sum correct 0.7522321428571429 loss 1.907686710357666
epoch 4 iter 7 sum correct 0.7529296875 loss 1.9076955318450928
epoch 4 iter 8 sum correct 0.7521701388888888 loss 1.907687783241272
epoch 4 iter 9 sum correct 0.7498046875 loss 1.907785415649414
epoch 4 iter 10 sum correct 0.7514204545454546 loss 1.9076182842254639
epoch 4 iter 11 sum correct 0.7555338541666666 loss 1.9073896408081055
epoch 4 iter 12 sum correct 0.7551081730769231 loss 1.9073584079742432
epoch 4 iter 13 sum correct 0.7536272321428571 loss 1.9074422121047974
epoch 4 iter 14 sum correct 0.7545572916666666 loss 1.9074015617370605
epoch 4 iter 15 sum correct 0.7564697265625 loss 1.907291054725647
epoch 4 iter 16 sum correct 0.755859375 loss 1.9073270559310913
epoch 4 iter 17 sum correct 0.7573784722222222 loss 1.907227635383606
epoch 4 iter 18 sum correct 0.7584292763157895 loss 1.90715491771698
epoch 4 iter 19 sum correct 0.75791015625 loss 1.9072033166885376
epoch 4 iter 20 sum correct 0.7593005952380952 loss 1.9071176052093506
epoch 4 iter 21 sum correct 0.7602982954545454 loss 1.907041072845459
epoch 4 iter 22 sum correct 0.7590013586956522 loss 1.9071120023727417
epoch 4 iter 23 sum correct 0.7582194010416666 loss 1.9071406126022339
epoch 4 iter 24 sum correct 0.75765625 loss 1.9071557521820068
epoch 4 iter 25 sum correct 0.7557842548076923 loss 1.9072537422180176
epoch 4 iter 26 sum correct 0.7546296296296297 loss 1.9072853326797485
epoch 4 iter 27 sum correct 0.7546735491071429 loss 1.9072765111923218
epoch 4 iter 28 sum correct 0.753973599137931 loss 1.9072928428649902
epoch 4 iter 29 sum correct 0.7529947916666667 loss 1.907320261001587
epoch 4 iter 30 sum correct 0.7535282258064516 loss 1.9072726964950562
epoch 4 iter 31 sum correct 0.754150390625 loss 1.9072394371032715
epoch 4 iter 32 sum correct 0.7551491477272727 loss 1.9071781635284424
epoch 4 iter 33 sum correct 0.7558019301470589 loss 1.9071276187896729
epoch 4 iter 34 sum correct 0.7559151785714285 loss 1.9071040153503418
epoch 4 iter 35 sum correct 0.7564019097222222 loss 1.9070583581924438
epoch 4 iter 36 sum correct 0.7569151182432432 loss 1.9070247411727905
epoch 4 iter 37 sum correct 0.7569901315789473 loss 1.9070125818252563
epoch 4 iter 38 sum correct 0.7567608173076923 loss 1.9070123434066772
epoch 4 iter 39 sum correct 0.75625 loss 1.907028079032898
epoch 4 iter 40 sum correct 0.7573837652439024 loss 1.9069480895996094
epoch 4 iter 41 sum correct 0.7562313988095238 loss 1.9069989919662476
epoch 4 iter 42 sum correct 0.7564498546511628 loss 1.9069650173187256
epoch 4 iter 43 sum correct 0.7575461647727273 loss 1.906879186630249
epoch 4 iter 44 sum correct 0.7582465277777778 loss 1.9068325757980347
epoch 4 iter 45 sum correct 0.7593410326086957 loss 1.9067513942718506
epoch 4 iter 46 sum correct 0.7593085106382979 loss 1.9067339897155762
epoch 4 iter 47 sum correct 0.7595621744791666 loss 1.9067027568817139
epoch 4 iter 48 sum correct 0.7599649234693877 loss 1.9066615104675293
epoch 4 iter 49 sum correct 0.7601953125 loss 1.9066447019577026
epoch 4 iter 50 sum correct 0.7601868872549019 loss 1.906642198562622
epoch 4 iter 51 sum correct 0.7608548677884616 loss 1.906594157218933
epoch 4 iter 52 sum correct 0.7607606132075472 loss 1.906596064567566
epoch 4 iter 53 sum correct 0.7613932291666666 loss 1.9065483808517456
epoch 4 iter 54 sum correct 0.761044034090909 loss 1.9065665006637573
epoch 4 iter 55 sum correct 0.7610212053571429 loss 1.906557321548462
epoch 4 iter 56 sum correct 0.7485951206140351 loss 1.906574010848999
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.63it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.21it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.49it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.79it/s]8it [00:01,  5.69it/s]                                 macro  0.5159053541222963
micro  0.6104764558372806
[[288   0  29  10  73   9  58]
 [ 33   0   8   3   7   1   4]
 [ 88   0 168  10 122  44  64]
 [ 76   0  13 673  34  23  76]
 [ 83   0  59  26 344  11 130]
 [ 22   0  27  13  11 328  14]
 [ 48   0  17  37 106   9 390]]
              precision    recall  f1-score   support

           0       0.45      0.62      0.52       467
           1       0.00      0.00      0.00        56
           2       0.52      0.34      0.41       496
           3       0.87      0.75      0.81       895
           4       0.49      0.53      0.51       653
           5       0.77      0.79      0.78       415
           6       0.53      0.64      0.58       607

    accuracy                           0.61      3589
   macro avg       0.52      0.52      0.52      3589
weighted avg       0.62      0.61      0.61      3589

correct 0.6104764558372806 
f1 0.5159053541222963 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.88it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.61it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.79it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.03it/s]8it [00:01,  6.09it/s]                                 macro  0.516303203788714
micro  0.6129841181387573
[[298   0  44   9  67   5  68]
 [ 33   0   9   3   7   1   2]
 [ 99   0 179  14 137  54  45]
 [ 73   0  18 690  34  19  45]
 [ 70   0  47  16 321   7 133]
 [ 21   0  41  15  12 305  22]
 [ 58   0  26  18 106  11 407]]
              precision    recall  f1-score   support

           0       0.46      0.61      0.52       491
           1       0.00      0.00      0.00        55
           2       0.49      0.34      0.40       528
           3       0.90      0.78      0.84       879
           4       0.47      0.54      0.50       594
           5       0.76      0.73      0.75       416
           6       0.56      0.65      0.60       626

    accuracy                           0.61      3589
   macro avg       0.52      0.52      0.52      3589
weighted avg       0.62      0.61      0.61      3589

correct 0.6129841181387573 
f1 0.516303203788714 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:38,  1.42it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.31it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.82it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.27it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.42it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.68it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.72it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.91it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.88it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.01it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.95it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.05it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  3.98it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.08it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  3.99it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.08it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.00it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.09it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.01it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.10it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.01it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.10it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.01it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.10it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.02it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.11it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.03it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.11it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.03it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.11it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.02it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.10it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.02it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.11it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.04it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.12it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.03it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.11it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  4.02it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.10it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.02it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.10it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  4.02it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:03,  3.87it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  3.86it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  3.99it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  3.96it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.06it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  3.99it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.09it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  4.02it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.11it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.04it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.12it/s] 98%|█████████▊| 55/56.072265625 [00:14<00:00,  4.06it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.13it/s]57it [00:14,  5.00it/s]                                  57it [00:14,  3.94it/s]epoch 5 iter 0 sum correct 0.8125 loss 1.90285325050354
epoch 5 iter 1 sum correct 0.8046875 loss 1.9031898975372314
epoch 5 iter 2 sum correct 0.7897135416666666 loss 1.90408456325531
epoch 5 iter 3 sum correct 0.78173828125 loss 1.9045268297195435
epoch 5 iter 4 sum correct 0.783203125 loss 1.904541015625
epoch 5 iter 5 sum correct 0.7822265625 loss 1.9046648740768433
epoch 5 iter 6 sum correct 0.7801339285714286 loss 1.9047757387161255
epoch 5 iter 7 sum correct 0.779296875 loss 1.9048339128494263
epoch 5 iter 8 sum correct 0.7762586805555556 loss 1.9049841165542603
epoch 5 iter 9 sum correct 0.7740234375 loss 1.9051055908203125
epoch 5 iter 10 sum correct 0.7734375 loss 1.9051368236541748
epoch 5 iter 11 sum correct 0.7721354166666666 loss 1.9052404165267944
epoch 5 iter 12 sum correct 0.7738882211538461 loss 1.9051066637039185
epoch 5 iter 13 sum correct 0.7710658482142857 loss 1.9052679538726807
epoch 5 iter 14 sum correct 0.7723958333333333 loss 1.905191421508789
epoch 5 iter 15 sum correct 0.7705078125 loss 1.9053012132644653
epoch 5 iter 16 sum correct 0.7705652573529411 loss 1.9052680730819702
epoch 5 iter 17 sum correct 0.7708333333333334 loss 1.905231237411499
epoch 5 iter 18 sum correct 0.768297697368421 loss 1.9053796529769897
epoch 5 iter 19 sum correct 0.7689453125 loss 1.9053341150283813
epoch 5 iter 20 sum correct 0.7709263392857143 loss 1.9052186012268066
epoch 5 iter 21 sum correct 0.7710404829545454 loss 1.9051862955093384
epoch 5 iter 22 sum correct 0.7710597826086957 loss 1.9051505327224731
epoch 5 iter 23 sum correct 0.7710774739583334 loss 1.9051501750946045
epoch 5 iter 24 sum correct 0.7734375 loss 1.9049949645996094
epoch 5 iter 25 sum correct 0.7723106971153846 loss 1.9050487279891968
epoch 5 iter 26 sum correct 0.7728587962962963 loss 1.9049948453903198
epoch 5 iter 27 sum correct 0.7739955357142857 loss 1.9049254655838013
epoch 5 iter 28 sum correct 0.7747844827586207 loss 1.9048516750335693
epoch 5 iter 29 sum correct 0.7746744791666667 loss 1.904842734336853
epoch 5 iter 30 sum correct 0.7740045362903226 loss 1.9048758745193481
epoch 5 iter 31 sum correct 0.77545166015625 loss 1.9047759771347046
epoch 5 iter 32 sum correct 0.7752130681818182 loss 1.9047750234603882
epoch 5 iter 33 sum correct 0.7749885110294118 loss 1.9047811031341553
epoch 5 iter 34 sum correct 0.7756696428571429 loss 1.904721736907959
epoch 5 iter 35 sum correct 0.7759874131944444 loss 1.9046798944473267
epoch 5 iter 36 sum correct 0.775707347972973 loss 1.9046813249588013
epoch 5 iter 37 sum correct 0.7757504111842105 loss 1.9046765565872192
epoch 5 iter 38 sum correct 0.7751903044871795 loss 1.9047012329101562
epoch 5 iter 39 sum correct 0.77509765625 loss 1.9046913385391235
epoch 5 iter 40 sum correct 0.7750571646341463 loss 1.9046801328659058
epoch 5 iter 41 sum correct 0.7754836309523809 loss 1.904651165008545
epoch 5 iter 42 sum correct 0.7749818313953488 loss 1.904676079750061
epoch 5 iter 43 sum correct 0.7745472301136364 loss 1.9046958684921265
epoch 5 iter 44 sum correct 0.7739149305555556 loss 1.9047260284423828
epoch 5 iter 45 sum correct 0.774031929347826 loss 1.9047012329101562
epoch 5 iter 46 sum correct 0.7740192819148937 loss 1.904691457748413
epoch 5 iter 47 sum correct 0.7749837239583334 loss 1.9046227931976318
epoch 5 iter 48 sum correct 0.7754703443877551 loss 1.9045865535736084
epoch 5 iter 49 sum correct 0.7751953125 loss 1.9045950174331665
epoch 5 iter 50 sum correct 0.7755438112745098 loss 1.9045742750167847
epoch 5 iter 51 sum correct 0.7760667067307693 loss 1.9045253992080688
epoch 5 iter 52 sum correct 0.7760170990566038 loss 1.9045164585113525
epoch 5 iter 53 sum correct 0.7760778356481481 loss 1.904513955116272
epoch 5 iter 54 sum correct 0.7761363636363636 loss 1.9045015573501587
epoch 5 iter 55 sum correct 0.7758440290178571 loss 1.9045172929763794
epoch 5 iter 56 sum correct 0.7632264254385965 loss 1.9044867753982544
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.66it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.28it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.68it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.83it/s]8it [00:01,  5.81it/s]                                 macro  0.5145175208587951
micro  0.616049038729451
[[268   0  26  40  78  11  44]
 [ 28   0   9   7  10   1   1]
 [ 65   0 185  34 129  45  38]
 [ 17   0   8 774  36  13  47]
 [ 86   0  63  47 387  16  54]
 [ 11   0  33  33  12 318   8]
 [ 51   0  28  66 174   9 279]]
              precision    recall  f1-score   support

           0       0.51      0.57      0.54       467
           1       0.00      0.00      0.00        56
           2       0.53      0.37      0.44       496
           3       0.77      0.86      0.82       895
           4       0.47      0.59      0.52       653
           5       0.77      0.77      0.77       415
           6       0.59      0.46      0.52       607

    accuracy                           0.62      3589
   macro avg       0.52      0.52      0.51      3589
weighted avg       0.61      0.62      0.61      3589

correct 0.616049038729451 
f1 0.5145175208587951 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.70it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.37it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.83it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.13it/s]8it [00:01,  6.05it/s]                                 macro  0.5177016213076745
micro  0.620785734187796
[[288   0  33  36  87   8  39]
 [ 28   0   6   9  10   1   1]
 [ 94   0 181  44 129  48  32]
 [ 20   0  11 777  33  14  24]
 [ 74   0  49  51 355   9  56]
 [ 13   0  34  41  13 298  17]
 [ 54   0  28  45 158  12 329]]
              precision    recall  f1-score   support

           0       0.50      0.59      0.54       491
           1       0.00      0.00      0.00        55
           2       0.53      0.34      0.42       528
           3       0.77      0.88      0.83       879
           4       0.45      0.60      0.51       594
           5       0.76      0.72      0.74       416
           6       0.66      0.53      0.59       626

    accuracy                           0.62      3589
   macro avg       0.53      0.52      0.52      3589
weighted avg       0.62      0.62      0.61      3589

correct 0.620785734187796 
f1 0.5177016213076745 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:38,  1.42it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.34it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.84it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.29it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.47it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.72it/s] 12%|█▏        | 7/56.072265625 [00:02<00:12,  3.78it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.95it/s] 16%|█▌        | 9/56.072265625 [00:02<00:11,  3.94it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.05it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.98it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.02it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  3.93it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  3.99it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  3.92it/s] 29%|██▊       | 16/56.072265625 [00:04<00:10,  3.98it/s] 30%|███       | 17/56.072265625 [00:04<00:10,  3.86it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  3.93it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  3.87it/s] 36%|███▌      | 20/56.072265625 [00:05<00:09,  3.93it/s] 37%|███▋      | 21/56.072265625 [00:05<00:09,  3.85it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  3.91it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  3.85it/s] 43%|████▎     | 24/56.072265625 [00:06<00:08,  3.93it/s] 45%|████▍     | 25/56.072265625 [00:06<00:08,  3.87it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  3.94it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  3.88it/s] 50%|████▉     | 28/56.072265625 [00:07<00:07,  3.94it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:07,  3.85it/s] 54%|█████▎    | 30/56.072265625 [00:08<00:06,  3.92it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  3.86it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:06,  3.93it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  3.87it/s] 61%|██████    | 34/56.072265625 [00:09<00:05,  3.94it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  3.88it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:05,  3.94it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  3.86it/s] 68%|██████▊   | 38/56.072265625 [00:10<00:04,  3.98it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  3.95it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.04it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.01it/s] 75%|███████▍  | 42/56.072265625 [00:11<00:03,  4.03it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  3.99it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.07it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  3.93it/s] 82%|████████▏ | 46/56.072265625 [00:12<00:02,  4.03it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  3.99it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.08it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.00it/s] 89%|████████▉ | 50/56.072265625 [00:13<00:01,  4.10it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  4.05it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.13it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.08it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.16it/s] 98%|█████████▊| 55/56.072265625 [00:14<00:00,  4.10it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.16it/s]57it [00:14,  5.03it/s]                                  57it [00:14,  3.89it/s]epoch 6 iter 0 sum correct 0.78125 loss 1.9037344455718994
epoch 6 iter 1 sum correct 0.791015625 loss 1.9028970003128052
epoch 6 iter 2 sum correct 0.7994791666666666 loss 1.9023817777633667
epoch 6 iter 3 sum correct 0.791015625 loss 1.9030311107635498
epoch 6 iter 4 sum correct 0.7953125 loss 1.9026758670806885
epoch 6 iter 5 sum correct 0.80078125 loss 1.9022730588912964
epoch 6 iter 6 sum correct 0.8018973214285714 loss 1.9022533893585205
epoch 6 iter 7 sum correct 0.802490234375 loss 1.9021624326705933
epoch 6 iter 8 sum correct 0.7990451388888888 loss 1.902416706085205
epoch 6 iter 9 sum correct 0.798828125 loss 1.9024593830108643
epoch 6 iter 10 sum correct 0.7979403409090909 loss 1.9024847745895386
epoch 6 iter 11 sum correct 0.7957356770833334 loss 1.9026293754577637
epoch 6 iter 12 sum correct 0.7955228365384616 loss 1.9026538133621216
epoch 6 iter 13 sum correct 0.7938058035714286 loss 1.902745008468628
epoch 6 iter 14 sum correct 0.7944010416666667 loss 1.902692198753357
epoch 6 iter 15 sum correct 0.796630859375 loss 1.9025520086288452
epoch 6 iter 16 sum correct 0.7966452205882353 loss 1.9025517702102661
epoch 6 iter 17 sum correct 0.7972005208333334 loss 1.9025131464004517
epoch 6 iter 18 sum correct 0.7986225328947368 loss 1.9024397134780884
epoch 6 iter 19 sum correct 0.79912109375 loss 1.9024028778076172
epoch 6 iter 20 sum correct 0.7989211309523809 loss 1.9023990631103516
epoch 6 iter 21 sum correct 0.7998046875 loss 1.9023383855819702
epoch 6 iter 22 sum correct 0.7990828804347826 loss 1.9023830890655518
epoch 6 iter 23 sum correct 0.8006184895833334 loss 1.9022750854492188
epoch 6 iter 24 sum correct 0.800390625 loss 1.9022867679595947
epoch 6 iter 25 sum correct 0.8005558894230769 loss 1.9022701978683472
epoch 6 iter 26 sum correct 0.7998408564814815 loss 1.9023118019104004
epoch 6 iter 27 sum correct 0.7991071428571429 loss 1.9023606777191162
epoch 6 iter 28 sum correct 0.7980199353448276 loss 1.9024217128753662
epoch 6 iter 29 sum correct 0.7972005208333334 loss 1.9024674892425537
epoch 6 iter 30 sum correct 0.7965599798387096 loss 1.9024989604949951
epoch 6 iter 31 sum correct 0.797119140625 loss 1.9024447202682495
epoch 6 iter 32 sum correct 0.7972892992424242 loss 1.9024224281311035
epoch 6 iter 33 sum correct 0.7956686580882353 loss 1.9025111198425293
epoch 6 iter 34 sum correct 0.7939174107142857 loss 1.902608871459961
epoch 6 iter 35 sum correct 0.7936740451388888 loss 1.902618408203125
epoch 6 iter 36 sum correct 0.7927576013513513 loss 1.9026604890823364
epoch 6 iter 37 sum correct 0.7923519736842105 loss 1.9026764631271362
epoch 6 iter 38 sum correct 0.7927684294871795 loss 1.9026397466659546
epoch 6 iter 39 sum correct 0.791943359375 loss 1.9026867151260376
epoch 6 iter 40 sum correct 0.7926829268292683 loss 1.902626872062683
epoch 6 iter 41 sum correct 0.7924572172619048 loss 1.9026353359222412
epoch 6 iter 42 sum correct 0.7924236918604651 loss 1.902625322341919
epoch 6 iter 43 sum correct 0.7927468039772727 loss 1.9025896787643433
epoch 6 iter 44 sum correct 0.7928385416666667 loss 1.9025691747665405
epoch 6 iter 45 sum correct 0.7918223505434783 loss 1.9026281833648682
epoch 6 iter 46 sum correct 0.7920545212765957 loss 1.902605652809143
epoch 6 iter 47 sum correct 0.7923177083333334 loss 1.902583122253418
epoch 6 iter 48 sum correct 0.7928890306122449 loss 1.9025352001190186
epoch 6 iter 49 sum correct 0.7927734375 loss 1.902532935142517
epoch 6 iter 50 sum correct 0.7930453431372549 loss 1.9025036096572876
epoch 6 iter 51 sum correct 0.7927058293269231 loss 1.9025250673294067
epoch 6 iter 52 sum correct 0.7925265330188679 loss 1.9025344848632812
epoch 6 iter 53 sum correct 0.7926070601851852 loss 1.9025245904922485
epoch 6 iter 54 sum correct 0.7922585227272727 loss 1.9025347232818604
epoch 6 iter 55 sum correct 0.7928989955357143 loss 1.9024821519851685
epoch 6 iter 56 sum correct 0.7798108552631579 loss 1.9026546478271484
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.68it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.25it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.79it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.92it/s]8it [00:01,  5.86it/s]                                 macro  0.49814720036561955
micro  0.6012816940651993
[[163   0  91  55  96  15  47]
 [ 11   0  10   8  22   1   4]
 [ 22   0 219  37 133  40  45]
 [  8   0  22 773  46  14  32]
 [ 33   0  92  43 392  14  79]
 [  5   0  52  24  13 314   7]
 [ 22   0  53  84 130  21 297]]
              precision    recall  f1-score   support

           0       0.62      0.35      0.45       467
           1       0.00      0.00      0.00        56
           2       0.41      0.44      0.42       496
           3       0.75      0.86      0.81       895
           4       0.47      0.60      0.53       653
           5       0.75      0.76      0.75       415
           6       0.58      0.49      0.53       607

    accuracy                           0.60      3589
   macro avg       0.51      0.50      0.50      3589
weighted avg       0.60      0.60      0.59      3589

correct 0.6012816940651993 
f1 0.49814720036561955 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.68it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.24it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.77it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.09it/s]8it [00:01,  5.91it/s]                                 macro  0.5073705100436686
micro  0.6124268598495403
[[165   0 118  49 104   6  49]
 [  9   0   6   8  29   2   1]
 [ 27   0 241  39 147  42  32]
 [  3   0  14 785  44  17  16]
 [ 21   0  66  27 374   8  98]
 [  2   0  52  32  13 304  13]
 [ 25   0  54  76 116  26 329]]
              precision    recall  f1-score   support

           0       0.65      0.34      0.44       491
           1       0.00      0.00      0.00        55
           2       0.44      0.46      0.45       528
           3       0.77      0.89      0.83       879
           4       0.45      0.63      0.53       594
           5       0.75      0.73      0.74       416
           6       0.61      0.53      0.57       626

    accuracy                           0.61      3589
   macro avg       0.53      0.51      0.51      3589
weighted avg       0.61      0.61      0.60      3589

correct 0.6124268598495403 
f1 0.5073705100436686 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:39,  1.38it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.26it/s]  5%|▌         | 3/56.072265625 [00:01<00:19,  2.77it/s]  7%|▋         | 4/56.072265625 [00:01<00:16,  3.19it/s]  9%|▉         | 5/56.072265625 [00:01<00:15,  3.34it/s] 11%|█         | 6/56.072265625 [00:01<00:14,  3.57it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.62it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.76it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.74it/s] 18%|█▊        | 10/56.072265625 [00:03<00:11,  3.89it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.88it/s] 21%|██▏       | 12/56.072265625 [00:03<00:11,  3.95it/s] 23%|██▎       | 13/56.072265625 [00:03<00:11,  3.86it/s] 25%|██▍       | 14/56.072265625 [00:04<00:10,  3.93it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  3.86it/s] 29%|██▊       | 16/56.072265625 [00:04<00:10,  3.98it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  3.92it/s] 32%|███▏      | 18/56.072265625 [00:05<00:09,  3.99it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  3.84it/s] 36%|███▌      | 20/56.072265625 [00:05<00:09,  3.95it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  3.91it/s] 39%|███▉      | 22/56.072265625 [00:06<00:08,  4.03it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  3.99it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.08it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.03it/s] 46%|████▋     | 26/56.072265625 [00:07<00:07,  4.10it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.00it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.09it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.04it/s] 54%|█████▎    | 30/56.072265625 [00:08<00:06,  4.12it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.05it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.05it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  3.94it/s] 61%|██████    | 34/56.072265625 [00:09<00:05,  4.03it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  3.99it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:05,  4.01it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  3.91it/s] 68%|██████▊   | 38/56.072265625 [00:10<00:04,  3.96it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  3.83it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:04,  3.93it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  3.93it/s] 75%|███████▍  | 42/56.072265625 [00:11<00:03,  4.00it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  3.97it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.07it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.02it/s] 82%|████████▏ | 46/56.072265625 [00:12<00:02,  4.10it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  3.93it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:02,  3.97it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  3.95it/s] 89%|████████▉ | 50/56.072265625 [00:13<00:01,  4.05it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  4.02it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.09it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.04it/s] 96%|█████████▋| 54/56.072265625 [00:14<00:00,  4.12it/s] 98%|█████████▊| 55/56.072265625 [00:14<00:00,  4.07it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.13it/s]57it [00:14,  4.98it/s]                                  57it [00:14,  3.88it/s]epoch 7 iter 0 sum correct 0.798828125 loss 1.9012444019317627
epoch 7 iter 1 sum correct 0.7890625 loss 1.9020485877990723
epoch 7 iter 2 sum correct 0.7942708333333334 loss 1.9017820358276367
epoch 7 iter 3 sum correct 0.787109375 loss 1.9022948741912842
epoch 7 iter 4 sum correct 0.790625 loss 1.9021717309951782
epoch 7 iter 5 sum correct 0.7926432291666666 loss 1.9019992351531982
epoch 7 iter 6 sum correct 0.791015625 loss 1.9020482301712036
epoch 7 iter 7 sum correct 0.7900390625 loss 1.9020538330078125
epoch 7 iter 8 sum correct 0.7868923611111112 loss 1.9022496938705444
epoch 7 iter 9 sum correct 0.7853515625 loss 1.9023367166519165
epoch 7 iter 10 sum correct 0.78515625 loss 1.902328372001648
epoch 7 iter 11 sum correct 0.7845052083333334 loss 1.9023597240447998
epoch 7 iter 12 sum correct 0.7877103365384616 loss 1.902161717414856
epoch 7 iter 13 sum correct 0.7876674107142857 loss 1.9021577835083008
epoch 7 iter 14 sum correct 0.7861979166666667 loss 1.90225350856781
epoch 7 iter 15 sum correct 0.78662109375 loss 1.9022380113601685
epoch 7 iter 16 sum correct 0.7859604779411765 loss 1.9022600650787354
epoch 7 iter 17 sum correct 0.7867838541666666 loss 1.9021954536437988
epoch 7 iter 18 sum correct 0.7875205592105263 loss 1.9021339416503906
epoch 7 iter 19 sum correct 0.78828125 loss 1.902077555656433
epoch 7 iter 20 sum correct 0.7879464285714286 loss 1.902086615562439
epoch 7 iter 21 sum correct 0.7888849431818182 loss 1.902017593383789
epoch 7 iter 22 sum correct 0.7869395380434783 loss 1.9021296501159668
epoch 7 iter 23 sum correct 0.78662109375 loss 1.9021391868591309
epoch 7 iter 24 sum correct 0.786796875 loss 1.9021317958831787
epoch 7 iter 25 sum correct 0.7866586538461539 loss 1.9021425247192383
epoch 7 iter 26 sum correct 0.7879050925925926 loss 1.902063012123108
epoch 7 iter 27 sum correct 0.7886439732142857 loss 1.9020200967788696
epoch 7 iter 28 sum correct 0.7884563577586207 loss 1.9020462036132812
epoch 7 iter 29 sum correct 0.788671875 loss 1.9020262956619263
epoch 7 iter 30 sum correct 0.7889364919354839 loss 1.9020140171051025
epoch 7 iter 31 sum correct 0.78955078125 loss 1.9019640684127808
epoch 7 iter 32 sum correct 0.7897727272727273 loss 1.9019527435302734
epoch 7 iter 33 sum correct 0.7907284007352942 loss 1.9018844366073608
epoch 7 iter 34 sum correct 0.7911272321428572 loss 1.9018580913543701
epoch 7 iter 35 sum correct 0.7912868923611112 loss 1.9018285274505615
epoch 7 iter 36 sum correct 0.7914379222972973 loss 1.901811122894287
epoch 7 iter 37 sum correct 0.7910670230263158 loss 1.9018235206604004
epoch 7 iter 38 sum correct 0.7907652243589743 loss 1.9018384218215942
epoch 7 iter 39 sum correct 0.790625 loss 1.9018410444259644
epoch 7 iter 40 sum correct 0.7903487042682927 loss 1.901847004890442
epoch 7 iter 41 sum correct 0.7907831101190477 loss 1.9018162488937378
epoch 7 iter 42 sum correct 0.7902434593023255 loss 1.9018446207046509
epoch 7 iter 43 sum correct 0.7903053977272727 loss 1.9018340110778809
epoch 7 iter 44 sum correct 0.7907118055555555 loss 1.9018008708953857
epoch 7 iter 45 sum correct 0.7903787364130435 loss 1.9018117189407349
epoch 7 iter 46 sum correct 0.7905585106382979 loss 1.9017963409423828
epoch 7 iter 47 sum correct 0.7905680338541666 loss 1.9017871618270874
epoch 7 iter 48 sum correct 0.7899792729591837 loss 1.9018235206604004
epoch 7 iter 49 sum correct 0.79 loss 1.9018168449401855
epoch 7 iter 50 sum correct 0.7900199142156863 loss 1.9018117189407349
epoch 7 iter 51 sum correct 0.78955078125 loss 1.9018299579620361
epoch 7 iter 52 sum correct 0.7892099056603774 loss 1.9018478393554688
epoch 7 iter 53 sum correct 0.7893880208333334 loss 1.9018311500549316
epoch 7 iter 54 sum correct 0.7893110795454545 loss 1.9018287658691406
epoch 7 iter 55 sum correct 0.7886090959821429 loss 1.901857852935791
epoch 7 iter 56 sum correct 0.7756647478070176 loss 1.9019584655761719
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.70it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.35it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.36it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.69it/s]8it [00:01,  5.67it/s]                                 macro  0.5124641091756754
micro  0.6127054889941488
[[288   0  26  27  66  14  46]
 [ 28   0   6   5  12   1   4]
 [ 78   0 182  23 120  48  45]
 [ 31   0  13 754  33  24  40]
 [ 87   0  73  47 336  12  98]
 [ 18   0  35  18  12 325   7]
 [ 57   0  41  63 117  15 314]]
              precision    recall  f1-score   support

           0       0.49      0.62      0.55       467
           1       0.00      0.00      0.00        56
           2       0.48      0.37      0.42       496
           3       0.80      0.84      0.82       895
           4       0.48      0.51      0.50       653
           5       0.74      0.78      0.76       415
           6       0.57      0.52      0.54       607

    accuracy                           0.61      3589
   macro avg       0.51      0.52      0.51      3589
weighted avg       0.60      0.61      0.60      3589

correct 0.6127054889941488 
f1 0.5124641091756754 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.88it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.63it/s] 71%|███████▏  | 5/7.009765625 [00:00<00:00,  5.96it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.21it/s]8it [00:01,  6.21it/s]                                 macro  0.5216545852063518
micro  0.6238506547784898
[[313   0  35  26  63   5  49]
 [ 26   0   7  10   7   3   2]
 [ 99   0 200  28 111  55  35]
 [ 27   0  11 764  36  20  21]
 [ 70   0  63  39 317   8  97]
 [ 14   0  45  28  10 309  10]
 [ 56   0  35  47 129  23 336]]
              precision    recall  f1-score   support

           0       0.52      0.64      0.57       491
           1       0.00      0.00      0.00        55
           2       0.51      0.38      0.43       528
           3       0.81      0.87      0.84       879
           4       0.47      0.53      0.50       594
           5       0.73      0.74      0.74       416
           6       0.61      0.54      0.57       626

    accuracy                           0.62      3589
   macro avg       0.52      0.53      0.52      3589
weighted avg       0.61      0.62      0.62      3589

correct 0.6238506547784898 
f1 0.5216545852063518 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:39,  1.39it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.28it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.81it/s]  7%|▋         | 4/56.072265625 [00:01<00:16,  3.25it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.45it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.68it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.75it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.70it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.76it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  3.90it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.89it/s] 21%|██▏       | 12/56.072265625 [00:03<00:11,  3.99it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  3.96it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.04it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  3.99it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.07it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.01it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.08it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.02it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.09it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.03it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.09it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.03it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.09it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.03it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.09it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.02it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.08it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.02it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.09it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.03it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.11it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.04it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.10it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.04it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.11it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.04it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.10it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  4.01it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.09it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.03it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.09it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  4.02it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.10it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.04it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.10it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  4.03it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.10it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.03it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.09it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  4.05it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.14it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.09it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.15it/s] 98%|█████████▊| 55/56.072265625 [00:14<00:00,  4.09it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.16it/s]57it [00:14,  5.03it/s]                                  57it [00:14,  3.94it/s]epoch 8 iter 0 sum correct 0.794921875 loss 1.900930643081665
epoch 8 iter 1 sum correct 0.796875 loss 1.9009692668914795
epoch 8 iter 2 sum correct 0.7897135416666666 loss 1.9014742374420166
epoch 8 iter 3 sum correct 0.78564453125 loss 1.9016971588134766
epoch 8 iter 4 sum correct 0.7828125 loss 1.9019047021865845
epoch 8 iter 5 sum correct 0.7835286458333334 loss 1.901813268661499
epoch 8 iter 6 sum correct 0.7868303571428571 loss 1.9015381336212158
epoch 8 iter 7 sum correct 0.78955078125 loss 1.9013452529907227
epoch 8 iter 8 sum correct 0.7899305555555556 loss 1.901315689086914
epoch 8 iter 9 sum correct 0.792578125 loss 1.9011338949203491
epoch 8 iter 10 sum correct 0.79296875 loss 1.901140809059143
epoch 8 iter 11 sum correct 0.7939453125 loss 1.9010881185531616
epoch 8 iter 12 sum correct 0.7934194711538461 loss 1.9011212587356567
epoch 8 iter 13 sum correct 0.7950613839285714 loss 1.9010099172592163
epoch 8 iter 14 sum correct 0.7955729166666666 loss 1.900971531867981
epoch 8 iter 15 sum correct 0.7974853515625 loss 1.9008572101593018
epoch 8 iter 16 sum correct 0.7967601102941176 loss 1.9009020328521729
epoch 8 iter 17 sum correct 0.7972005208333334 loss 1.9008642435073853
epoch 8 iter 18 sum correct 0.7986225328947368 loss 1.9007641077041626
epoch 8 iter 19 sum correct 0.79873046875 loss 1.9007463455200195
epoch 8 iter 20 sum correct 0.7996651785714286 loss 1.9006896018981934
epoch 8 iter 21 sum correct 0.8006036931818182 loss 1.9006426334381104
epoch 8 iter 22 sum correct 0.8014605978260869 loss 1.900566577911377
epoch 8 iter 23 sum correct 0.801025390625 loss 1.900583028793335
epoch 8 iter 24 sum correct 0.80296875 loss 1.900457739830017
epoch 8 iter 25 sum correct 0.8013070913461539 loss 1.9005542993545532
epoch 8 iter 26 sum correct 0.8016493055555556 loss 1.9005132913589478
epoch 8 iter 27 sum correct 0.8026646205357143 loss 1.900429368019104
epoch 8 iter 28 sum correct 0.8029364224137931 loss 1.9004062414169312
epoch 8 iter 29 sum correct 0.8036458333333333 loss 1.9003463983535767
epoch 8 iter 30 sum correct 0.8039944556451613 loss 1.900303840637207
epoch 8 iter 31 sum correct 0.80340576171875 loss 1.900348424911499
epoch 8 iter 32 sum correct 0.8031486742424242 loss 1.9003719091415405
epoch 8 iter 33 sum correct 0.8033662683823529 loss 1.9003511667251587
epoch 8 iter 34 sum correct 0.8029575892857143 loss 1.9003655910491943
epoch 8 iter 35 sum correct 0.8021918402777778 loss 1.9004052877426147
epoch 8 iter 36 sum correct 0.8020481418918919 loss 1.900400996208191
epoch 8 iter 37 sum correct 0.802477384868421 loss 1.9003547430038452
epoch 8 iter 38 sum correct 0.8021834935897436 loss 1.9003630876541138
epoch 8 iter 39 sum correct 0.801806640625 loss 1.9003857374191284
epoch 8 iter 40 sum correct 0.8008765243902439 loss 1.900434136390686
epoch 8 iter 41 sum correct 0.8005952380952381 loss 1.9004498720169067
epoch 8 iter 42 sum correct 0.8010992005813954 loss 1.9004087448120117
epoch 8 iter 43 sum correct 0.80126953125 loss 1.9003913402557373
epoch 8 iter 44 sum correct 0.8015190972222223 loss 1.9003669023513794
epoch 8 iter 45 sum correct 0.8018002717391305 loss 1.9003443717956543
epoch 8 iter 46 sum correct 0.8018201462765957 loss 1.9003475904464722
epoch 8 iter 47 sum correct 0.8006998697916666 loss 1.9004162549972534
epoch 8 iter 48 sum correct 0.8011001275510204 loss 1.9003819227218628
epoch 8 iter 49 sum correct 0.801171875 loss 1.9003702402114868
epoch 8 iter 50 sum correct 0.8012408088235294 loss 1.9003576040267944
epoch 8 iter 51 sum correct 0.8003680889423077 loss 1.9004067182540894
epoch 8 iter 52 sum correct 0.8012603183962265 loss 1.9003385305404663
epoch 8 iter 53 sum correct 0.8013599537037037 loss 1.9003244638442993
epoch 8 iter 54 sum correct 0.8016335227272727 loss 1.9003033638000488
epoch 8 iter 55 sum correct 0.8016183035714286 loss 1.900299310684204
epoch 8 iter 56 sum correct 0.7884457236842105 loss 1.9004137516021729
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.68it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.35it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.62it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.81it/s]8it [00:01,  5.82it/s]                                 macro  0.5188857650081052
micro  0.6263583170799666
[[233   0  27  42  70  15  80]
 [ 30   0   6   4   7   2   7]
 [ 49   0 166  35 127  57  62]
 [ 12   0   6 798  25  18  36]
 [ 57   0  40  55 362  14 125]
 [  7   0  17  26  14 333  18]
 [ 28   0  11  82 122   8 356]]
              precision    recall  f1-score   support

           0       0.56      0.50      0.53       467
           1       0.00      0.00      0.00        56
           2       0.61      0.33      0.43       496
           3       0.77      0.89      0.82       895
           4       0.50      0.55      0.52       653
           5       0.74      0.80      0.77       415
           6       0.52      0.59      0.55       607

    accuracy                           0.63      3589
   macro avg       0.53      0.52      0.52      3589
weighted avg       0.61      0.63      0.61      3589

correct 0.6263583170799666 
f1 0.5188857650081052 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.73it/s] 29%|██▊       | 2/7.009765625 [00:00<00:01,  3.37it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.58it/s] 57%|█████▋    | 4/7.009765625 [00:00<00:00,  5.83it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  6.33it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.50it/s]8it [00:01,  5.91it/s]                                 macro  0.534718244984367
micro  0.6430760657564781
[[256   0  34  39  76  14  72]
 [ 27   0   5   7   8   2   6]
 [ 66   0 192  41 112  54  63]
 [  8   0   5 805  28  16  17]
 [ 59   0  40  42 330   9 114]
 [  6   0  14  33  14 330  19]
 [ 24   0  22  68 108   9 395]]
              precision    recall  f1-score   support

           0       0.57      0.52      0.55       491
           1       0.00      0.00      0.00        55
           2       0.62      0.36      0.46       528
           3       0.78      0.92      0.84       879
           4       0.49      0.56      0.52       594
           5       0.76      0.79      0.78       416
           6       0.58      0.63      0.60       626

    accuracy                           0.64      3589
   macro avg       0.54      0.54      0.53      3589
weighted avg       0.63      0.64      0.63      3589

correct 0.6430760657564781 
f1 0.534718244984367 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:39,  1.38it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.31it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.83it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.28it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.43it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.68it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.71it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.89it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.86it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  3.99it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.98it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.07it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  4.03it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.11it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.03it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.11it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.05it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.12it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.03it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.10it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.02it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.10it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.01it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.10it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.04it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.11it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.02it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.10it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.03it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.11it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.03it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.11it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.05it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.12it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.04it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.12it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.04it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.12it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  4.04it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.12it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.04it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.12it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  4.04it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.12it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.06it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.13it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  4.06it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.13it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.05it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.13it/s] 91%|█████████ | 51/56.072265625 [00:12<00:01,  4.07it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.15it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.08it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.15it/s] 98%|█████████▊| 55/56.072265625 [00:13<00:00,  4.08it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.15it/s]57it [00:14,  3.97it/s]                                  epoch 9 iter 0 sum correct 0.8359375 loss 1.8976765871047974
epoch 9 iter 1 sum correct 0.822265625 loss 1.8984571695327759
epoch 9 iter 2 sum correct 0.806640625 loss 1.8993949890136719
epoch 9 iter 3 sum correct 0.802734375 loss 1.8997119665145874
epoch 9 iter 4 sum correct 0.794921875 loss 1.9002611637115479
epoch 9 iter 5 sum correct 0.7861328125 loss 1.900873064994812
epoch 9 iter 6 sum correct 0.7840401785714286 loss 1.9010452032089233
epoch 9 iter 7 sum correct 0.782470703125 loss 1.901209831237793
epoch 9 iter 8 sum correct 0.7842881944444444 loss 1.9011067152023315
epoch 9 iter 9 sum correct 0.7861328125 loss 1.9009952545166016
epoch 9 iter 10 sum correct 0.7856889204545454 loss 1.901019811630249
epoch 9 iter 11 sum correct 0.78662109375 loss 1.9009788036346436
epoch 9 iter 12 sum correct 0.7865084134615384 loss 1.900978446006775
epoch 9 iter 13 sum correct 0.7852957589285714 loss 1.9010672569274902
epoch 9 iter 14 sum correct 0.784765625 loss 1.9010612964630127
epoch 9 iter 15 sum correct 0.7850341796875 loss 1.9010260105133057
epoch 9 iter 16 sum correct 0.7856158088235294 loss 1.9009727239608765
epoch 9 iter 17 sum correct 0.7861328125 loss 1.9009435176849365
epoch 9 iter 18 sum correct 0.787109375 loss 1.9008588790893555
epoch 9 iter 19 sum correct 0.7888671875 loss 1.9007549285888672
epoch 9 iter 20 sum correct 0.7881324404761905 loss 1.900766372680664
epoch 9 iter 21 sum correct 0.7887073863636364 loss 1.9007139205932617
epoch 9 iter 22 sum correct 0.7886379076086957 loss 1.900728702545166
epoch 9 iter 23 sum correct 0.7908528645833334 loss 1.9005764722824097
epoch 9 iter 24 sum correct 0.78984375 loss 1.9006487131118774
epoch 9 iter 25 sum correct 0.7901893028846154 loss 1.900620937347412
epoch 9 iter 26 sum correct 0.7905815972222222 loss 1.9005800485610962
epoch 9 iter 27 sum correct 0.7903878348214286 loss 1.9006037712097168
epoch 9 iter 28 sum correct 0.7908809267241379 loss 1.9005663394927979
epoch 9 iter 29 sum correct 0.7916666666666666 loss 1.9005060195922852
epoch 9 iter 30 sum correct 0.7930317540322581 loss 1.9003959894180298
epoch 9 iter 31 sum correct 0.7930908203125 loss 1.9003783464431763
epoch 9 iter 32 sum correct 0.7942116477272727 loss 1.9003068208694458
epoch 9 iter 33 sum correct 0.7940027573529411 loss 1.900322675704956
epoch 9 iter 34 sum correct 0.7942522321428571 loss 1.9002995491027832
epoch 9 iter 35 sum correct 0.7950303819444444 loss 1.9002412557601929
epoch 9 iter 36 sum correct 0.7965054898648649 loss 1.9001277685165405
epoch 9 iter 37 sum correct 0.7955386513157895 loss 1.9001880884170532
epoch 9 iter 38 sum correct 0.7945713141025641 loss 1.9002434015274048
epoch 9 iter 39 sum correct 0.79541015625 loss 1.9001801013946533
epoch 9 iter 40 sum correct 0.7952553353658537 loss 1.9001851081848145
epoch 9 iter 41 sum correct 0.7956194196428571 loss 1.9001555442810059
epoch 9 iter 42 sum correct 0.7955577761627907 loss 1.9001542329788208
epoch 9 iter 43 sum correct 0.7963423295454546 loss 1.9000955820083618
epoch 9 iter 44 sum correct 0.7964409722222222 loss 1.900077223777771
epoch 9 iter 45 sum correct 0.7962381114130435 loss 1.9000790119171143
epoch 9 iter 46 sum correct 0.7962101063829787 loss 1.900073528289795
epoch 9 iter 47 sum correct 0.7961018880208334 loss 1.9000771045684814
epoch 9 iter 48 sum correct 0.797233737244898 loss 1.899998664855957
epoch 9 iter 49 sum correct 0.7975 loss 1.8999706506729126
epoch 9 iter 50 sum correct 0.7976792279411765 loss 1.8999524116516113
epoch 9 iter 51 sum correct 0.7974384014423077 loss 1.899958610534668
epoch 9 iter 52 sum correct 0.7976857311320755 loss 1.8999403715133667
epoch 9 iter 53 sum correct 0.7974898726851852 loss 1.8999439477920532
epoch 9 iter 54 sum correct 0.7975852272727273 loss 1.8999329805374146
epoch 9 iter 55 sum correct 0.79736328125 loss 1.8999381065368652
epoch 9 iter 56 sum correct 0.7844024122807017 loss 1.8999186754226685
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.68it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.35it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.51it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.78it/s]8it [00:01,  5.82it/s]                                 macro  0.5188296490838874
micro  0.6113123432711062
[[186   0  87  24 104   9  57]
 [  9   0  16   4  21   0   6]
 [ 35   0 268  15 117  18  43]
 [ 13   0  36 712  42  10  82]
 [ 38   0  98  25 402   4  86]
 [  7   0  56  23  16 293  20]
 [ 19   0  62  37 152   4 333]]
              precision    recall  f1-score   support

           0       0.61      0.40      0.48       467
           1       0.00      0.00      0.00        56
           2       0.43      0.54      0.48       496
           3       0.85      0.80      0.82       895
           4       0.47      0.62      0.53       653
           5       0.87      0.71      0.78       415
           6       0.53      0.55      0.54       607

    accuracy                           0.61      3589
   macro avg       0.54      0.51      0.52      3589
weighted avg       0.63      0.61      0.61      3589

correct 0.6113123432711062 
f1 0.5188296490838874 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.68it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.32it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.83it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.02it/s]8it [00:01,  5.94it/s]                                 macro  0.5250694393478901
micro  0.6244079130677069
[[196   0  92  26 110   4  63]
 [  8   0  18   7  17   1   4]
 [ 25   0 292  17 116  28  50]
 [  7   0  23 748  48  13  40]
 [ 25   0  88  23 373   2  83]
 [  5   0  86  28  16 256  25]
 [ 35   0  55  18 139   3 376]]
              precision    recall  f1-score   support

           0       0.65      0.40      0.49       491
           1       0.00      0.00      0.00        55
           2       0.45      0.55      0.49       528
           3       0.86      0.85      0.86       879
           4       0.46      0.63      0.53       594
           5       0.83      0.62      0.71       416
           6       0.59      0.60      0.59       626

    accuracy                           0.62      3589
   macro avg       0.55      0.52      0.53      3589
weighted avg       0.64      0.62      0.62      3589

correct 0.6244079130677069 
f1 0.5250694393478901 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:38,  1.43it/s]  4%|▎         | 2/56.072265625 [00:00<00:22,  2.35it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.86it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.29it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.47it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.72it/s] 12%|█▏        | 7/56.072265625 [00:02<00:12,  3.78it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.94it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.92it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.04it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.99it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.08it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  4.01it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.10it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.05it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.12it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  3.98it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.08it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.01it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.10it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.00it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  3.86it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  3.88it/s] 43%|████▎     | 24/56.072265625 [00:06<00:08,  4.00it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  3.97it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.07it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.03it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.11it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.04it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.11it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.04it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.12it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.06it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.13it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.07it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.08it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.00it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.08it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  4.02it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.10it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.03it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.11it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  4.03it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.11it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.04it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.12it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  4.05it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.13it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.02it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.11it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  4.07it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.15it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.10it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.17it/s] 98%|█████████▊| 55/56.072265625 [00:13<00:00,  4.10it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.16it/s]57it [00:14,  3.96it/s]                                  epoch 10 iter 0 sum correct 0.8125 loss 1.898435115814209
epoch 10 iter 1 sum correct 0.806640625 loss 1.898913860321045
epoch 10 iter 2 sum correct 0.810546875 loss 1.898559331893921
epoch 10 iter 3 sum correct 0.80859375 loss 1.8987441062927246
epoch 10 iter 4 sum correct 0.809375 loss 1.8986425399780273
epoch 10 iter 5 sum correct 0.814453125 loss 1.898341417312622
epoch 10 iter 6 sum correct 0.8063616071428571 loss 1.8989181518554688
epoch 10 iter 7 sum correct 0.80712890625 loss 1.8988451957702637
epoch 10 iter 8 sum correct 0.8068576388888888 loss 1.8988728523254395
epoch 10 iter 9 sum correct 0.80703125 loss 1.8988488912582397
epoch 10 iter 10 sum correct 0.8053977272727273 loss 1.8989523649215698
epoch 10 iter 11 sum correct 0.8035481770833334 loss 1.89907968044281
epoch 10 iter 12 sum correct 0.8022836538461539 loss 1.899158000946045
epoch 10 iter 13 sum correct 0.8017578125 loss 1.899187445640564
epoch 10 iter 14 sum correct 0.8001302083333334 loss 1.899306058883667
epoch 10 iter 15 sum correct 0.800048828125 loss 1.8992912769317627
epoch 10 iter 16 sum correct 0.7977941176470589 loss 1.8994578123092651
epoch 10 iter 17 sum correct 0.7969835069444444 loss 1.8995083570480347
epoch 10 iter 18 sum correct 0.7979029605263158 loss 1.8994389772415161
epoch 10 iter 19 sum correct 0.7974609375 loss 1.8994722366333008
epoch 10 iter 20 sum correct 0.7964099702380952 loss 1.8995503187179565
epoch 10 iter 21 sum correct 0.7964311079545454 loss 1.8995535373687744
epoch 10 iter 22 sum correct 0.7966202445652174 loss 1.8995314836502075
epoch 10 iter 23 sum correct 0.7962239583333334 loss 1.899528980255127
epoch 10 iter 24 sum correct 0.795546875 loss 1.899583101272583
epoch 10 iter 25 sum correct 0.7950721153846154 loss 1.899607539176941
epoch 10 iter 26 sum correct 0.7957175925925926 loss 1.8995487689971924
epoch 10 iter 27 sum correct 0.7967354910714286 loss 1.8994781970977783
epoch 10 iter 28 sum correct 0.7957974137931034 loss 1.8995463848114014
epoch 10 iter 29 sum correct 0.7974609375 loss 1.8994213342666626
epoch 10 iter 30 sum correct 0.7975680443548387 loss 1.8994112014770508
epoch 10 iter 31 sum correct 0.7979736328125 loss 1.8993747234344482
epoch 10 iter 32 sum correct 0.7986505681818182 loss 1.8993252515792847
epoch 10 iter 33 sum correct 0.7992302389705882 loss 1.8992807865142822
epoch 10 iter 34 sum correct 0.7989955357142857 loss 1.8992940187454224
epoch 10 iter 35 sum correct 0.7985568576388888 loss 1.8993208408355713
epoch 10 iter 36 sum correct 0.7989336993243243 loss 1.899287462234497
epoch 10 iter 37 sum correct 0.7984683388157895 loss 1.8993171453475952
epoch 10 iter 38 sum correct 0.7986778846153846 loss 1.8992959260940552
epoch 10 iter 39 sum correct 0.799267578125 loss 1.8992595672607422
epoch 10 iter 40 sum correct 0.7993521341463414 loss 1.899256706237793
epoch 10 iter 41 sum correct 0.7996186755952381 loss 1.8992146253585815
epoch 10 iter 42 sum correct 0.8004632994186046 loss 1.8991518020629883
epoch 10 iter 43 sum correct 0.80078125 loss 1.8991267681121826
epoch 10 iter 44 sum correct 0.8007378472222222 loss 1.8991204500198364
epoch 10 iter 45 sum correct 0.8000169836956522 loss 1.8991707563400269
epoch 10 iter 46 sum correct 0.800282579787234 loss 1.8991453647613525
epoch 10 iter 47 sum correct 0.800048828125 loss 1.8991546630859375
epoch 10 iter 48 sum correct 0.8008609693877551 loss 1.899097204208374
epoch 10 iter 49 sum correct 0.8014453125 loss 1.8990588188171387
epoch 10 iter 50 sum correct 0.8010876225490197 loss 1.8990718126296997
epoch 10 iter 51 sum correct 0.8007436899038461 loss 1.8990870714187622
epoch 10 iter 52 sum correct 0.8006706957547169 loss 1.8990952968597412
epoch 10 iter 53 sum correct 0.8006365740740741 loss 1.8990905284881592
epoch 10 iter 54 sum correct 0.8008877840909091 loss 1.8990720510482788
epoch 10 iter 55 sum correct 0.8015485491071429 loss 1.899025559425354
epoch 10 iter 56 sum correct 0.7882743969298246 loss 1.8992329835891724
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.75it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.45it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.94it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.15it/s]8it [00:01,  6.13it/s]                                 macro  0.5028659431417358
micro  0.6001671774867651
[[275   0  50  20  65  23  34]
 [ 29   0  10   4   6   3   4]
 [ 67   0 197  16 110  62  44]
 [ 31   0  34 726  45  28  31]
 [103   0 104  30 317  17  82]
 [ 14   0  32  15   7 343   4]
 [ 60   0  66  59 103  23 296]]
              precision    recall  f1-score   support

           0       0.47      0.59      0.53       467
           1       0.00      0.00      0.00        56
           2       0.40      0.40      0.40       496
           3       0.83      0.81      0.82       895
           4       0.49      0.49      0.49       653
           5       0.69      0.83      0.75       415
           6       0.60      0.49      0.54       607

    accuracy                           0.60      3589
   macro avg       0.50      0.51      0.50      3589
weighted avg       0.59      0.60      0.59      3589

correct 0.6001671774867651 
f1 0.5028659431417358 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.82it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.56it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.78it/s] 86%|████████▌ | 6/7.009765625 [00:01<00:00,  6.52it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.97it/s]8it [00:01,  5.95it/s]                                 macro  0.5181530036333405
micro  0.6177208135971023
[[294   0  48  24  63  24  38]
 [ 30   0   8   6   4   3   4]
 [ 78   0 247  22  86  60  35]
 [ 19   0  36 745  39  21  19]
 [ 94   0  81  31 297  11  80]
 [ 13   0  48  21  13 317   4]
 [ 58   0  66  58  98  29 317]]
              precision    recall  f1-score   support

           0       0.50      0.60      0.55       491
           1       0.00      0.00      0.00        55
           2       0.46      0.47      0.47       528
           3       0.82      0.85      0.83       879
           4       0.49      0.50      0.50       594
           5       0.68      0.76      0.72       416
           6       0.64      0.51      0.56       626

    accuracy                           0.62      3589
   macro avg       0.51      0.53      0.52      3589
weighted avg       0.61      0.62      0.61      3589

correct 0.6177208135971023 
f1 0.5181530036333405 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:38,  1.42it/s]  4%|▎         | 2/56.072265625 [00:00<00:22,  2.36it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.86it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.30it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.49it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.74it/s] 12%|█▏        | 7/56.072265625 [00:02<00:12,  3.79it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.95it/s] 16%|█▌        | 9/56.072265625 [00:02<00:11,  3.93it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.04it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.93it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.03it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  4.00it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.09it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.03it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.11it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.05it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.11it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.02it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.10it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.01it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.10it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.02it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.09it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.01it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.10it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.03it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.11it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.00it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.08it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.01it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.09it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.01it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.09it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.02it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.10it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.01it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.09it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  4.01it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.09it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.00it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.08it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  4.01it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.10it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.02it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.10it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  4.01it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.09it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.01it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.10it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  4.03it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.11it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.04it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.12it/s] 98%|█████████▊| 55/56.072265625 [00:13<00:00,  4.04it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.11it/s]57it [00:14,  4.88it/s]                                  57it [00:14,  3.95it/s]epoch 11 iter 0 sum correct 0.8203125 loss 1.897612452507019
epoch 11 iter 1 sum correct 0.8232421875 loss 1.8973840475082397
epoch 11 iter 2 sum correct 0.8157552083333334 loss 1.8978731632232666
epoch 11 iter 3 sum correct 0.81103515625 loss 1.8981014490127563
epoch 11 iter 4 sum correct 0.80859375 loss 1.8982692956924438
epoch 11 iter 5 sum correct 0.8082682291666666 loss 1.8982079029083252
epoch 11 iter 6 sum correct 0.8116629464285714 loss 1.897919774055481
epoch 11 iter 7 sum correct 0.809814453125 loss 1.898054838180542
epoch 11 iter 8 sum correct 0.8083767361111112 loss 1.898181676864624
epoch 11 iter 9 sum correct 0.8044921875 loss 1.8984626531600952
epoch 11 iter 10 sum correct 0.8050426136363636 loss 1.8984107971191406
epoch 11 iter 11 sum correct 0.80517578125 loss 1.898399829864502
epoch 11 iter 12 sum correct 0.8052884615384616 loss 1.898377537727356
epoch 11 iter 13 sum correct 0.8055245535714286 loss 1.8983749151229858
epoch 11 iter 14 sum correct 0.8046875 loss 1.8984203338623047
epoch 11 iter 15 sum correct 0.802978515625 loss 1.8985310792922974
epoch 11 iter 16 sum correct 0.8037683823529411 loss 1.8984781503677368
epoch 11 iter 17 sum correct 0.8049045138888888 loss 1.898386836051941
epoch 11 iter 18 sum correct 0.8038651315789473 loss 1.8984521627426147
epoch 11 iter 19 sum correct 0.80263671875 loss 1.8985337018966675
epoch 11 iter 20 sum correct 0.8035714285714286 loss 1.8984678983688354
epoch 11 iter 21 sum correct 0.8041548295454546 loss 1.8984241485595703
epoch 11 iter 22 sum correct 0.8048573369565217 loss 1.898373007774353
epoch 11 iter 23 sum correct 0.8052571614583334 loss 1.8983368873596191
epoch 11 iter 24 sum correct 0.805625 loss 1.8983190059661865
epoch 11 iter 25 sum correct 0.8046875 loss 1.8983912467956543
epoch 11 iter 26 sum correct 0.8052662037037037 loss 1.8983521461486816
epoch 11 iter 27 sum correct 0.8046875 loss 1.8984006643295288
epoch 11 iter 28 sum correct 0.804754849137931 loss 1.8984074592590332
epoch 11 iter 29 sum correct 0.8046875 loss 1.898413062095642
epoch 11 iter 30 sum correct 0.8060105846774194 loss 1.8983134031295776
epoch 11 iter 31 sum correct 0.80694580078125 loss 1.8982462882995605
epoch 11 iter 32 sum correct 0.8065814393939394 loss 1.8982700109481812
epoch 11 iter 33 sum correct 0.8060087316176471 loss 1.8983120918273926
epoch 11 iter 34 sum correct 0.8063058035714286 loss 1.898285150527954
epoch 11 iter 35 sum correct 0.8069118923611112 loss 1.8982374668121338
epoch 11 iter 36 sum correct 0.8079075168918919 loss 1.898151159286499
epoch 11 iter 37 sum correct 0.807874177631579 loss 1.8981496095657349
epoch 11 iter 38 sum correct 0.8079927884615384 loss 1.898138403892517
epoch 11 iter 39 sum correct 0.8087890625 loss 1.8980827331542969
epoch 11 iter 40 sum correct 0.8093083079268293 loss 1.8980411291122437
epoch 11 iter 41 sum correct 0.8095238095238095 loss 1.8980289697647095
epoch 11 iter 42 sum correct 0.8091388081395349 loss 1.8980432748794556
epoch 11 iter 43 sum correct 0.8085493607954546 loss 1.8980826139450073
epoch 11 iter 44 sum correct 0.8091145833333333 loss 1.8980449438095093
epoch 11 iter 45 sum correct 0.8093580163043478 loss 1.898024559020996
epoch 11 iter 46 sum correct 0.8090508643617021 loss 1.8980400562286377
epoch 11 iter 47 sum correct 0.8091634114583334 loss 1.8980355262756348
epoch 11 iter 48 sum correct 0.8095105229591837 loss 1.8980145454406738
epoch 11 iter 49 sum correct 0.8095703125 loss 1.8980098962783813
epoch 11 iter 50 sum correct 0.8099724264705882 loss 1.8979849815368652
epoch 11 iter 51 sum correct 0.81005859375 loss 1.8979761600494385
epoch 11 iter 52 sum correct 0.8099204009433962 loss 1.8979767560958862
epoch 11 iter 53 sum correct 0.8099681712962963 loss 1.8979682922363281
epoch 11 iter 54 sum correct 0.8098011363636364 loss 1.89797842502594
epoch 11 iter 55 sum correct 0.8101283482142857 loss 1.8979469537734985
epoch 11 iter 56 sum correct 0.7968064692982456 loss 1.898055076599121
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.67it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.29it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.54it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.73it/s]8it [00:01,  5.76it/s]                                 macro  0.5258977396445209
micro  0.6291446085260518
[[249   0  41  42  69  12  54]
 [ 20   0  10   8  11   1   6]
 [ 52   0 198  41 114  40  51]
 [ 15   0  12 789  14  18  47]
 [ 73   0  59  59 340  11 111]
 [ 11   0  34  22   9 331   8]
 [ 40   0  46  62 100   8 351]]
              precision    recall  f1-score   support

           0       0.54      0.53      0.54       467
           1       0.00      0.00      0.00        56
           2       0.49      0.40      0.44       496
           3       0.77      0.88      0.82       895
           4       0.52      0.52      0.52       653
           5       0.79      0.80      0.79       415
           6       0.56      0.58      0.57       607

    accuracy                           0.63      3589
   macro avg       0.52      0.53      0.53      3589
weighted avg       0.61      0.63      0.62      3589

correct 0.6291446085260518 
f1 0.5258977396445209 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.65it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.28it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.71it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.01it/s]8it [00:01,  5.94it/s]                                 macro  0.5282538252721386
micro  0.6319308999721371
[[262   0  51  44  63   6  65]
 [ 26   0   5   9   9   3   3]
 [ 77   0 228  30 111  43  39]
 [  9   0  24 794  20  11  21]
 [ 55   0  70  50 297   7 115]
 [ 10   0  44  32  16 302  12]
 [ 34   0  47  58  92  10 385]]
              precision    recall  f1-score   support

           0       0.55      0.53      0.54       491
           1       0.00      0.00      0.00        55
           2       0.49      0.43      0.46       528
           3       0.78      0.90      0.84       879
           4       0.49      0.50      0.49       594
           5       0.79      0.73      0.76       416
           6       0.60      0.62      0.61       626

    accuracy                           0.63      3589
   macro avg       0.53      0.53      0.53      3589
weighted avg       0.62      0.63      0.62      3589

correct 0.6319308999721371 
f1 0.5282538252721386 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:38,  1.42it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.35it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.85it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.29it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.46it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.71it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.77it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.93it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.90it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.02it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.97it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.07it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  3.99it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.08it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.00it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.09it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.00it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.09it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  3.99it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.07it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.00it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.08it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  3.99it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.06it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  3.99it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.08it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.00it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.08it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.00it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.09it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.00it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.09it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.01it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.09it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.01it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.09it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.02it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.10it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  4.01it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.10it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.04it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.11it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  4.02it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.10it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.02it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.10it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  4.01it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:02,  3.84it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  3.83it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  3.97it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  3.95it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:01,  4.06it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.03it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.12it/s] 98%|█████████▊| 55/56.072265625 [00:14<00:00,  4.06it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.13it/s]57it [00:14,  4.99it/s]                                  57it [00:14,  3.94it/s]epoch 12 iter 0 sum correct 0.798828125 loss 1.8986626863479614
epoch 12 iter 1 sum correct 0.8173828125 loss 1.8973419666290283
epoch 12 iter 2 sum correct 0.818359375 loss 1.8972759246826172
epoch 12 iter 3 sum correct 0.81396484375 loss 1.8974661827087402
epoch 12 iter 4 sum correct 0.8140625 loss 1.897448182106018
epoch 12 iter 5 sum correct 0.810546875 loss 1.8977020978927612
epoch 12 iter 6 sum correct 0.8094308035714286 loss 1.897746205329895
epoch 12 iter 7 sum correct 0.8154296875 loss 1.8973329067230225
epoch 12 iter 8 sum correct 0.8166232638888888 loss 1.89727783203125
epoch 12 iter 9 sum correct 0.81640625 loss 1.8972851037979126
epoch 12 iter 10 sum correct 0.8153409090909091 loss 1.8973345756530762
epoch 12 iter 11 sum correct 0.8154296875 loss 1.8973287343978882
epoch 12 iter 12 sum correct 0.8135516826923077 loss 1.8974614143371582
epoch 12 iter 13 sum correct 0.8136160714285714 loss 1.8974597454071045
epoch 12 iter 14 sum correct 0.8138020833333334 loss 1.8974274396896362
epoch 12 iter 15 sum correct 0.8150634765625 loss 1.897336483001709
epoch 12 iter 16 sum correct 0.8134191176470589 loss 1.897450566291809
epoch 12 iter 17 sum correct 0.8147786458333334 loss 1.8973475694656372
epoch 12 iter 18 sum correct 0.8160978618421053 loss 1.8972572088241577
epoch 12 iter 19 sum correct 0.81689453125 loss 1.897207260131836
epoch 12 iter 20 sum correct 0.8168712797619048 loss 1.8972108364105225
epoch 12 iter 21 sum correct 0.8172052556818182 loss 1.8971740007400513
epoch 12 iter 22 sum correct 0.8159816576086957 loss 1.8972394466400146
epoch 12 iter 23 sum correct 0.8168131510416666 loss 1.8971805572509766
epoch 12 iter 24 sum correct 0.817421875 loss 1.8971456289291382
epoch 12 iter 25 sum correct 0.8172325721153846 loss 1.8971623182296753
epoch 12 iter 26 sum correct 0.8165509259259259 loss 1.8971985578536987
epoch 12 iter 27 sum correct 0.8163364955357143 loss 1.8972136974334717
epoch 12 iter 28 sum correct 0.8159348060344828 loss 1.8972399234771729
epoch 12 iter 29 sum correct 0.8160807291666666 loss 1.897228717803955
epoch 12 iter 30 sum correct 0.8159022177419355 loss 1.8972363471984863
epoch 12 iter 31 sum correct 0.8170166015625 loss 1.8971495628356934
epoch 12 iter 32 sum correct 0.8168205492424242 loss 1.8971519470214844
epoch 12 iter 33 sum correct 0.8161764705882353 loss 1.8971883058547974
epoch 12 iter 34 sum correct 0.8166294642857143 loss 1.897152066230774
epoch 12 iter 35 sum correct 0.81591796875 loss 1.89719557762146
epoch 12 iter 36 sum correct 0.8158255912162162 loss 1.897202968597412
epoch 12 iter 37 sum correct 0.8154810855263158 loss 1.8972223997116089
epoch 12 iter 38 sum correct 0.8155048076923077 loss 1.897216796875
epoch 12 iter 39 sum correct 0.815625 loss 1.8972103595733643
epoch 12 iter 40 sum correct 0.8158822408536586 loss 1.897192120552063
epoch 12 iter 41 sum correct 0.8158017113095238 loss 1.8971939086914062
epoch 12 iter 42 sum correct 0.8157703488372093 loss 1.8971859216690063
epoch 12 iter 43 sum correct 0.8158735795454546 loss 1.8971757888793945
epoch 12 iter 44 sum correct 0.8162326388888889 loss 1.8971476554870605
epoch 12 iter 45 sum correct 0.8164487092391305 loss 1.8971281051635742
epoch 12 iter 46 sum correct 0.8167386968085106 loss 1.8971017599105835
epoch 12 iter 47 sum correct 0.81689453125 loss 1.897096037864685
epoch 12 iter 48 sum correct 0.8172831632653061 loss 1.8970674276351929
epoch 12 iter 49 sum correct 0.8169140625 loss 1.897088885307312
epoch 12 iter 50 sum correct 0.8170189950980392 loss 1.8970825672149658
epoch 12 iter 51 sum correct 0.8170823317307693 loss 1.8970704078674316
epoch 12 iter 52 sum correct 0.816627358490566 loss 1.8970905542373657
epoch 12 iter 53 sum correct 0.8164785879629629 loss 1.8971039056777954
epoch 12 iter 54 sum correct 0.8160511363636364 loss 1.8971364498138428
epoch 12 iter 55 sum correct 0.8161969866071429 loss 1.8971188068389893
epoch 12 iter 56 sum correct 0.8028371710526315 loss 1.8971775770187378
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.65it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.26it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.73it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.02it/s]8it [00:01,  5.92it/s]                                 macro  0.5129726881375677
micro  0.6085260518250208
[[253   0  25  21  88  17  63]
 [ 24   0  10   4  11   2   5]
 [ 54   0 173  16 120  67  66]
 [ 22   0  13 683  60  23  94]
 [ 60   0  64  16 357  21 135]
 [ 14   0  14  14  13 339  21]
 [ 38   0  26  30 125   9 379]]
              precision    recall  f1-score   support

           0       0.54      0.54      0.54       467
           1       0.00      0.00      0.00        56
           2       0.53      0.35      0.42       496
           3       0.87      0.76      0.81       895
           4       0.46      0.55      0.50       653
           5       0.71      0.82      0.76       415
           6       0.50      0.62      0.55       607

    accuracy                           0.61      3589
   macro avg       0.52      0.52      0.51      3589
weighted avg       0.61      0.61      0.60      3589

correct 0.6085260518250208 
f1 0.5129726881375677 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.76it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.44it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.97it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.21it/s]8it [00:01,  6.05it/s]                                 macro  0.5264377096755949
micro  0.6260796879353581
[[279   0  44  12  90  11  55]
 [ 22   0   5   4  16   3   5]
 [ 73   0 184  11 117  70  73]
 [ 19   0  13 708  75  16  48]
 [ 49   0  56   9 337  11 132]
 [ 11   0  12  26  20 321  26]
 [ 36   0  33  19 105  15 418]]
              precision    recall  f1-score   support

           0       0.57      0.57      0.57       491
           1       0.00      0.00      0.00        55
           2       0.53      0.35      0.42       528
           3       0.90      0.81      0.85       879
           4       0.44      0.57      0.50       594
           5       0.72      0.77      0.74       416
           6       0.55      0.67      0.60       626

    accuracy                           0.63      3589
   macro avg       0.53      0.53      0.53      3589
weighted avg       0.63      0.63      0.62      3589

correct 0.6260796879353581 
f1 0.5264377096755949 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:38,  1.45it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.35it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.85it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.28it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.45it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.68it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.75it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.90it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.90it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  3.99it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.97it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.05it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  3.96it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.04it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.00it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.07it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.00it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.07it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.00it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.07it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.03it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.10it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.04it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.11it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.04it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.11it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.05it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.09it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.03it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.10it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.03it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.10it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.04it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.10it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.04it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.10it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.04it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.11it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  4.03it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.09it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.03it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.10it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  4.02it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.09it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.03it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.10it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  4.05it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.10it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.04it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.11it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  4.04it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.10it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.06it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.12it/s] 98%|█████████▊| 55/56.072265625 [00:13<00:00,  4.05it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.11it/s]57it [00:14,  3.96it/s]                                  epoch 13 iter 0 sum correct 0.8359375 loss 1.8953410387039185
epoch 13 iter 1 sum correct 0.8330078125 loss 1.8957092761993408
epoch 13 iter 2 sum correct 0.8190104166666666 loss 1.8966388702392578
epoch 13 iter 3 sum correct 0.81005859375 loss 1.8972610235214233
epoch 13 iter 4 sum correct 0.809375 loss 1.897344946861267
epoch 13 iter 5 sum correct 0.8125 loss 1.8971103429794312
epoch 13 iter 6 sum correct 0.8136160714285714 loss 1.8970236778259277
epoch 13 iter 7 sum correct 0.81396484375 loss 1.897049069404602
epoch 13 iter 8 sum correct 0.8142361111111112 loss 1.8970330953598022
epoch 13 iter 9 sum correct 0.8125 loss 1.897186279296875
epoch 13 iter 10 sum correct 0.8110795454545454 loss 1.8972610235214233
epoch 13 iter 11 sum correct 0.8076171875 loss 1.8974885940551758
epoch 13 iter 12 sum correct 0.8073918269230769 loss 1.897486686706543
epoch 13 iter 13 sum correct 0.806640625 loss 1.8975518941879272
epoch 13 iter 14 sum correct 0.8069010416666667 loss 1.8975093364715576
epoch 13 iter 15 sum correct 0.8089599609375 loss 1.897345781326294
epoch 13 iter 16 sum correct 0.8099724264705882 loss 1.8972859382629395
epoch 13 iter 17 sum correct 0.8097873263888888 loss 1.897308111190796
epoch 13 iter 18 sum correct 0.8103412828947368 loss 1.8972833156585693
epoch 13 iter 19 sum correct 0.81064453125 loss 1.8972523212432861
epoch 13 iter 20 sum correct 0.8098958333333334 loss 1.8972933292388916
epoch 13 iter 21 sum correct 0.8084161931818182 loss 1.8973865509033203
epoch 13 iter 22 sum correct 0.8079144021739131 loss 1.897426962852478
epoch 13 iter 23 sum correct 0.8065592447916666 loss 1.8975180387496948
epoch 13 iter 24 sum correct 0.80703125 loss 1.89747953414917
epoch 13 iter 25 sum correct 0.8070913461538461 loss 1.8974764347076416
epoch 13 iter 26 sum correct 0.8068576388888888 loss 1.8974889516830444
epoch 13 iter 27 sum correct 0.8083147321428571 loss 1.8973853588104248
epoch 13 iter 28 sum correct 0.8084590517241379 loss 1.8973674774169922
epoch 13 iter 29 sum correct 0.8088541666666667 loss 1.8973290920257568
epoch 13 iter 30 sum correct 0.8079637096774194 loss 1.8973937034606934
epoch 13 iter 31 sum correct 0.80877685546875 loss 1.8973273038864136
epoch 13 iter 32 sum correct 0.8084753787878788 loss 1.8973464965820312
epoch 13 iter 33 sum correct 0.8083639705882353 loss 1.897351861000061
epoch 13 iter 34 sum correct 0.8087611607142857 loss 1.8973201513290405
epoch 13 iter 35 sum correct 0.8085394965277778 loss 1.8973287343978882
epoch 13 iter 36 sum correct 0.8084881756756757 loss 1.897322177886963
epoch 13 iter 37 sum correct 0.8090049342105263 loss 1.8972781896591187
epoch 13 iter 38 sum correct 0.8095452724358975 loss 1.897233247756958
epoch 13 iter 39 sum correct 0.8091796875 loss 1.8972638845443726
epoch 13 iter 40 sum correct 0.8098799542682927 loss 1.8972070217132568
epoch 13 iter 41 sum correct 0.8100818452380952 loss 1.8971924781799316
epoch 13 iter 42 sum correct 0.811046511627907 loss 1.8971149921417236
epoch 13 iter 43 sum correct 0.8111239346590909 loss 1.8971110582351685
epoch 13 iter 44 sum correct 0.8110677083333333 loss 1.8971134424209595
epoch 13 iter 45 sum correct 0.810929008152174 loss 1.8971178531646729
epoch 13 iter 46 sum correct 0.8117104388297872 loss 1.8970643281936646
epoch 13 iter 47 sum correct 0.8114420572916666 loss 1.8970792293548584
epoch 13 iter 48 sum correct 0.8115035076530612 loss 1.8970673084259033
epoch 13 iter 49 sum correct 0.8110546875 loss 1.8970919847488403
epoch 13 iter 50 sum correct 0.8107383578431373 loss 1.8971178531646729
epoch 13 iter 51 sum correct 0.8109975961538461 loss 1.8971033096313477
epoch 13 iter 52 sum correct 0.810767983490566 loss 1.8971174955368042
epoch 13 iter 53 sum correct 0.8117042824074074 loss 1.8970452547073364
epoch 13 iter 54 sum correct 0.8118963068181818 loss 1.8970223665237427
epoch 13 iter 55 sum correct 0.8115583147321429 loss 1.8970457315444946
epoch 13 iter 56 sum correct 0.7982456140350878 loss 1.8971325159072876
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.70it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.39it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.81it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.96it/s]8it [00:01,  5.92it/s]                                 macro  0.5140710134624255
micro  0.6138200055725829
[[246   0  53  44  71  14  39]
 [ 18   0  17   5   9   0   7]
 [ 48   0 228  36  97  43  44]
 [ 19   0  20 794  16  18  28]
 [ 75   0  92  85 307   5  89]
 [ 16   0  34  24  10 322   9]
 [ 43   0  47  95 108   8 306]]
              precision    recall  f1-score   support

           0       0.53      0.53      0.53       467
           1       0.00      0.00      0.00        56
           2       0.46      0.46      0.46       496
           3       0.73      0.89      0.80       895
           4       0.50      0.47      0.48       653
           5       0.79      0.78      0.78       415
           6       0.59      0.50      0.54       607

    accuracy                           0.61      3589
   macro avg       0.51      0.52      0.51      3589
weighted avg       0.60      0.61      0.60      3589

correct 0.6138200055725829 
f1 0.5140710134624255 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.78it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.48it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.77it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.02it/s]8it [00:01,  5.99it/s]                                 macro  0.5301933984720477
micro  0.6324881582613542
[[259   0  77  44  59   6  46]
 [ 22   0  12  10   8   2   1]
 [ 60   0 261  32  87  40  48]
 [ 19   0  13 801  18  14  14]
 [ 58   0  74  66 297   7  92]
 [ 15   0  36  29  13 313  10]
 [ 47   0  52  88  89  11 339]]
              precision    recall  f1-score   support

           0       0.54      0.53      0.53       491
           1       0.00      0.00      0.00        55
           2       0.50      0.49      0.50       528
           3       0.75      0.91      0.82       879
           4       0.52      0.50      0.51       594
           5       0.80      0.75      0.77       416
           6       0.62      0.54      0.58       626

    accuracy                           0.63      3589
   macro avg       0.53      0.53      0.53      3589
weighted avg       0.62      0.63      0.62      3589

correct 0.6324881582613542 
f1 0.5301933984720477 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:39,  1.40it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.29it/s]  5%|▌         | 3/56.072265625 [00:01<00:19,  2.78it/s]  7%|▋         | 4/56.072265625 [00:01<00:16,  3.22it/s]  9%|▉         | 5/56.072265625 [00:01<00:15,  3.38it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.65it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.73it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.90it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.86it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  3.98it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.94it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.04it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  3.98it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.07it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.00it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.03it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  3.94it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.04it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  3.98it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.06it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.00it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.08it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.01it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.07it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.01it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.09it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.01it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.03it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  3.98it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.06it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.01it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.05it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  3.95it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.05it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  3.95it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.03it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  3.94it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.04it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  3.97it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.05it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  3.98it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.07it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  4.00it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.07it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.00it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.08it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  4.00it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.04it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  3.96it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.05it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  4.00it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.08it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.00it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.09it/s] 98%|█████████▊| 55/56.072265625 [00:14<00:00,  4.04it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.10it/s]57it [00:14,  4.88it/s]                                  57it [00:14,  3.92it/s]epoch 14 iter 0 sum correct 0.833984375 loss 1.8952032327651978
epoch 14 iter 1 sum correct 0.8310546875 loss 1.8955583572387695
epoch 14 iter 2 sum correct 0.8294270833333334 loss 1.895674228668213
epoch 14 iter 3 sum correct 0.828125 loss 1.8956717252731323
epoch 14 iter 4 sum correct 0.821875 loss 1.8961187601089478
epoch 14 iter 5 sum correct 0.8173828125 loss 1.896466612815857
epoch 14 iter 6 sum correct 0.8186383928571429 loss 1.8964300155639648
epoch 14 iter 7 sum correct 0.819580078125 loss 1.8963303565979004
epoch 14 iter 8 sum correct 0.8185763888888888 loss 1.8963764905929565
epoch 14 iter 9 sum correct 0.818359375 loss 1.8963717222213745
epoch 14 iter 10 sum correct 0.8178267045454546 loss 1.8964194059371948
epoch 14 iter 11 sum correct 0.81591796875 loss 1.8965504169464111
epoch 14 iter 12 sum correct 0.8168569711538461 loss 1.8964709043502808
epoch 14 iter 13 sum correct 0.8150111607142857 loss 1.8966184854507446
epoch 14 iter 14 sum correct 0.8153645833333333 loss 1.896591305732727
epoch 14 iter 15 sum correct 0.8148193359375 loss 1.8966262340545654
epoch 14 iter 16 sum correct 0.8146829044117647 loss 1.8966420888900757
epoch 14 iter 17 sum correct 0.8139105902777778 loss 1.8966827392578125
epoch 14 iter 18 sum correct 0.8136307565789473 loss 1.8966952562332153
epoch 14 iter 19 sum correct 0.813671875 loss 1.8966890573501587
epoch 14 iter 20 sum correct 0.8135230654761905 loss 1.8966954946517944
epoch 14 iter 21 sum correct 0.8143643465909091 loss 1.896632194519043
epoch 14 iter 22 sum correct 0.8141134510869565 loss 1.8966529369354248
epoch 14 iter 23 sum correct 0.81396484375 loss 1.8966617584228516
epoch 14 iter 24 sum correct 0.814296875 loss 1.8966606855392456
epoch 14 iter 25 sum correct 0.8154296875 loss 1.8965795040130615
epoch 14 iter 26 sum correct 0.81640625 loss 1.8965122699737549
epoch 14 iter 27 sum correct 0.8163364955357143 loss 1.8965123891830444
epoch 14 iter 28 sum correct 0.81640625 loss 1.8965121507644653
epoch 14 iter 29 sum correct 0.8159505208333333 loss 1.8965463638305664
epoch 14 iter 30 sum correct 0.8162172379032258 loss 1.8965134620666504
epoch 14 iter 31 sum correct 0.815185546875 loss 1.8965860605239868
epoch 14 iter 32 sum correct 0.8156960227272727 loss 1.8965548276901245
epoch 14 iter 33 sum correct 0.8152573529411765 loss 1.8965799808502197
epoch 14 iter 34 sum correct 0.8159598214285714 loss 1.896523356437683
epoch 14 iter 35 sum correct 0.8166232638888888 loss 1.89646315574646
epoch 14 iter 36 sum correct 0.8165646114864865 loss 1.8964699506759644
epoch 14 iter 37 sum correct 0.8163548519736842 loss 1.8964858055114746
epoch 14 iter 38 sum correct 0.8160056089743589 loss 1.8965023756027222
epoch 14 iter 39 sum correct 0.81591796875 loss 1.8965071439743042
epoch 14 iter 40 sum correct 0.8165491615853658 loss 1.896454930305481
epoch 14 iter 41 sum correct 0.8172433035714286 loss 1.896398901939392
epoch 14 iter 42 sum correct 0.8184502180232558 loss 1.8963037729263306
epoch 14 iter 43 sum correct 0.8186257102272727 loss 1.8962821960449219
epoch 14 iter 44 sum correct 0.819921875 loss 1.8961807489395142
epoch 14 iter 45 sum correct 0.8198454483695652 loss 1.8961787223815918
epoch 14 iter 46 sum correct 0.8199384973404256 loss 1.896170735359192
epoch 14 iter 47 sum correct 0.8200276692708334 loss 1.8961598873138428
epoch 14 iter 48 sum correct 0.8201530612244898 loss 1.8961375951766968
epoch 14 iter 49 sum correct 0.8200390625 loss 1.8961400985717773
epoch 14 iter 50 sum correct 0.8200444240196079 loss 1.8961373567581177
epoch 14 iter 51 sum correct 0.8200120192307693 loss 1.8961377143859863
epoch 14 iter 52 sum correct 0.8203862028301887 loss 1.8961070775985718
epoch 14 iter 53 sum correct 0.8205656828703703 loss 1.896089792251587
epoch 14 iter 54 sum correct 0.8202414772727272 loss 1.8961135149002075
epoch 14 iter 55 sum correct 0.8199637276785714 loss 1.8961347341537476
epoch 14 iter 56 sum correct 0.8064350328947368 loss 1.8963338136672974
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.64it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.24it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.81it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.04it/s]8it [00:01,  5.94it/s]                                 macro  0.5012297082796395
micro  0.5945945945945946
[[274   0  79  24  46  19  25]
 [ 28   0  15   3   9   0   1]
 [ 62   0 280  17  63  50  24]
 [ 41   0  46 716  25  38  29]
 [106   0 157  41 275  14  60]
 [ 16   0  38  14   7 335   5]
 [ 91   0  76  70  99  17 254]]
              precision    recall  f1-score   support

           0       0.44      0.59      0.51       467
           1       0.00      0.00      0.00        56
           2       0.41      0.56      0.47       496
           3       0.81      0.80      0.80       895
           4       0.52      0.42      0.47       653
           5       0.71      0.81      0.75       415
           6       0.64      0.42      0.51       607

    accuracy                           0.59      3589
   macro avg       0.50      0.51      0.50      3589
weighted avg       0.60      0.59      0.59      3589

correct 0.5945945945945946 
f1 0.5012297082796395 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.76it/s] 29%|██▊       | 2/7.009765625 [00:00<00:01,  3.39it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.59it/s] 57%|█████▋    | 4/7.009765625 [00:00<00:00,  5.84it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  6.41it/s] 86%|████████▌ | 6/7.009765625 [00:01<00:00,  7.28it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.56it/s]8it [00:01,  5.85it/s]                                 macro  0.517037097282728
micro  0.6135413764279743
[[281   0  97  20  54   5  34]
 [ 28   0  15   3   6   2   1]
 [ 72   0 288  19  69  56  24]
 [ 38   0  32 738  30  30  11]
 [ 86   0 136  32 275  14  51]
 [ 14   0  42  20  10 325   5]
 [ 89   0  73  47  97  25 295]]
              precision    recall  f1-score   support

           0       0.46      0.57      0.51       491
           1       0.00      0.00      0.00        55
           2       0.42      0.55      0.48       528
           3       0.84      0.84      0.84       879
           4       0.51      0.46      0.48       594
           5       0.71      0.78      0.74       416
           6       0.70      0.47      0.56       626

    accuracy                           0.61      3589
   macro avg       0.52      0.52      0.52      3589
weighted avg       0.62      0.61      0.61      3589

correct 0.6135413764279743 
f1 0.517037097282728 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:38,  1.42it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.33it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.85it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.29it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.47it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.71it/s] 12%|█▏        | 7/56.072265625 [00:02<00:12,  3.78it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.94it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.92it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.04it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.99it/s] 21%|██▏       | 12/56.072265625 [00:03<00:11,  3.85it/s] 23%|██▎       | 13/56.072265625 [00:03<00:11,  3.87it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  3.99it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  3.97it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.06it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.02it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.10it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.05it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.13it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  3.98it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.02it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  3.95it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.04it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  3.97it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.05it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.01it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.09it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.01it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.09it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.00it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.08it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.04it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.11it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.06it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.13it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.06it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.13it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  4.05it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.12it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.06it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.14it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  4.07it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.14it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.06it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.13it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  4.07it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.14it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.07it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.15it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  4.08it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.15it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.10it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.17it/s] 98%|█████████▊| 55/56.072265625 [00:13<00:00,  4.12it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.18it/s]57it [00:14,  5.06it/s]                                  57it [00:14,  3.96it/s]epoch 15 iter 0 sum correct 0.818359375 loss 1.8958613872528076
epoch 15 iter 1 sum correct 0.8291015625 loss 1.895164966583252
epoch 15 iter 2 sum correct 0.8333333333333334 loss 1.8948414325714111
epoch 15 iter 3 sum correct 0.82666015625 loss 1.8953176736831665
epoch 15 iter 4 sum correct 0.8265625 loss 1.8953344821929932
epoch 15 iter 5 sum correct 0.8277994791666666 loss 1.895229697227478
epoch 15 iter 6 sum correct 0.8236607142857143 loss 1.8955169916152954
epoch 15 iter 7 sum correct 0.822998046875 loss 1.8955897092819214
epoch 15 iter 8 sum correct 0.8196614583333334 loss 1.8958415985107422
epoch 15 iter 9 sum correct 0.81875 loss 1.8958901166915894
epoch 15 iter 10 sum correct 0.8172940340909091 loss 1.8960298299789429
epoch 15 iter 11 sum correct 0.8181966145833334 loss 1.8959732055664062
epoch 15 iter 12 sum correct 0.8203125 loss 1.895821452140808
epoch 15 iter 13 sum correct 0.8198939732142857 loss 1.895866870880127
epoch 15 iter 14 sum correct 0.8209635416666666 loss 1.8958109617233276
epoch 15 iter 15 sum correct 0.822509765625 loss 1.8957206010818481
epoch 15 iter 16 sum correct 0.8235294117647058 loss 1.8956336975097656
epoch 15 iter 17 sum correct 0.8235677083333334 loss 1.8956443071365356
epoch 15 iter 18 sum correct 0.8239103618421053 loss 1.895618200302124
epoch 15 iter 19 sum correct 0.8232421875 loss 1.895664095878601
epoch 15 iter 20 sum correct 0.8226376488095238 loss 1.8957116603851318
epoch 15 iter 21 sum correct 0.8218217329545454 loss 1.8957608938217163
epoch 15 iter 22 sum correct 0.8215013586956522 loss 1.8957958221435547
epoch 15 iter 23 sum correct 0.8212890625 loss 1.895824909210205
epoch 15 iter 24 sum correct 0.82203125 loss 1.895772933959961
epoch 15 iter 25 sum correct 0.8221905048076923 loss 1.8957455158233643
epoch 15 iter 26 sum correct 0.8219039351851852 loss 1.8957648277282715
epoch 15 iter 27 sum correct 0.8215680803571429 loss 1.8957782983779907
epoch 15 iter 28 sum correct 0.8220635775862069 loss 1.8957421779632568
epoch 15 iter 29 sum correct 0.8219401041666666 loss 1.8957574367523193
epoch 15 iter 30 sum correct 0.8218245967741935 loss 1.8957698345184326
epoch 15 iter 31 sum correct 0.822509765625 loss 1.8957281112670898
epoch 15 iter 32 sum correct 0.8232717803030303 loss 1.8956637382507324
epoch 15 iter 33 sum correct 0.8231272977941176 loss 1.89567232131958
epoch 15 iter 34 sum correct 0.8228794642857142 loss 1.8956913948059082
epoch 15 iter 35 sum correct 0.8230794270833334 loss 1.8956753015518188
epoch 15 iter 36 sum correct 0.8231102195945946 loss 1.8956711292266846
epoch 15 iter 37 sum correct 0.8230365953947368 loss 1.8956718444824219
epoch 15 iter 38 sum correct 0.8227163461538461 loss 1.8956884145736694
epoch 15 iter 39 sum correct 0.822802734375 loss 1.895678162574768
epoch 15 iter 40 sum correct 0.823218368902439 loss 1.895642876625061
epoch 15 iter 41 sum correct 0.8230561755952381 loss 1.895646572113037
epoch 15 iter 42 sum correct 0.8228561046511628 loss 1.8956575393676758
epoch 15 iter 43 sum correct 0.8233753551136364 loss 1.8956068754196167
epoch 15 iter 44 sum correct 0.8233506944444444 loss 1.8956146240234375
epoch 15 iter 45 sum correct 0.822647758152174 loss 1.8956602811813354
epoch 15 iter 46 sum correct 0.8228474069148937 loss 1.8956379890441895
epoch 15 iter 47 sum correct 0.8228352864583334 loss 1.8956341743469238
epoch 15 iter 48 sum correct 0.8223852040816326 loss 1.895658016204834
epoch 15 iter 49 sum correct 0.822734375 loss 1.8956338167190552
epoch 15 iter 50 sum correct 0.8223422181372549 loss 1.8956623077392578
epoch 15 iter 51 sum correct 0.8224534254807693 loss 1.8956562280654907
epoch 15 iter 52 sum correct 0.8227446933962265 loss 1.8956354856491089
epoch 15 iter 53 sum correct 0.8223379629629629 loss 1.8956581354141235
epoch 15 iter 54 sum correct 0.8224786931818182 loss 1.8956432342529297
epoch 15 iter 55 sum correct 0.8224051339285714 loss 1.895646333694458
epoch 15 iter 56 sum correct 0.8089021381578947 loss 1.8957520723342896
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.65it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.15it/s] 57%|█████▋    | 4/7.009765625 [00:00<00:00,  5.17it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.74it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.15it/s]8it [00:01,  5.71it/s]                                 macro  0.5215432008583699
micro  0.6146558930064084
[[268   0  66  16  68  12  37]
 [ 25   0  13   3   9   0   6]
 [ 59   0 235  10  99  45  48]
 [ 42   0  26 698  35  26  68]
 [ 62   0 106  24 341  12 108]
 [ 21   0  28  16  14 328   8]
 [ 51   0  63  36 105  16 336]]
              precision    recall  f1-score   support

           0       0.51      0.57      0.54       467
           1       0.00      0.00      0.00        56
           2       0.44      0.47      0.45       496
           3       0.87      0.78      0.82       895
           4       0.51      0.52      0.52       653
           5       0.75      0.79      0.77       415
           6       0.55      0.55      0.55       607

    accuracy                           0.61      3589
   macro avg       0.52      0.53      0.52      3589
weighted avg       0.62      0.61      0.61      3589

correct 0.6146558930064084 
f1 0.5215432008583699 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.72it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.39it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.62it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.91it/s]8it [00:01,  5.92it/s]                                 macro  0.5323872042758084
micro  0.6291446085260518
[[265   0  90  12  71   5  48]
 [ 21   0  16   2  13   3   0]
 [ 69   0 246  13  99  54  47]
 [ 40   0  25 723  41  17  33]
 [ 49   0 104  24 326  11  80]
 [ 15   0  32  21  14 319  15]
 [ 50   0  59  20 107  11 379]]
              precision    recall  f1-score   support

           0       0.52      0.54      0.53       491
           1       0.00      0.00      0.00        55
           2       0.43      0.47      0.45       528
           3       0.89      0.82      0.85       879
           4       0.49      0.55      0.52       594
           5       0.76      0.77      0.76       416
           6       0.63      0.61      0.62       626

    accuracy                           0.63      3589
   macro avg       0.53      0.54      0.53      3589
weighted avg       0.63      0.63      0.63      3589

correct 0.6291446085260518 
f1 0.5323872042758084 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:38,  1.44it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.35it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.86it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.30it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.49it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.73it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.76it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.92it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.91it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.02it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.99it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.09it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  3.95it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.03it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  3.93it/s] 29%|██▊       | 16/56.072265625 [00:04<00:10,  3.96it/s] 30%|███       | 17/56.072265625 [00:04<00:10,  3.86it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  3.93it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  3.85it/s] 36%|███▌      | 20/56.072265625 [00:05<00:09,  3.96it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  3.95it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.05it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.01it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.10it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.04it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.05it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.01it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.02it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  3.92it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  3.97it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  3.88it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:06,  3.95it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  3.87it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  3.92it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  3.86it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:05,  3.93it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  3.85it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  3.92it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  3.92it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.03it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  3.98it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.07it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  3.95it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:03,  4.00it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  3.92it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.02it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  3.91it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:02,  3.97it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  3.89it/s] 89%|████████▉ | 50/56.072265625 [00:13<00:01,  3.98it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  3.98it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.07it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.04it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.11it/s] 98%|█████████▊| 55/56.072265625 [00:14<00:00,  4.02it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.10it/s]57it [00:14,  3.89it/s]                                  epoch 16 iter 0 sum correct 0.833984375 loss 1.8944636583328247
epoch 16 iter 1 sum correct 0.8310546875 loss 1.8946468830108643
epoch 16 iter 2 sum correct 0.8170572916666666 loss 1.895708441734314
epoch 16 iter 3 sum correct 0.81787109375 loss 1.8956705331802368
epoch 16 iter 4 sum correct 0.816796875 loss 1.8957948684692383
epoch 16 iter 5 sum correct 0.8186848958333334 loss 1.8956278562545776
epoch 16 iter 6 sum correct 0.8147321428571429 loss 1.8959391117095947
epoch 16 iter 7 sum correct 0.81787109375 loss 1.8957574367523193
epoch 16 iter 8 sum correct 0.8187934027777778 loss 1.8956748247146606
epoch 16 iter 9 sum correct 0.818359375 loss 1.8956669569015503
epoch 16 iter 10 sum correct 0.8165838068181818 loss 1.8958280086517334
epoch 16 iter 11 sum correct 0.8181966145833334 loss 1.89573335647583
epoch 16 iter 12 sum correct 0.8195612980769231 loss 1.8956496715545654
epoch 16 iter 13 sum correct 0.8219866071428571 loss 1.8954696655273438
epoch 16 iter 14 sum correct 0.8225260416666667 loss 1.8953920602798462
epoch 16 iter 15 sum correct 0.8231201171875 loss 1.8953478336334229
epoch 16 iter 16 sum correct 0.8247931985294118 loss 1.8952174186706543
epoch 16 iter 17 sum correct 0.8267144097222222 loss 1.8950867652893066
epoch 16 iter 18 sum correct 0.827405427631579 loss 1.8950327634811401
epoch 16 iter 19 sum correct 0.829296875 loss 1.8949003219604492
epoch 16 iter 20 sum correct 0.8295200892857143 loss 1.8948793411254883
epoch 16 iter 21 sum correct 0.8283913352272727 loss 1.8949658870697021
epoch 16 iter 22 sum correct 0.8282099184782609 loss 1.8949716091156006
epoch 16 iter 23 sum correct 0.8280436197916666 loss 1.8949822187423706
epoch 16 iter 24 sum correct 0.8290625 loss 1.8949048519134521
epoch 16 iter 25 sum correct 0.8290264423076923 loss 1.8949092626571655
epoch 16 iter 26 sum correct 0.8288483796296297 loss 1.8949244022369385
epoch 16 iter 27 sum correct 0.8281947544642857 loss 1.8949692249298096
epoch 16 iter 28 sum correct 0.8292699353448276 loss 1.8948909044265747
epoch 16 iter 29 sum correct 0.8301432291666667 loss 1.8948166370391846
epoch 16 iter 30 sum correct 0.830078125 loss 1.8948179483413696
epoch 16 iter 31 sum correct 0.82940673828125 loss 1.8948643207550049
epoch 16 iter 32 sum correct 0.8301964962121212 loss 1.894806146621704
epoch 16 iter 33 sum correct 0.8307100183823529 loss 1.8947672843933105
epoch 16 iter 34 sum correct 0.8299107142857143 loss 1.8948174715042114
epoch 16 iter 35 sum correct 0.8299153645833334 loss 1.8948112726211548
epoch 16 iter 36 sum correct 0.830078125 loss 1.8947908878326416
epoch 16 iter 37 sum correct 0.8301809210526315 loss 1.894784688949585
epoch 16 iter 38 sum correct 0.8294771634615384 loss 1.894838571548462
epoch 16 iter 39 sum correct 0.8302734375 loss 1.894776701927185
epoch 16 iter 40 sum correct 0.830078125 loss 1.8947840929031372
epoch 16 iter 41 sum correct 0.8312406994047619 loss 1.8946938514709473
epoch 16 iter 42 sum correct 0.831031976744186 loss 1.8947041034698486
epoch 16 iter 43 sum correct 0.8303444602272727 loss 1.8947547674179077
epoch 16 iter 44 sum correct 0.8308159722222223 loss 1.89472234249115
epoch 16 iter 45 sum correct 0.830672554347826 loss 1.8947304487228394
epoch 16 iter 46 sum correct 0.8307845744680851 loss 1.8947159051895142
epoch 16 iter 47 sum correct 0.8308919270833334 loss 1.8947017192840576
epoch 16 iter 48 sum correct 0.8305165816326531 loss 1.8947237730026245
epoch 16 iter 49 sum correct 0.8303515625 loss 1.8947346210479736
epoch 16 iter 50 sum correct 0.8303462009803921 loss 1.8947265148162842
epoch 16 iter 51 sum correct 0.8304161658653846 loss 1.8947176933288574
epoch 16 iter 52 sum correct 0.8305203419811321 loss 1.8947046995162964
epoch 16 iter 53 sum correct 0.8298249421296297 loss 1.894753336906433
epoch 16 iter 54 sum correct 0.829403409090909 loss 1.8947824239730835
epoch 16 iter 55 sum correct 0.8291015625 loss 1.8948091268539429
epoch 16 iter 56 sum correct 0.8154125548245614 loss 1.8950096368789673
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.71it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.39it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.71it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.92it/s]8it [00:01,  5.85it/s]                                 macro  0.528474016217464
micro  0.6244079130677069
[[274   0  74  24  49  11  35]
 [ 31   0  12   4   6   0   3]
 [ 51   0 262  16  90  34  43]
 [ 28   0  24 754  30  17  42]
 [ 64   0 121  47 309  10 102]
 [ 12   0  49  16   7 324   7]
 [ 56   0  56  69  98  10 318]]
              precision    recall  f1-score   support

           0       0.53      0.59      0.56       467
           1       0.00      0.00      0.00        56
           2       0.44      0.53      0.48       496
           3       0.81      0.84      0.83       895
           4       0.52      0.47      0.50       653
           5       0.80      0.78      0.79       415
           6       0.58      0.52      0.55       607

    accuracy                           0.62      3589
   macro avg       0.53      0.53      0.53      3589
weighted avg       0.62      0.62      0.62      3589

correct 0.6244079130677069 
f1 0.528474016217464 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.76it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.30it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.84it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.13it/s]8it [00:01,  6.03it/s]                                 macro  0.5379173863941699
micro  0.636667595430482
[[301   0  85  21  42   9  33]
 [ 29   0  10   6   7   2   1]
 [ 66   0 273  22  80  48  39]
 [ 25   0  19 754  39  18  24]
 [ 57   0 116  31 290   7  93]
 [ 10   0  51  20   8 315  12]
 [ 61   0  70  46  85  12 352]]
              precision    recall  f1-score   support

           0       0.55      0.61      0.58       491
           1       0.00      0.00      0.00        55
           2       0.44      0.52      0.47       528
           3       0.84      0.86      0.85       879
           4       0.53      0.49      0.51       594
           5       0.77      0.76      0.76       416
           6       0.64      0.56      0.60       626

    accuracy                           0.64      3589
   macro avg       0.54      0.54      0.54      3589
weighted avg       0.63      0.64      0.63      3589

correct 0.636667595430482 
f1 0.5379173863941699 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:39,  1.38it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.30it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.81it/s]  7%|▋         | 4/56.072265625 [00:01<00:16,  3.25it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.43it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.69it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.77it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.91it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.90it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.00it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.97it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.05it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  3.99it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.01it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  3.98it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.07it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  3.99it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.08it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.02it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.11it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.05it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.13it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.07it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.13it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.02it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  3.86it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  3.76it/s] 50%|████▉     | 28/56.072265625 [00:07<00:07,  3.90it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  3.92it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.03it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  3.96it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.05it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  3.99it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.06it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  3.99it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.07it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.01it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.09it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  4.04it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.12it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.05it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.13it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  4.00it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.09it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.03it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.11it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  4.06it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.13it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.03it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.10it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  4.07it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.15it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.08it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.16it/s] 98%|█████████▊| 55/56.072265625 [00:14<00:00,  4.09it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.16it/s]57it [00:14,  3.94it/s]                                  epoch 17 iter 0 sum correct 0.837890625 loss 1.8941091299057007
epoch 17 iter 1 sum correct 0.82421875 loss 1.8950412273406982
epoch 17 iter 2 sum correct 0.8352864583333334 loss 1.894197940826416
epoch 17 iter 3 sum correct 0.83837890625 loss 1.8939435482025146
epoch 17 iter 4 sum correct 0.84140625 loss 1.893709421157837
epoch 17 iter 5 sum correct 0.8421223958333334 loss 1.8936904668807983
epoch 17 iter 6 sum correct 0.837890625 loss 1.8940176963806152
epoch 17 iter 7 sum correct 0.838623046875 loss 1.8939414024353027
epoch 17 iter 8 sum correct 0.8413628472222222 loss 1.8937574625015259
epoch 17 iter 9 sum correct 0.8427734375 loss 1.8936604261398315
epoch 17 iter 10 sum correct 0.8425071022727273 loss 1.8936748504638672
epoch 17 iter 11 sum correct 0.8439127604166666 loss 1.8935693502426147
epoch 17 iter 12 sum correct 0.8460036057692307 loss 1.893412709236145
epoch 17 iter 13 sum correct 0.8473772321428571 loss 1.8932852745056152
epoch 17 iter 14 sum correct 0.8497395833333333 loss 1.893107533454895
epoch 17 iter 15 sum correct 0.8502197265625 loss 1.893069863319397
epoch 17 iter 16 sum correct 0.8513327205882353 loss 1.892974615097046
epoch 17 iter 17 sum correct 0.8513454861111112 loss 1.8929693698883057
epoch 17 iter 18 sum correct 0.852282072368421 loss 1.8929208517074585
epoch 17 iter 19 sum correct 0.85146484375 loss 1.8929678201675415
epoch 17 iter 20 sum correct 0.8515625 loss 1.8929585218429565
epoch 17 iter 21 sum correct 0.8516512784090909 loss 1.8929532766342163
epoch 17 iter 22 sum correct 0.8507982336956522 loss 1.8930201530456543
epoch 17 iter 23 sum correct 0.8524576822916666 loss 1.892913579940796
epoch 17 iter 24 sum correct 0.85203125 loss 1.8929417133331299
epoch 17 iter 25 sum correct 0.8517878605769231 loss 1.8929637670516968
epoch 17 iter 26 sum correct 0.8513454861111112 loss 1.8929862976074219
epoch 17 iter 27 sum correct 0.8506556919642857 loss 1.8930444717407227
epoch 17 iter 28 sum correct 0.8510910560344828 loss 1.8930197954177856
epoch 17 iter 29 sum correct 0.8505859375 loss 1.8930492401123047
epoch 17 iter 30 sum correct 0.8501134072580645 loss 1.8930823802947998
epoch 17 iter 31 sum correct 0.84906005859375 loss 1.8931584358215332
epoch 17 iter 32 sum correct 0.8486032196969697 loss 1.8931944370269775
epoch 17 iter 33 sum correct 0.8489774816176471 loss 1.8931548595428467
epoch 17 iter 34 sum correct 0.8487165178571429 loss 1.8931676149368286
epoch 17 iter 35 sum correct 0.8481987847222222 loss 1.8932069540023804
epoch 17 iter 36 sum correct 0.8485008445945946 loss 1.8931831121444702
epoch 17 iter 37 sum correct 0.8487870065789473 loss 1.893153190612793
epoch 17 iter 38 sum correct 0.8481570512820513 loss 1.8931955099105835
epoch 17 iter 39 sum correct 0.847900390625 loss 1.8932085037231445
epoch 17 iter 40 sum correct 0.8476086128048781 loss 1.8932327032089233
epoch 17 iter 41 sum correct 0.8474237351190477 loss 1.8932397365570068
epoch 17 iter 42 sum correct 0.8474291424418605 loss 1.893231987953186
epoch 17 iter 43 sum correct 0.8472567471590909 loss 1.8932429552078247
epoch 17 iter 44 sum correct 0.847265625 loss 1.8932405710220337
epoch 17 iter 45 sum correct 0.8472316576086957 loss 1.893241286277771
epoch 17 iter 46 sum correct 0.8469913563829787 loss 1.8932572603225708
epoch 17 iter 47 sum correct 0.8466796875 loss 1.893280029296875
epoch 17 iter 48 sum correct 0.8463010204081632 loss 1.893301010131836
epoch 17 iter 49 sum correct 0.845546875 loss 1.8933566808700562
epoch 17 iter 50 sum correct 0.8455882352941176 loss 1.8933485746383667
epoch 17 iter 51 sum correct 0.8452899639423077 loss 1.8933669328689575
epoch 17 iter 52 sum correct 0.8449660966981132 loss 1.893390417098999
epoch 17 iter 53 sum correct 0.8444372106481481 loss 1.8934293985366821
epoch 17 iter 54 sum correct 0.8439985795454545 loss 1.8934615850448608
epoch 17 iter 55 sum correct 0.8441685267857143 loss 1.8934423923492432
epoch 17 iter 56 sum correct 0.8304207785087719 loss 1.893453598022461
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.67it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.11it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.81it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.06it/s]8it [00:01,  5.88it/s]                                 macro  0.5214818849093945
micro  0.6199498467539705
[[214   0  70  42  93   8  40]
 [ 13   0  17   8  16   0   2]
 [ 28   0 242  30 125  32  39]
 [  9   0  20 787  40  13  26]
 [ 44   0 102  43 360   6  98]
 [ 11   0  40  25  17 312  10]
 [ 34   0  56  71 128   8 310]]
              precision    recall  f1-score   support

           0       0.61      0.46      0.52       467
           1       0.00      0.00      0.00        56
           2       0.44      0.49      0.46       496
           3       0.78      0.88      0.83       895
           4       0.46      0.55      0.50       653
           5       0.82      0.75      0.79       415
           6       0.59      0.51      0.55       607

    accuracy                           0.62      3589
   macro avg       0.53      0.52      0.52      3589
weighted avg       0.61      0.62      0.61      3589

correct 0.6199498467539705 
f1 0.5214818849093945 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.76it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.37it/s] 57%|█████▋    | 4/7.009765625 [00:00<00:00,  5.42it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  6.03it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.53it/s]8it [00:01,  6.05it/s]                                 macro  0.5314505504923235
micro  0.63137364168292
[[229   0 104  33  80   4  41]
 [ 15   0  15   8  14   2   1]
 [ 37   0 250  30 133  42  36]
 [  8   0  20 772  51  10  18]
 [ 31   0  79  32 360   9  83]
 [  8   0  47  32  11 303  15]
 [ 36   0  56  64 108  10 352]]
              precision    recall  f1-score   support

           0       0.63      0.47      0.54       491
           1       0.00      0.00      0.00        55
           2       0.44      0.47      0.45       528
           3       0.80      0.88      0.83       879
           4       0.48      0.61      0.53       594
           5       0.80      0.73      0.76       416
           6       0.64      0.56      0.60       626

    accuracy                           0.63      3589
   macro avg       0.54      0.53      0.53      3589
weighted avg       0.63      0.63      0.63      3589

correct 0.63137364168292 
f1 0.5314505504923235 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:39,  1.41it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.34it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.85it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.30it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.49it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.74it/s] 12%|█▏        | 7/56.072265625 [00:02<00:12,  3.78it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.93it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.90it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.02it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.99it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.07it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  3.98it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.00it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  3.92it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.02it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  3.97it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.00it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  3.93it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.02it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  3.91it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.02it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  3.98it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.07it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.03it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.11it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.06it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.13it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.07it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.14it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.07it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.13it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.07it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.14it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.08it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.15it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.06it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.08it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  4.02it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.10it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.05it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.04it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  3.99it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.08it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.03it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.11it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  4.05it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.13it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.04it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.13it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  4.09it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.16it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.10it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.17it/s] 98%|█████████▊| 55/56.072265625 [00:13<00:00,  4.11it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.17it/s]57it [00:14,  3.95it/s]                                  epoch 18 iter 0 sum correct 0.84765625 loss 1.8931602239608765
epoch 18 iter 1 sum correct 0.8388671875 loss 1.8937349319458008
epoch 18 iter 2 sum correct 0.8411458333333334 loss 1.893521785736084
epoch 18 iter 3 sum correct 0.84375 loss 1.8932334184646606
epoch 18 iter 4 sum correct 0.841015625 loss 1.8934834003448486
epoch 18 iter 5 sum correct 0.8369140625 loss 1.893796682357788
epoch 18 iter 6 sum correct 0.8342633928571429 loss 1.893993854522705
epoch 18 iter 7 sum correct 0.832275390625 loss 1.8942235708236694
epoch 18 iter 8 sum correct 0.8302951388888888 loss 1.8943716287612915
epoch 18 iter 9 sum correct 0.829296875 loss 1.89445960521698
epoch 18 iter 10 sum correct 0.8272372159090909 loss 1.8946478366851807
epoch 18 iter 11 sum correct 0.82763671875 loss 1.894606351852417
epoch 18 iter 12 sum correct 0.8279747596153846 loss 1.8945534229278564
epoch 18 iter 13 sum correct 0.8282645089285714 loss 1.894527554512024
epoch 18 iter 14 sum correct 0.8274739583333334 loss 1.8945913314819336
epoch 18 iter 15 sum correct 0.826416015625 loss 1.8946654796600342
epoch 18 iter 16 sum correct 0.8264016544117647 loss 1.894665241241455
epoch 18 iter 17 sum correct 0.8270399305555556 loss 1.8946236371994019
epoch 18 iter 18 sum correct 0.8260690789473685 loss 1.8946818113327026
epoch 18 iter 19 sum correct 0.82646484375 loss 1.89464271068573
epoch 18 iter 20 sum correct 0.8270089285714286 loss 1.894600510597229
epoch 18 iter 21 sum correct 0.8277698863636364 loss 1.8945353031158447
epoch 18 iter 22 sum correct 0.8280400815217391 loss 1.8945138454437256
epoch 18 iter 23 sum correct 0.8267415364583334 loss 1.8945984840393066
epoch 18 iter 24 sum correct 0.825859375 loss 1.8946725130081177
epoch 18 iter 25 sum correct 0.8269230769230769 loss 1.894597053527832
epoch 18 iter 26 sum correct 0.8270399305555556 loss 1.8945866823196411
epoch 18 iter 27 sum correct 0.8274972098214286 loss 1.894548773765564
epoch 18 iter 28 sum correct 0.8283270474137931 loss 1.894486904144287
epoch 18 iter 29 sum correct 0.8289713541666667 loss 1.8944276571273804
epoch 18 iter 30 sum correct 0.8291960685483871 loss 1.894409418106079
epoch 18 iter 31 sum correct 0.82891845703125 loss 1.8944246768951416
epoch 18 iter 32 sum correct 0.8288944128787878 loss 1.8944206237792969
epoch 18 iter 33 sum correct 0.8292738970588235 loss 1.8943819999694824
epoch 18 iter 34 sum correct 0.8290178571428571 loss 1.8944072723388672
epoch 18 iter 35 sum correct 0.8288302951388888 loss 1.8944261074066162
epoch 18 iter 36 sum correct 0.8293918918918919 loss 1.8943839073181152
epoch 18 iter 37 sum correct 0.8297183388157895 loss 1.8943613767623901
epoch 18 iter 38 sum correct 0.8302283653846154 loss 1.8943191766738892
epoch 18 iter 39 sum correct 0.82998046875 loss 1.8943309783935547
epoch 18 iter 40 sum correct 0.8297446646341463 loss 1.8943389654159546
epoch 18 iter 41 sum correct 0.8303106398809523 loss 1.8942985534667969
epoch 18 iter 42 sum correct 0.8301235465116279 loss 1.8943103551864624
epoch 18 iter 43 sum correct 0.8305220170454546 loss 1.8942781686782837
epoch 18 iter 44 sum correct 0.8305121527777778 loss 1.894282579421997
epoch 18 iter 45 sum correct 0.8307999320652174 loss 1.89425528049469
epoch 18 iter 46 sum correct 0.8307014627659575 loss 1.894260048866272
epoch 18 iter 47 sum correct 0.8310953776041666 loss 1.8942327499389648
epoch 18 iter 48 sum correct 0.8313934948979592 loss 1.8942043781280518
epoch 18 iter 49 sum correct 0.8310546875 loss 1.894229531288147
epoch 18 iter 50 sum correct 0.831265318627451 loss 1.8942101001739502
epoch 18 iter 51 sum correct 0.8317307692307693 loss 1.8941731452941895
epoch 18 iter 52 sum correct 0.8322892099056604 loss 1.8941261768341064
epoch 18 iter 53 sum correct 0.8322120949074074 loss 1.894132137298584
epoch 18 iter 54 sum correct 0.8326349431818182 loss 1.8940976858139038
epoch 18 iter 55 sum correct 0.8326939174107143 loss 1.8940917253494263
epoch 18 iter 56 sum correct 0.8192845394736842 loss 1.893936276435852
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.76it/s] 29%|██▊       | 2/7.009765625 [00:00<00:01,  3.39it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.51it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  6.27it/s] 86%|████████▌ | 6/7.009765625 [00:01<00:00,  7.01it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.21it/s]8it [00:01,  5.75it/s]                                 macro  0.5300435475211488
micro  0.6213429924770131
[[247   0  43  17 119   4  37]
 [ 15   0  14   4  16   0   7]
 [ 56   0 222  10 156  19  33]
 [ 27   0  29 699  58  14  68]
 [ 44   0  62  20 446   5  76]
 [ 15   0  55  18  18 297  12]
 [ 29   0  45  30 180   4 319]]
              precision    recall  f1-score   support

           0       0.57      0.53      0.55       467
           1       0.00      0.00      0.00        56
           2       0.47      0.45      0.46       496
           3       0.88      0.78      0.83       895
           4       0.45      0.68      0.54       653
           5       0.87      0.72      0.78       415
           6       0.58      0.53      0.55       607

    accuracy                           0.62      3589
   macro avg       0.54      0.53      0.53      3589
weighted avg       0.64      0.62      0.62      3589

correct 0.6213429924770131 
f1 0.5300435475211488 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.78it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.45it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.68it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.02it/s]8it [00:01,  6.04it/s]                                 macro  0.5327108846747197
micro  0.6274728336584007
[[246   0  60  14 123   5  43]
 [ 19   0   9   6  17   2   2]
 [ 57   0 240  16 147  30  38]
 [ 24   0  23 710  73  11  38]
 [ 39   0  53  16 412   5  69]
 [ 15   0  63  22  22 275  19]
 [ 28   0  45  17 161   6 369]]
              precision    recall  f1-score   support

           0       0.57      0.50      0.54       491
           1       0.00      0.00      0.00        55
           2       0.49      0.45      0.47       528
           3       0.89      0.81      0.85       879
           4       0.43      0.69      0.53       594
           5       0.82      0.66      0.73       416
           6       0.64      0.59      0.61       626

    accuracy                           0.63      3589
   macro avg       0.55      0.53      0.53      3589
weighted avg       0.65      0.63      0.63      3589

correct 0.6274728336584007 
f1 0.5327108846747197 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:39,  1.39it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.28it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.81it/s]  7%|▋         | 4/56.072265625 [00:01<00:16,  3.23it/s]  9%|▉         | 5/56.072265625 [00:01<00:15,  3.40it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.67it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.73it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.89it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.91it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.03it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.98it/s] 21%|██▏       | 12/56.072265625 [00:03<00:11,  4.00it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  3.97it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.06it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.03it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.11it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.03it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.11it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.06it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.14it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.08it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.15it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.07it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.06it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  3.92it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.01it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  3.88it/s] 50%|████▉     | 28/56.072265625 [00:07<00:07,  3.99it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  3.96it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.06it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  3.93it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:06,  3.97it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  3.94it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.04it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  3.99it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:05,  4.00it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  3.98it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.07it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  4.01it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.03it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  3.97it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.04it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  3.98it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:03,  4.00it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  3.93it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.00it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  3.90it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:02,  3.96it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  3.90it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.00it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  3.98it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:01,  3.88it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  3.89it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.01it/s] 98%|█████████▊| 55/56.072265625 [00:14<00:00,  3.99it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.08it/s]57it [00:14,  4.91it/s]                                  57it [00:14,  3.90it/s]epoch 19 iter 0 sum correct 0.849609375 loss 1.892701268196106
epoch 19 iter 1 sum correct 0.8466796875 loss 1.8928837776184082
epoch 19 iter 2 sum correct 0.84765625 loss 1.892789602279663
epoch 19 iter 3 sum correct 0.85546875 loss 1.8922021389007568
epoch 19 iter 4 sum correct 0.85234375 loss 1.8924649953842163
epoch 19 iter 5 sum correct 0.8492838541666666 loss 1.892728567123413
epoch 19 iter 6 sum correct 0.84765625 loss 1.8928505182266235
epoch 19 iter 7 sum correct 0.84716796875 loss 1.8929219245910645
epoch 19 iter 8 sum correct 0.84765625 loss 1.8928430080413818
epoch 19 iter 9 sum correct 0.847265625 loss 1.8928712606430054
epoch 19 iter 10 sum correct 0.8478338068181818 loss 1.8927991390228271
epoch 19 iter 11 sum correct 0.8484700520833334 loss 1.8927615880966187
epoch 19 iter 12 sum correct 0.8484074519230769 loss 1.8927702903747559
epoch 19 iter 13 sum correct 0.849609375 loss 1.8926787376403809
epoch 19 iter 14 sum correct 0.8484375 loss 1.8927733898162842
epoch 19 iter 15 sum correct 0.8486328125 loss 1.892773985862732
epoch 19 iter 16 sum correct 0.8488051470588235 loss 1.8927589654922485
epoch 19 iter 17 sum correct 0.8493923611111112 loss 1.8927135467529297
epoch 19 iter 18 sum correct 0.8495065789473685 loss 1.8927022218704224
epoch 19 iter 19 sum correct 0.84931640625 loss 1.8927007913589478
epoch 19 iter 20 sum correct 0.849609375 loss 1.892675518989563
epoch 19 iter 21 sum correct 0.8481001420454546 loss 1.8927711248397827
epoch 19 iter 22 sum correct 0.8488451086956522 loss 1.8927135467529297
epoch 19 iter 23 sum correct 0.8482259114583334 loss 1.8927569389343262
epoch 19 iter 24 sum correct 0.84765625 loss 1.8927967548370361
epoch 19 iter 25 sum correct 0.8478816105769231 loss 1.8927712440490723
epoch 19 iter 26 sum correct 0.8474392361111112 loss 1.8928025960922241
epoch 19 iter 27 sum correct 0.8466796875 loss 1.8928543329238892
epoch 19 iter 28 sum correct 0.8466460129310345 loss 1.8928438425064087
epoch 19 iter 29 sum correct 0.8468098958333333 loss 1.8928420543670654
epoch 19 iter 30 sum correct 0.8465221774193549 loss 1.8928606510162354
epoch 19 iter 31 sum correct 0.8463134765625 loss 1.8928741216659546
epoch 19 iter 32 sum correct 0.8475378787878788 loss 1.8927836418151855
epoch 19 iter 33 sum correct 0.8485753676470589 loss 1.8927032947540283
epoch 19 iter 34 sum correct 0.8487165178571429 loss 1.892691731452942
epoch 19 iter 35 sum correct 0.8485785590277778 loss 1.8926985263824463
epoch 19 iter 36 sum correct 0.8487119932432432 loss 1.8926879167556763
epoch 19 iter 37 sum correct 0.8488384046052632 loss 1.8926752805709839
epoch 19 iter 38 sum correct 0.8489583333333334 loss 1.8926646709442139
epoch 19 iter 39 sum correct 0.84892578125 loss 1.892666220664978
epoch 19 iter 40 sum correct 0.8495617378048781 loss 1.8926169872283936
epoch 19 iter 41 sum correct 0.8497488839285714 loss 1.892598271369934
epoch 19 iter 42 sum correct 0.8495185319767442 loss 1.8926167488098145
epoch 19 iter 43 sum correct 0.8490323153409091 loss 1.8926448822021484
epoch 19 iter 44 sum correct 0.8490885416666667 loss 1.8926403522491455
epoch 19 iter 45 sum correct 0.8495669157608695 loss 1.8926085233688354
epoch 19 iter 46 sum correct 0.8496924867021277 loss 1.892593264579773
epoch 19 iter 47 sum correct 0.8497314453125 loss 1.8925889730453491
epoch 19 iter 48 sum correct 0.8496890943877551 loss 1.89259672164917
epoch 19 iter 49 sum correct 0.8498046875 loss 1.8925836086273193
epoch 19 iter 50 sum correct 0.8493412990196079 loss 1.8926172256469727
epoch 19 iter 51 sum correct 0.8493464543269231 loss 1.8926154375076294
epoch 19 iter 52 sum correct 0.8494988207547169 loss 1.892604112625122
epoch 19 iter 53 sum correct 0.8488498263888888 loss 1.8926548957824707
epoch 19 iter 54 sum correct 0.848934659090909 loss 1.8926479816436768
epoch 19 iter 55 sum correct 0.8490513392857143 loss 1.8926340341567993
epoch 19 iter 56 sum correct 0.835217927631579 loss 1.8926498889923096
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.72it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.41it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.85it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.11it/s]8it [00:01,  6.02it/s]                                 macro  0.5153542323410046
micro  0.6051825020897186
[[248   0  81  16  88   7  27]
 [ 18   0  19   4  10   0   5]
 [ 42   0 269  13 119  20  33]
 [ 29   0  39 713  39  14  61]
 [ 70   0 119  26 373   7  58]
 [ 15   0  82  17  16 281   4]
 [ 50   0  72  43 147   7 288]]
              precision    recall  f1-score   support

           0       0.53      0.53      0.53       467
           1       0.00      0.00      0.00        56
           2       0.40      0.54      0.46       496
           3       0.86      0.80      0.83       895
           4       0.47      0.57      0.52       653
           5       0.84      0.68      0.75       415
           6       0.61      0.47      0.53       607

    accuracy                           0.61      3589
   macro avg       0.53      0.51      0.52      3589
weighted avg       0.62      0.61      0.61      3589

correct 0.6051825020897186 
f1 0.5153542323410046 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.75it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.49it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.78it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.98it/s]8it [00:01,  6.00it/s]                                 macro  0.5310227088181968
micro  0.6260796879353581
[[259   0  87  18 101   2  24]
 [ 19   0  14   7  13   0   2]
 [ 47   0 307  12 110  29  23]
 [ 26   0  29 741  48   9  26]
 [ 57   0  90  22 355   4  66]
 [  9   0 100  23  16 257  11]
 [ 45   0  64  25 156   8 328]]
              precision    recall  f1-score   support

           0       0.56      0.53      0.54       491
           1       0.00      0.00      0.00        55
           2       0.44      0.58      0.50       528
           3       0.87      0.84      0.86       879
           4       0.44      0.60      0.51       594
           5       0.83      0.62      0.71       416
           6       0.68      0.52      0.59       626

    accuracy                           0.63      3589
   macro avg       0.55      0.53      0.53      3589
weighted avg       0.65      0.63      0.63      3589

correct 0.6260796879353581 
f1 0.5310227088181968 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:40,  1.37it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.29it/s]  5%|▌         | 3/56.072265625 [00:01<00:19,  2.78it/s]  7%|▋         | 4/56.072265625 [00:01<00:16,  3.23it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.41it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.68it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.74it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.90it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.89it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.01it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.96it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.06it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  3.98it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.07it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  3.99it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.07it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.02it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.09it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.02it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.10it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.04it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.11it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.04it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.12it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.05it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.13it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.06it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.14it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.06it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.14it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.07it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.14it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.07it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.14it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.06it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.14it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.07it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.14it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  4.07it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.14it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.07it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.14it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  4.07it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.14it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.06it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.14it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  4.06it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.13it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.05it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.13it/s] 91%|█████████ | 51/56.072265625 [00:12<00:01,  4.07it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.15it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.09it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.16it/s] 98%|█████████▊| 55/56.072265625 [00:13<00:00,  4.09it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.16it/s]57it [00:14,  5.04it/s]                                  57it [00:14,  3.97it/s]epoch 20 iter 0 sum correct 0.857421875 loss 1.892095685005188
epoch 20 iter 1 sum correct 0.8466796875 loss 1.8927757740020752
epoch 20 iter 2 sum correct 0.849609375 loss 1.892433524131775
epoch 20 iter 3 sum correct 0.85107421875 loss 1.8923254013061523
epoch 20 iter 4 sum correct 0.84921875 loss 1.8924235105514526
epoch 20 iter 5 sum correct 0.845703125 loss 1.8926790952682495
epoch 20 iter 6 sum correct 0.8420758928571429 loss 1.8929452896118164
epoch 20 iter 7 sum correct 0.846435546875 loss 1.8926050662994385
epoch 20 iter 8 sum correct 0.8474392361111112 loss 1.8925338983535767
epoch 20 iter 9 sum correct 0.846875 loss 1.8925756216049194
epoch 20 iter 10 sum correct 0.8439275568181818 loss 1.8928124904632568
epoch 20 iter 11 sum correct 0.8427734375 loss 1.8929009437561035
epoch 20 iter 12 sum correct 0.8429987980769231 loss 1.892868161201477
epoch 20 iter 13 sum correct 0.8429129464285714 loss 1.8928817510604858
epoch 20 iter 14 sum correct 0.84375 loss 1.8928197622299194
epoch 20 iter 15 sum correct 0.84326171875 loss 1.8928743600845337
epoch 20 iter 16 sum correct 0.8429457720588235 loss 1.8928954601287842
epoch 20 iter 17 sum correct 0.8442925347222222 loss 1.8928048610687256
epoch 20 iter 18 sum correct 0.8440583881578947 loss 1.8928146362304688
epoch 20 iter 19 sum correct 0.84287109375 loss 1.8929016590118408
epoch 20 iter 20 sum correct 0.8422619047619048 loss 1.8929401636123657
epoch 20 iter 21 sum correct 0.8420632102272727 loss 1.892972469329834
epoch 20 iter 22 sum correct 0.8429857336956522 loss 1.8929039239883423
epoch 20 iter 23 sum correct 0.8431803385416666 loss 1.8929085731506348
epoch 20 iter 24 sum correct 0.843046875 loss 1.8929110765457153
epoch 20 iter 25 sum correct 0.8429987980769231 loss 1.8929197788238525
epoch 20 iter 26 sum correct 0.8435329861111112 loss 1.8928873538970947
epoch 20 iter 27 sum correct 0.84326171875 loss 1.8929152488708496
epoch 20 iter 28 sum correct 0.8435479525862069 loss 1.892900824546814
epoch 20 iter 29 sum correct 0.8439453125 loss 1.8928765058517456
epoch 20 iter 30 sum correct 0.8445690524193549 loss 1.8928343057632446
epoch 20 iter 31 sum correct 0.84454345703125 loss 1.8928314447402954
epoch 20 iter 32 sum correct 0.8449928977272727 loss 1.892791748046875
epoch 20 iter 33 sum correct 0.8446116727941176 loss 1.892822265625
epoch 20 iter 34 sum correct 0.8454241071428571 loss 1.8927638530731201
epoch 20 iter 35 sum correct 0.8449978298611112 loss 1.8927881717681885
epoch 20 iter 36 sum correct 0.845386402027027 loss 1.8927592039108276
epoch 20 iter 37 sum correct 0.845446134868421 loss 1.892758846282959
epoch 20 iter 38 sum correct 0.844551282051282 loss 1.8928329944610596
epoch 20 iter 39 sum correct 0.84453125 loss 1.892837405204773
epoch 20 iter 40 sum correct 0.8455602134146342 loss 1.8927580118179321
epoch 20 iter 41 sum correct 0.8450520833333334 loss 1.892798900604248
epoch 20 iter 42 sum correct 0.845203488372093 loss 1.8927885293960571
epoch 20 iter 43 sum correct 0.8457919034090909 loss 1.892744779586792
epoch 20 iter 44 sum correct 0.8458333333333333 loss 1.8927415609359741
epoch 20 iter 45 sum correct 0.8460427989130435 loss 1.892727255821228
epoch 20 iter 46 sum correct 0.8452875664893617 loss 1.8927829265594482
epoch 20 iter 47 sum correct 0.8451741536458334 loss 1.8927901983261108
epoch 20 iter 48 sum correct 0.845344387755102 loss 1.8927756547927856
epoch 20 iter 49 sum correct 0.84515625 loss 1.8927865028381348
epoch 20 iter 50 sum correct 0.8458946078431373 loss 1.8927335739135742
epoch 20 iter 51 sum correct 0.8460036057692307 loss 1.8927252292633057
epoch 20 iter 52 sum correct 0.8464033018867925 loss 1.8926914930343628
epoch 20 iter 53 sum correct 0.8463903356481481 loss 1.8926825523376465
epoch 20 iter 54 sum correct 0.8465198863636364 loss 1.892671823501587
epoch 20 iter 55 sum correct 0.8468540736607143 loss 1.8926445245742798
epoch 20 iter 56 sum correct 0.8332305372807017 loss 1.892472505569458
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.74it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.38it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.79it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.94it/s]8it [00:01,  5.95it/s]                                 macro  0.5249724874202024
micro  0.6266369462245751
[[268   0  32  20  53  28  66]
 [ 26   0   8   5   8   1   8]
 [ 53   0 198  14  95  67  69]
 [ 27   0  17 728  30  31  62]
 [ 57   0  69  32 304  17 174]
 [ 13   0  17  17  10 347  11]
 [ 30   0  35  48  70  20 404]]
              precision    recall  f1-score   support

           0       0.57      0.57      0.57       467
           1       0.00      0.00      0.00        56
           2       0.53      0.40      0.45       496
           3       0.84      0.81      0.83       895
           4       0.53      0.47      0.50       653
           5       0.68      0.84      0.75       415
           6       0.51      0.67      0.58       607

    accuracy                           0.63      3589
   macro avg       0.52      0.54      0.52      3589
weighted avg       0.62      0.63      0.62      3589

correct 0.6266369462245751 
f1 0.5249724874202024 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.81it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.56it/s] 71%|███████▏  | 5/7.009765625 [00:00<00:00,  5.99it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.21it/s]8it [00:01,  6.16it/s]                                 macro  0.5357494876121461
micro  0.64084703259961
[[289   0  49  16  47  20  70]
 [ 25   0   9   4  11   3   3]
 [ 61   0 224  14  90  72  67]
 [ 25   0  10 738  35  27  44]
 [ 65   0  63  25 266  12 163]
 [  9   0  25  17  12 334  19]
 [ 38   0  30  24  64  21 449]]
              precision    recall  f1-score   support

           0       0.56      0.59      0.58       491
           1       0.00      0.00      0.00        55
           2       0.55      0.42      0.48       528
           3       0.88      0.84      0.86       879
           4       0.51      0.45      0.48       594
           5       0.68      0.80      0.74       416
           6       0.55      0.72      0.62       626

    accuracy                           0.64      3589
   macro avg       0.53      0.55      0.54      3589
weighted avg       0.63      0.64      0.63      3589

correct 0.64084703259961 
f1 0.5357494876121461 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:38,  1.42it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.33it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.84it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.27it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.45it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.69it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.76it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.92it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.92it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.03it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  4.00it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.09it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  4.03it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.12it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.06it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.14it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.05it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.13it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.02it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.09it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.03it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.11it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.03it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.11it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.04it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.10it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.03it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.11it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.04it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.12it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.04it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.11it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.04it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.11it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.03it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.11it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.04it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.11it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  4.02it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.10it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.02it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.10it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  4.02it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.11it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.01it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.10it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  4.03it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.12it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.02it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.11it/s] 91%|█████████ | 51/56.072265625 [00:12<00:01,  4.04it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.13it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.07it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.15it/s] 98%|█████████▊| 55/56.072265625 [00:13<00:00,  4.08it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.15it/s]57it [00:14,  5.02it/s]                                  57it [00:14,  3.97it/s]epoch 21 iter 0 sum correct 0.837890625 loss 1.8932539224624634
epoch 21 iter 1 sum correct 0.8388671875 loss 1.8931244611740112
epoch 21 iter 2 sum correct 0.84765625 loss 1.8924274444580078
epoch 21 iter 3 sum correct 0.85107421875 loss 1.892177700996399
epoch 21 iter 4 sum correct 0.85390625 loss 1.891958475112915
epoch 21 iter 5 sum correct 0.8551432291666666 loss 1.8918886184692383
epoch 21 iter 6 sum correct 0.8543526785714286 loss 1.8919435739517212
epoch 21 iter 7 sum correct 0.858154296875 loss 1.8916295766830444
epoch 21 iter 8 sum correct 0.8556857638888888 loss 1.8918184041976929
epoch 21 iter 9 sum correct 0.85546875 loss 1.891844630241394
epoch 21 iter 10 sum correct 0.8561789772727273 loss 1.8918216228485107
epoch 21 iter 11 sum correct 0.8577473958333334 loss 1.8917096853256226
epoch 21 iter 12 sum correct 0.8562199519230769 loss 1.8918156623840332
epoch 21 iter 13 sum correct 0.8572823660714286 loss 1.8917391300201416
epoch 21 iter 14 sum correct 0.8555989583333333 loss 1.891861081123352
epoch 21 iter 15 sum correct 0.8572998046875 loss 1.8917244672775269
epoch 21 iter 16 sum correct 0.8560431985294118 loss 1.891802191734314
epoch 21 iter 17 sum correct 0.8569878472222222 loss 1.8917183876037598
epoch 21 iter 18 sum correct 0.8583470394736842 loss 1.8916150331497192
epoch 21 iter 19 sum correct 0.8583984375 loss 1.8916116952896118
epoch 21 iter 20 sum correct 0.8576078869047619 loss 1.8916704654693604
epoch 21 iter 21 sum correct 0.8571555397727273 loss 1.8916960954666138
epoch 21 iter 22 sum correct 0.8567425271739131 loss 1.8917299509048462
epoch 21 iter 23 sum correct 0.8561197916666666 loss 1.8917829990386963
epoch 21 iter 24 sum correct 0.8553125 loss 1.891852855682373
epoch 21 iter 25 sum correct 0.8552433894230769 loss 1.8918484449386597
epoch 21 iter 26 sum correct 0.8548177083333334 loss 1.8918858766555786
epoch 21 iter 27 sum correct 0.853515625 loss 1.8919821977615356
epoch 21 iter 28 sum correct 0.8532462284482759 loss 1.891998529434204
epoch 21 iter 29 sum correct 0.8530598958333333 loss 1.8920116424560547
epoch 21 iter 30 sum correct 0.853515625 loss 1.8919810056686401
epoch 21 iter 31 sum correct 0.85357666015625 loss 1.8919713497161865
epoch 21 iter 32 sum correct 0.8544625946969697 loss 1.89190673828125
epoch 21 iter 33 sum correct 0.8542049632352942 loss 1.8919233083724976
epoch 21 iter 34 sum correct 0.8535714285714285 loss 1.891970157623291
epoch 21 iter 35 sum correct 0.8534071180555556 loss 1.891985535621643
epoch 21 iter 36 sum correct 0.8537267736486487 loss 1.8919645547866821
epoch 21 iter 37 sum correct 0.8528474506578947 loss 1.8920276165008545
epoch 21 iter 38 sum correct 0.852714342948718 loss 1.8920351266860962
epoch 21 iter 39 sum correct 0.85263671875 loss 1.8920379877090454
epoch 21 iter 40 sum correct 0.8528963414634146 loss 1.892011046409607
epoch 21 iter 41 sum correct 0.8520275297619048 loss 1.8920772075653076
epoch 21 iter 42 sum correct 0.8523800872093024 loss 1.8920502662658691
epoch 21 iter 43 sum correct 0.8528053977272727 loss 1.8920214176177979
epoch 21 iter 44 sum correct 0.8532118055555555 loss 1.8919929265975952
epoch 21 iter 45 sum correct 0.8528787364130435 loss 1.8920202255249023
epoch 21 iter 46 sum correct 0.852310505319149 loss 1.8920658826828003
epoch 21 iter 47 sum correct 0.8524983723958334 loss 1.8920471668243408
epoch 21 iter 48 sum correct 0.8529974489795918 loss 1.89200758934021
epoch 21 iter 49 sum correct 0.852421875 loss 1.8920443058013916
epoch 21 iter 50 sum correct 0.8520603553921569 loss 1.89207124710083
epoch 21 iter 51 sum correct 0.8517878605769231 loss 1.8920886516571045
epoch 21 iter 52 sum correct 0.851783608490566 loss 1.892087697982788
epoch 21 iter 53 sum correct 0.8519965277777778 loss 1.892072081565857
epoch 21 iter 54 sum correct 0.8520596590909091 loss 1.8920596837997437
epoch 21 iter 55 sum correct 0.85205078125 loss 1.892053246498108
epoch 21 iter 56 sum correct 0.8382675438596491 loss 1.8920049667358398
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.53it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.09it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.55it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.77it/s]8it [00:01,  5.70it/s]                                 macro  0.5361462374688165
micro  0.6336026748397883
[[270   0  50  17  79   8  43]
 [ 26   0  11   4  11   1   3]
 [ 50   0 237  20 107  44  38]
 [ 41   0  18 742  40  17  37]
 [ 64   0  91  24 380  11  83]
 [ 15   0  39  16  16 320   9]
 [ 33   0  45  57 139   8 325]]
              precision    recall  f1-score   support

           0       0.54      0.58      0.56       467
           1       0.00      0.00      0.00        56
           2       0.48      0.48      0.48       496
           3       0.84      0.83      0.84       895
           4       0.49      0.58      0.53       653
           5       0.78      0.77      0.78       415
           6       0.60      0.54      0.57       607

    accuracy                           0.63      3589
   macro avg       0.54      0.54      0.54      3589
weighted avg       0.63      0.63      0.63      3589

correct 0.6336026748397883 
f1 0.5361462374688165 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.76it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.45it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.82it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.07it/s]8it [00:01,  6.03it/s]                                 macro  0.5382928629184177
micro  0.6375034828643076
[[275   0  71  15  80   7  43]
 [ 29   0  11   6   5   3   1]
 [ 63   0 251  25  95  53  41]
 [ 37   0  18 747  38  16  23]
 [ 49   0  86  33 335   8  83]
 [ 11   0  43  19  14 312  17]
 [ 45   0  40  39 127   7 368]]
              precision    recall  f1-score   support

           0       0.54      0.56      0.55       491
           1       0.00      0.00      0.00        55
           2       0.48      0.48      0.48       528
           3       0.85      0.85      0.85       879
           4       0.48      0.56      0.52       594
           5       0.77      0.75      0.76       416
           6       0.64      0.59      0.61       626

    accuracy                           0.64      3589
   macro avg       0.54      0.54      0.54      3589
weighted avg       0.63      0.64      0.63      3589

correct 0.6375034828643076 
f1 0.5382928629184177 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:39,  1.40it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.29it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.82it/s]  7%|▋         | 4/56.072265625 [00:01<00:16,  3.23it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.41it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.65it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.74it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.91it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.88it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.01it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.99it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.08it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  4.04it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.11it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.04it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.09it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.02it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.11it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.04it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.12it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.05it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  3.88it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  3.90it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.01it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  3.98it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.08it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.00it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.09it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.04it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.11it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.06it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.13it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.07it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.14it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.08it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.15it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.09it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.15it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  4.06it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.13it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.06it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.14it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  4.08it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.15it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.08it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.15it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  4.08it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.14it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.08it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.16it/s] 91%|█████████ | 51/56.072265625 [00:12<00:01,  4.11it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.18it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.11it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.18it/s] 98%|█████████▊| 55/56.072265625 [00:13<00:00,  4.07it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.14it/s]57it [00:14,  4.96it/s]                                  57it [00:14,  3.97it/s]epoch 22 iter 0 sum correct 0.841796875 loss 1.8927159309387207
epoch 22 iter 1 sum correct 0.859375 loss 1.8915660381317139
epoch 22 iter 2 sum correct 0.8541666666666666 loss 1.8918379545211792
epoch 22 iter 3 sum correct 0.84716796875 loss 1.8923568725585938
epoch 22 iter 4 sum correct 0.84375 loss 1.8925918340682983
epoch 22 iter 5 sum correct 0.8427734375 loss 1.8927253484725952
epoch 22 iter 6 sum correct 0.8395647321428571 loss 1.8928972482681274
epoch 22 iter 7 sum correct 0.836181640625 loss 1.8931777477264404
epoch 22 iter 8 sum correct 0.8326822916666666 loss 1.8934495449066162
epoch 22 iter 9 sum correct 0.829296875 loss 1.893694519996643
epoch 22 iter 10 sum correct 0.8302556818181818 loss 1.8936355113983154
epoch 22 iter 11 sum correct 0.8273111979166666 loss 1.8938510417938232
epoch 22 iter 12 sum correct 0.8252704326923077 loss 1.8940068483352661
epoch 22 iter 13 sum correct 0.8236607142857143 loss 1.894132375717163
epoch 22 iter 14 sum correct 0.820703125 loss 1.8943548202514648
epoch 22 iter 15 sum correct 0.820556640625 loss 1.894368052482605
epoch 22 iter 16 sum correct 0.8215762867647058 loss 1.894275426864624
epoch 22 iter 17 sum correct 0.8215060763888888 loss 1.8942875862121582
epoch 22 iter 18 sum correct 0.8211348684210527 loss 1.8943099975585938
epoch 22 iter 19 sum correct 0.8203125 loss 1.894352912902832
epoch 22 iter 20 sum correct 0.8194754464285714 loss 1.8944153785705566
epoch 22 iter 21 sum correct 0.8197798295454546 loss 1.894389271736145
epoch 22 iter 22 sum correct 0.8190387228260869 loss 1.8944422006607056
epoch 22 iter 23 sum correct 0.817626953125 loss 1.894557237625122
epoch 22 iter 24 sum correct 0.8178125 loss 1.8945354223251343
epoch 22 iter 25 sum correct 0.8179086538461539 loss 1.8945392370224
epoch 22 iter 26 sum correct 0.8178530092592593 loss 1.894537091255188
epoch 22 iter 27 sum correct 0.8184988839285714 loss 1.894485592842102
epoch 22 iter 28 sum correct 0.8188308189655172 loss 1.8944708108901978
epoch 22 iter 29 sum correct 0.8195963541666667 loss 1.8944215774536133
epoch 22 iter 30 sum correct 0.8198084677419355 loss 1.8944019079208374
epoch 22 iter 31 sum correct 0.8197021484375 loss 1.8944035768508911
epoch 22 iter 32 sum correct 0.8201941287878788 loss 1.8943779468536377
epoch 22 iter 33 sum correct 0.8204273897058824 loss 1.8943575620651245
epoch 22 iter 34 sum correct 0.8210379464285714 loss 1.8943095207214355
epoch 22 iter 35 sum correct 0.8215060763888888 loss 1.8942753076553345
epoch 22 iter 36 sum correct 0.821948902027027 loss 1.8942378759384155
epoch 22 iter 37 sum correct 0.8217516447368421 loss 1.8942632675170898
epoch 22 iter 38 sum correct 0.8228165064102564 loss 1.8941738605499268
epoch 22 iter 39 sum correct 0.8232421875 loss 1.894142746925354
epoch 22 iter 40 sum correct 0.8237423780487805 loss 1.894107460975647
epoch 22 iter 41 sum correct 0.8243582589285714 loss 1.8940554857254028
epoch 22 iter 42 sum correct 0.8248092296511628 loss 1.8940181732177734
epoch 22 iter 43 sum correct 0.8250177556818182 loss 1.8940019607543945
epoch 22 iter 44 sum correct 0.8256944444444444 loss 1.8939528465270996
epoch 22 iter 45 sum correct 0.8263841711956522 loss 1.893898844718933
epoch 22 iter 46 sum correct 0.8271276595744681 loss 1.8938429355621338
epoch 22 iter 47 sum correct 0.8272298177083334 loss 1.89383065700531
epoch 22 iter 48 sum correct 0.8266900510204082 loss 1.893869161605835
epoch 22 iter 49 sum correct 0.826953125 loss 1.8938524723052979
epoch 22 iter 50 sum correct 0.8262484681372549 loss 1.8939030170440674
epoch 22 iter 51 sum correct 0.8264723557692307 loss 1.8938822746276855
epoch 22 iter 52 sum correct 0.8267246462264151 loss 1.8938653469085693
epoch 22 iter 53 sum correct 0.8272207754629629 loss 1.893828272819519
epoch 22 iter 54 sum correct 0.8273082386363636 loss 1.8938223123550415
epoch 22 iter 55 sum correct 0.8274623325892857 loss 1.89381742477417
epoch 22 iter 56 sum correct 0.8140419407894737 loss 1.8937605619430542
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.74it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.35it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.75it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.02it/s]8it [00:01,  5.93it/s]                                 macro  0.5217609457317354
micro  0.6213429924770131
[[268   0  23  20  94  19  43]
 [ 25   0   5   4  19   0   3]
 [ 56   0 185  19 136  47  53]
 [ 35   0  13 727  41  33  46]
 [ 70   0  48  18 404  16  97]
 [ 15   0  27  15  17 331  10]
 [ 67   0  24  43 150   8 315]]
              precision    recall  f1-score   support

           0       0.50      0.57      0.53       467
           1       0.00      0.00      0.00        56
           2       0.57      0.37      0.45       496
           3       0.86      0.81      0.84       895
           4       0.47      0.62      0.53       653
           5       0.73      0.80      0.76       415
           6       0.56      0.52      0.54       607

    accuracy                           0.62      3589
   macro avg       0.53      0.53      0.52      3589
weighted avg       0.62      0.62      0.62      3589

correct 0.6213429924770131 
f1 0.5217609457317354 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.78it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.50it/s] 71%|███████▏  | 5/7.009765625 [00:00<00:00,  6.04it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.28it/s]8it [00:01,  6.18it/s]                                 macro  0.5462831409141112
micro  0.6494845360824743
[[291   0  33  11 100  13  43]
 [ 22   0   8   7  16   2   0]
 [ 74   0 209  18 129  50  48]
 [ 19   0  15 757  39  20  29]
 [ 63   0  28  19 386   7  91]
 [ 12   0  25  18  20 323  18]
 [ 68   0  17  32 133  11 365]]
              precision    recall  f1-score   support

           0       0.53      0.59      0.56       491
           1       0.00      0.00      0.00        55
           2       0.62      0.40      0.48       528
           3       0.88      0.86      0.87       879
           4       0.47      0.65      0.54       594
           5       0.76      0.78      0.77       416
           6       0.61      0.58      0.60       626

    accuracy                           0.65      3589
   macro avg       0.55      0.55      0.55      3589
weighted avg       0.65      0.65      0.64      3589

correct 0.6494845360824743 
f1 0.5462831409141112 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:38,  1.42it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.33it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.84it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.29it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.45it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.69it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.75it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.92it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.89it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.02it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.87it/s] 21%|██▏       | 12/56.072265625 [00:03<00:11,  3.99it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  3.95it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.05it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.02it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.11it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.05it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.12it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.07it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.14it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.06it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.14it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.09it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.16it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.07it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.14it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.07it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.13it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.07it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.14it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.09it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.16it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.07it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.14it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.09it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.15it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.07it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.14it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  4.07it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.13it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.05it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.13it/s] 77%|███████▋  | 43/56.072265625 [00:10<00:03,  4.07it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.14it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.06it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.13it/s] 84%|████████▍ | 47/56.072265625 [00:11<00:02,  4.06it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.14it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.06it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.14it/s] 91%|█████████ | 51/56.072265625 [00:12<00:01,  4.07it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.15it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.09it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.17it/s] 98%|█████████▊| 55/56.072265625 [00:13<00:00,  4.09it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.16it/s]57it [00:14,  5.00it/s]                                  57it [00:14,  3.98it/s]epoch 23 iter 0 sum correct 0.84375 loss 1.8926312923431396
epoch 23 iter 1 sum correct 0.85546875 loss 1.891730546951294
epoch 23 iter 2 sum correct 0.853515625 loss 1.891677737236023
epoch 23 iter 3 sum correct 0.8544921875 loss 1.8916432857513428
epoch 23 iter 4 sum correct 0.85 loss 1.892007827758789
epoch 23 iter 5 sum correct 0.8453776041666666 loss 1.8923720121383667
epoch 23 iter 6 sum correct 0.8451450892857143 loss 1.892441987991333
epoch 23 iter 7 sum correct 0.84423828125 loss 1.8924704790115356
epoch 23 iter 8 sum correct 0.8407118055555556 loss 1.8927549123764038
epoch 23 iter 9 sum correct 0.8357421875 loss 1.893158197402954
epoch 23 iter 10 sum correct 0.8336292613636364 loss 1.893336296081543
epoch 23 iter 11 sum correct 0.8321940104166666 loss 1.893446445465088
epoch 23 iter 12 sum correct 0.8302283653846154 loss 1.8936001062393188
epoch 23 iter 13 sum correct 0.830078125 loss 1.8936161994934082
epoch 23 iter 14 sum correct 0.8294270833333334 loss 1.8936374187469482
epoch 23 iter 15 sum correct 0.8306884765625 loss 1.8935503959655762
epoch 23 iter 16 sum correct 0.8298483455882353 loss 1.8936139345169067
epoch 23 iter 17 sum correct 0.8296440972222222 loss 1.8936522006988525
epoch 23 iter 18 sum correct 0.828844572368421 loss 1.8937116861343384
epoch 23 iter 19 sum correct 0.82998046875 loss 1.8936210870742798
epoch 23 iter 20 sum correct 0.830078125 loss 1.8936023712158203
epoch 23 iter 21 sum correct 0.8308771306818182 loss 1.8935489654541016
epoch 23 iter 22 sum correct 0.830078125 loss 1.893585443496704
epoch 23 iter 23 sum correct 0.828857421875 loss 1.8936761617660522
epoch 23 iter 24 sum correct 0.829375 loss 1.8936184644699097
epoch 23 iter 25 sum correct 0.8290264423076923 loss 1.8936357498168945
epoch 23 iter 26 sum correct 0.8297164351851852 loss 1.8935821056365967
epoch 23 iter 27 sum correct 0.8302176339285714 loss 1.8935410976409912
epoch 23 iter 28 sum correct 0.830010775862069 loss 1.8935438394546509
epoch 23 iter 29 sum correct 0.8291666666666667 loss 1.8936008214950562
epoch 23 iter 30 sum correct 0.8290070564516129 loss 1.8936080932617188
epoch 23 iter 31 sum correct 0.82855224609375 loss 1.8936339616775513
epoch 23 iter 32 sum correct 0.8288352272727273 loss 1.8936059474945068
epoch 23 iter 33 sum correct 0.8285271139705882 loss 1.8936172723770142
epoch 23 iter 34 sum correct 0.8296316964285714 loss 1.8935357332229614
epoch 23 iter 35 sum correct 0.8294813368055556 loss 1.8935508728027344
epoch 23 iter 36 sum correct 0.829761402027027 loss 1.89352548122406
epoch 23 iter 37 sum correct 0.8293071546052632 loss 1.8935561180114746
epoch 23 iter 38 sum correct 0.8293770032051282 loss 1.8935457468032837
epoch 23 iter 39 sum correct 0.830322265625 loss 1.8934738636016846
epoch 23 iter 40 sum correct 0.8313166920731707 loss 1.8933998346328735
epoch 23 iter 41 sum correct 0.8316127232142857 loss 1.8933815956115723
epoch 23 iter 42 sum correct 0.8317587209302325 loss 1.8933686017990112
epoch 23 iter 43 sum correct 0.8314098011363636 loss 1.8933943510055542
epoch 23 iter 44 sum correct 0.8318142361111112 loss 1.8933627605438232
epoch 23 iter 45 sum correct 0.8321586277173914 loss 1.8933320045471191
epoch 23 iter 46 sum correct 0.8323636968085106 loss 1.8933134078979492
epoch 23 iter 47 sum correct 0.8326822916666666 loss 1.893286108970642
epoch 23 iter 48 sum correct 0.8331074617346939 loss 1.8932533264160156
epoch 23 iter 49 sum correct 0.833359375 loss 1.89322829246521
epoch 23 iter 50 sum correct 0.8342524509803921 loss 1.8931596279144287
epoch 23 iter 51 sum correct 0.8342848557692307 loss 1.893153429031372
epoch 23 iter 52 sum correct 0.8347951061320755 loss 1.8931125402450562
epoch 23 iter 53 sum correct 0.8347077546296297 loss 1.8931130170822144
epoch 23 iter 54 sum correct 0.8349431818181818 loss 1.8930941820144653
epoch 23 iter 55 sum correct 0.8353794642857143 loss 1.8930593729019165
epoch 23 iter 56 sum correct 0.8217173793859649 loss 1.8931193351745605
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.61it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.19it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.47it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.69it/s]8it [00:01,  5.65it/s]                                 macro  0.518848575915882
micro  0.6185567010309279
[[286   0  49  23  55  19  35]
 [ 25   0   8   4  13   0   6]
 [ 75   0 227  18  85  45  46]
 [ 34   0  19 749  17  32  44]
 [ 93   0 111  45 288  17  99]
 [ 20   0  36  18   4 330   7]
 [ 63   0  45  58  82  19 340]]
              precision    recall  f1-score   support

           0       0.48      0.61      0.54       467
           1       0.00      0.00      0.00        56
           2       0.46      0.46      0.46       496
           3       0.82      0.84      0.83       895
           4       0.53      0.44      0.48       653
           5       0.71      0.80      0.75       415
           6       0.59      0.56      0.57       607

    accuracy                           0.62      3589
   macro avg       0.51      0.53      0.52      3589
weighted avg       0.61      0.62      0.61      3589

correct 0.6185567010309279 
f1 0.518848575915882 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.79it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.51it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.95it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.15it/s]8it [00:01,  6.10it/s]                                 macro  0.5338182337081723
micro  0.6377821120089161
[[316   0  50  19  49  18  39]
 [ 27   0  13   8   4   0   3]
 [ 86   0 246  26  73  51  46]
 [ 30   0  14 772  20  19  24]
 [ 70   0 101  53 257  11 102]
 [ 17   0  36  20   5 323  15]
 [ 70   0  48  44  70  19 375]]
              precision    recall  f1-score   support

           0       0.51      0.64      0.57       491
           1       0.00      0.00      0.00        55
           2       0.48      0.47      0.47       528
           3       0.82      0.88      0.85       879
           4       0.54      0.43      0.48       594
           5       0.73      0.78      0.75       416
           6       0.62      0.60      0.61       626

    accuracy                           0.64      3589
   macro avg       0.53      0.54      0.53      3589
weighted avg       0.62      0.64      0.63      3589

correct 0.6377821120089161 
f1 0.5338182337081723 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:38,  1.43it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.34it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.84it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.28it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.47it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.72it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.75it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.92it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.88it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.00it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.96it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.06it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  3.99it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.08it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.02it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.11it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.05it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.13it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.06it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.13it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.07it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.14it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.06it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.14it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.06it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.14it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.06it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.14it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.05it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.12it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.05it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.13it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.06it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.13it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.05it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.13it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.06it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.13it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  4.06it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.13it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.06it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.13it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  3.98it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.03it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  3.95it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.05it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  3.96it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:02,  3.83it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  3.85it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  3.98it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  3.97it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.07it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.04it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.12it/s] 98%|█████████▊| 55/56.072265625 [00:13<00:00,  4.07it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.14it/s]57it [00:14,  5.02it/s]                                  57it [00:14,  3.96it/s]epoch 24 iter 0 sum correct 0.833984375 loss 1.8930097818374634
epoch 24 iter 1 sum correct 0.84375 loss 1.892254114151001
epoch 24 iter 2 sum correct 0.8509114583333334 loss 1.8917315006256104
epoch 24 iter 3 sum correct 0.8466796875 loss 1.8919978141784668
epoch 24 iter 4 sum correct 0.84453125 loss 1.8921892642974854
epoch 24 iter 5 sum correct 0.8414713541666666 loss 1.892397403717041
epoch 24 iter 6 sum correct 0.8364955357142857 loss 1.892795205116272
epoch 24 iter 7 sum correct 0.835205078125 loss 1.8929121494293213
epoch 24 iter 8 sum correct 0.8374565972222222 loss 1.8927595615386963
epoch 24 iter 9 sum correct 0.8361328125 loss 1.892857551574707
epoch 24 iter 10 sum correct 0.8362926136363636 loss 1.8928827047348022
epoch 24 iter 11 sum correct 0.8367513020833334 loss 1.8928459882736206
epoch 24 iter 12 sum correct 0.8374399038461539 loss 1.8928016424179077
epoch 24 iter 13 sum correct 0.8366350446428571 loss 1.8928546905517578
epoch 24 iter 14 sum correct 0.8369791666666667 loss 1.892846941947937
epoch 24 iter 15 sum correct 0.8353271484375 loss 1.8929715156555176
epoch 24 iter 16 sum correct 0.8363970588235294 loss 1.892900824546814
epoch 24 iter 17 sum correct 0.8368055555555556 loss 1.8928710222244263
epoch 24 iter 18 sum correct 0.8365542763157895 loss 1.8929132223129272
epoch 24 iter 19 sum correct 0.8341796875 loss 1.8931076526641846
epoch 24 iter 20 sum correct 0.8338913690476191 loss 1.8931305408477783
epoch 24 iter 21 sum correct 0.8326526988636364 loss 1.8932123184204102
epoch 24 iter 22 sum correct 0.8316066576086957 loss 1.8932791948318481
epoch 24 iter 23 sum correct 0.830810546875 loss 1.8933417797088623
epoch 24 iter 24 sum correct 0.8309375 loss 1.8933401107788086
epoch 24 iter 25 sum correct 0.8298527644230769 loss 1.8934253454208374
epoch 24 iter 26 sum correct 0.830078125 loss 1.8934087753295898
epoch 24 iter 27 sum correct 0.8300083705357143 loss 1.8934059143066406
epoch 24 iter 28 sum correct 0.8304148706896551 loss 1.8933717012405396
epoch 24 iter 29 sum correct 0.83046875 loss 1.8933573961257935
epoch 24 iter 30 sum correct 0.8307081653225806 loss 1.893334984779358
epoch 24 iter 31 sum correct 0.83062744140625 loss 1.893341302871704
epoch 24 iter 32 sum correct 0.8306699810606061 loss 1.8933407068252563
epoch 24 iter 33 sum correct 0.8316291360294118 loss 1.8932663202285767
epoch 24 iter 34 sum correct 0.8318638392857143 loss 1.893252968788147
epoch 24 iter 35 sum correct 0.8321397569444444 loss 1.8932304382324219
epoch 24 iter 36 sum correct 0.8327174831081081 loss 1.8931858539581299
epoch 24 iter 37 sum correct 0.8326480263157895 loss 1.893183708190918
epoch 24 iter 38 sum correct 0.832832532051282 loss 1.8931689262390137
epoch 24 iter 39 sum correct 0.832421875 loss 1.8931931257247925
epoch 24 iter 40 sum correct 0.8323647103658537 loss 1.8931972980499268
epoch 24 iter 41 sum correct 0.8319847470238095 loss 1.8932262659072876
epoch 24 iter 42 sum correct 0.8319858284883721 loss 1.8932240009307861
epoch 24 iter 43 sum correct 0.8322531960227273 loss 1.8932039737701416
epoch 24 iter 44 sum correct 0.8324652777777778 loss 1.8931934833526611
epoch 24 iter 45 sum correct 0.8324983016304348 loss 1.8931916952133179
epoch 24 iter 46 sum correct 0.8323221409574468 loss 1.893198847770691
epoch 24 iter 47 sum correct 0.8327229817708334 loss 1.893169641494751
epoch 24 iter 48 sum correct 0.8329081632653061 loss 1.8931516408920288
epoch 24 iter 49 sum correct 0.8333203125 loss 1.8931220769882202
epoch 24 iter 50 sum correct 0.8336014093137255 loss 1.893104076385498
epoch 24 iter 51 sum correct 0.8336838942307693 loss 1.8930981159210205
epoch 24 iter 52 sum correct 0.8338738207547169 loss 1.8930789232254028
epoch 24 iter 53 sum correct 0.8342737268518519 loss 1.8930449485778809
epoch 24 iter 54 sum correct 0.8344815340909091 loss 1.8930245637893677
epoch 24 iter 55 sum correct 0.8341587611607143 loss 1.8930426836013794
epoch 24 iter 56 sum correct 0.8203467653508771 loss 1.8932723999023438
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.67it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.32it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.75it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.03it/s]8it [00:01,  5.95it/s]                                 macro  0.5227083812245137
micro  0.6168849261632767
[[292   0  57  17  58  11  32]
 [ 30   0   9   3  12   0   2]
 [ 61   0 248  13  91  37  46]
 [ 41   0  36 720  24  28  46]
 [ 93   0  99  32 316  12 101]
 [ 15   0  45  15  12 324   4]
 [ 69   0  49  56 104  15 314]]
              precision    recall  f1-score   support

           0       0.49      0.63      0.55       467
           1       0.00      0.00      0.00        56
           2       0.46      0.50      0.48       496
           3       0.84      0.80      0.82       895
           4       0.51      0.48      0.50       653
           5       0.76      0.78      0.77       415
           6       0.58      0.52      0.55       607

    accuracy                           0.62      3589
   macro avg       0.52      0.53      0.52      3589
weighted avg       0.61      0.62      0.61      3589

correct 0.6168849261632767 
f1 0.5227083812245137 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.78it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.48it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.84it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.10it/s]8it [00:01,  6.03it/s]                                 macro  0.5344204320758381
micro  0.6327667874059627
[[303   0  66  17  63   5  37]
 [ 29   0  11   7   5   1   2]
 [ 80   0 274  11  78  45  40]
 [ 35   0  30 738  23  24  29]
 [ 80   0 104  32 282   9  87]
 [ 10   0  62  14  11 305  14]
 [ 65   0  53  43  81  15 369]]
              precision    recall  f1-score   support

           0       0.50      0.62      0.55       491
           1       0.00      0.00      0.00        55
           2       0.46      0.52      0.49       528
           3       0.86      0.84      0.85       879
           4       0.52      0.47      0.50       594
           5       0.75      0.73      0.74       416
           6       0.64      0.59      0.61       626

    accuracy                           0.63      3589
   macro avg       0.53      0.54      0.53      3589
weighted avg       0.63      0.63      0.63      3589

correct 0.6327667874059627 
f1 0.5344204320758381 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:39,  1.40it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.30it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.83it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.27it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.45it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.70it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.76it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.92it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.90it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.02it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.99it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.02it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  3.98it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.07it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  3.97it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.01it/s] 30%|███       | 17/56.072265625 [00:04<00:10,  3.91it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  3.98it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  3.96it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.01it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  3.95it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.04it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  3.96it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.06it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.00it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.09it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.00it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.09it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.01it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.10it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.05it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.08it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.03it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.11it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.05it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.12it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  3.95it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  3.99it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  3.92it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:04,  3.97it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  3.94it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.05it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  3.99it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.08it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.04it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.11it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  4.04it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.12it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.06it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.14it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  4.07it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.15it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.10it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.16it/s] 98%|█████████▊| 55/56.072265625 [00:14<00:00,  4.10it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.16it/s]57it [00:14,  5.03it/s]                                  57it [00:14,  3.94it/s]epoch 25 iter 0 sum correct 0.8515625 loss 1.8914635181427002
epoch 25 iter 1 sum correct 0.8447265625 loss 1.8921904563903809
epoch 25 iter 2 sum correct 0.85546875 loss 1.8913602828979492
epoch 25 iter 3 sum correct 0.84619140625 loss 1.8920331001281738
epoch 25 iter 4 sum correct 0.844921875 loss 1.8921699523925781
epoch 25 iter 5 sum correct 0.8421223958333334 loss 1.8924329280853271
epoch 25 iter 6 sum correct 0.8373325892857143 loss 1.892796277999878
epoch 25 iter 7 sum correct 0.8349609375 loss 1.8929486274719238
epoch 25 iter 8 sum correct 0.8385416666666666 loss 1.8926969766616821
epoch 25 iter 9 sum correct 0.8369140625 loss 1.8928076028823853
epoch 25 iter 10 sum correct 0.833984375 loss 1.8930144309997559
epoch 25 iter 11 sum correct 0.8351236979166666 loss 1.8929446935653687
epoch 25 iter 12 sum correct 0.8363882211538461 loss 1.8928523063659668
epoch 25 iter 13 sum correct 0.8355189732142857 loss 1.8929378986358643
epoch 25 iter 14 sum correct 0.8354166666666667 loss 1.8929401636123657
epoch 25 iter 15 sum correct 0.836181640625 loss 1.8928816318511963
epoch 25 iter 16 sum correct 0.8366268382352942 loss 1.892839789390564
epoch 25 iter 17 sum correct 0.8371310763888888 loss 1.8927942514419556
epoch 25 iter 18 sum correct 0.8369654605263158 loss 1.892803430557251
epoch 25 iter 19 sum correct 0.83876953125 loss 1.8926782608032227
epoch 25 iter 20 sum correct 0.8400297619047619 loss 1.892573595046997
epoch 25 iter 21 sum correct 0.8401100852272727 loss 1.8925673961639404
epoch 25 iter 22 sum correct 0.8411175271739131 loss 1.8924840688705444
epoch 25 iter 23 sum correct 0.8416341145833334 loss 1.892425775527954
epoch 25 iter 24 sum correct 0.840859375 loss 1.8924775123596191
epoch 25 iter 25 sum correct 0.8417217548076923 loss 1.8924174308776855
epoch 25 iter 26 sum correct 0.8402054398148148 loss 1.8925361633300781
epoch 25 iter 27 sum correct 0.8410993303571429 loss 1.892469882965088
epoch 25 iter 28 sum correct 0.8409886853448276 loss 1.892481803894043
epoch 25 iter 29 sum correct 0.8404947916666666 loss 1.8925126791000366
epoch 25 iter 30 sum correct 0.8406628024193549 loss 1.8925001621246338
epoch 25 iter 31 sum correct 0.8404541015625 loss 1.8925169706344604
epoch 25 iter 32 sum correct 0.8393110795454546 loss 1.8925995826721191
epoch 25 iter 33 sum correct 0.8387522977941176 loss 1.8926342725753784
epoch 25 iter 34 sum correct 0.8391183035714286 loss 1.892602801322937
epoch 25 iter 35 sum correct 0.8395182291666666 loss 1.892569899559021
epoch 25 iter 36 sum correct 0.8392103040540541 loss 1.8925955295562744
epoch 25 iter 37 sum correct 0.8389185855263158 loss 1.8926177024841309
epoch 25 iter 38 sum correct 0.8394431089743589 loss 1.8925795555114746
epoch 25 iter 39 sum correct 0.83955078125 loss 1.892565131187439
epoch 25 iter 40 sum correct 0.8397008384146342 loss 1.8925576210021973
epoch 25 iter 41 sum correct 0.8400297619047619 loss 1.8925327062606812
epoch 25 iter 42 sum correct 0.8402525436046512 loss 1.8925130367279053
epoch 25 iter 43 sum correct 0.8400213068181818 loss 1.8925279378890991
epoch 25 iter 44 sum correct 0.8399739583333333 loss 1.8925310373306274
epoch 25 iter 45 sum correct 0.8400985054347826 loss 1.8925178050994873
epoch 25 iter 46 sum correct 0.8405086436170213 loss 1.8924853801727295
epoch 25 iter 47 sum correct 0.84033203125 loss 1.89249587059021
epoch 25 iter 48 sum correct 0.8405213647959183 loss 1.892478585243225
epoch 25 iter 49 sum correct 0.8403125 loss 1.8924912214279175
epoch 25 iter 50 sum correct 0.8407628676470589 loss 1.8924593925476074
epoch 25 iter 51 sum correct 0.8404071514423077 loss 1.8924826383590698
epoch 25 iter 52 sum correct 0.8408755896226415 loss 1.8924483060836792
epoch 25 iter 53 sum correct 0.8412905092592593 loss 1.8924143314361572
epoch 25 iter 54 sum correct 0.8417613636363637 loss 1.892377257347107
epoch 25 iter 55 sum correct 0.8421805245535714 loss 1.892345905303955
epoch 25 iter 56 sum correct 0.8283648574561403 loss 1.8924788236618042
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.56it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.14it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.40it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.72it/s]8it [00:01,  5.69it/s]                                 macro  0.518675317637233
micro  0.6199498467539705
[[222   0  41  41  72   8  83]
 [ 15   0  13   9  11   0   8]
 [ 38   0 200  43 104  25  86]
 [  9   0   9 776  22  11  68]
 [ 50   0  55  44 319   7 178]
 [ 15   0  38  30  14 281  37]
 [ 20   0  20  58  80   2 427]]
              precision    recall  f1-score   support

           0       0.60      0.48      0.53       467
           1       0.00      0.00      0.00        56
           2       0.53      0.40      0.46       496
           3       0.78      0.87      0.82       895
           4       0.51      0.49      0.50       653
           5       0.84      0.68      0.75       415
           6       0.48      0.70      0.57       607

    accuracy                           0.62      3589
   macro avg       0.53      0.52      0.52      3589
weighted avg       0.62      0.62      0.61      3589

correct 0.6199498467539705 
f1 0.518675317637233 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.85it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.53it/s] 71%|███████▏  | 5/7.009765625 [00:00<00:00,  6.09it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.32it/s]8it [00:01,  6.24it/s]                                 macro  0.5307438814770766
micro  0.6372248537196991
[[239   0  57  29  75   6  85]
 [ 18   0  10   8  11   1   7]
 [ 45   0 227  30  97  31  98]
 [  8   0  12 788  20   9  42]
 [ 39   0  45  39 300   6 165]
 [ 13   0  46  37  11 266  43]
 [ 27   0  13  48  67   4 467]]
              precision    recall  f1-score   support

           0       0.61      0.49      0.54       491
           1       0.00      0.00      0.00        55
           2       0.55      0.43      0.48       528
           3       0.80      0.90      0.85       879
           4       0.52      0.51      0.51       594
           5       0.82      0.64      0.72       416
           6       0.51      0.75      0.61       626

    accuracy                           0.64      3589
   macro avg       0.55      0.53      0.53      3589
weighted avg       0.63      0.64      0.63      3589

correct 0.6372248537196991 
f1 0.5307438814770766 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:39,  1.40it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.31it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.83it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.28it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.46it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.70it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.77it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.94it/s] 16%|█▌        | 9/56.072265625 [00:02<00:11,  3.93it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.05it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  4.02it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.11it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  4.06it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.13it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.07it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.14it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.04it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.09it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.02it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.11it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.05it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.13it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.05it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.13it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.06it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.13it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.06it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.13it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.07it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.14it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.08it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.15it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.09it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.16it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.09it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.16it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.09it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.16it/s] 70%|██████▉   | 39/56.072265625 [00:09<00:04,  4.09it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.15it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.09it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.15it/s] 77%|███████▋  | 43/56.072265625 [00:10<00:03,  4.10it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.17it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.04it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.06it/s] 84%|████████▍ | 47/56.072265625 [00:11<00:02,  4.01it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.07it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  3.99it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.08it/s] 91%|█████████ | 51/56.072265625 [00:12<00:01,  4.05it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.13it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.08it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.15it/s] 98%|█████████▊| 55/56.072265625 [00:13<00:00,  4.09it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.15it/s]57it [00:14,  4.96it/s]                                  57it [00:14,  3.98it/s]epoch 26 iter 0 sum correct 0.865234375 loss 1.890449047088623
epoch 26 iter 1 sum correct 0.84765625 loss 1.8917917013168335
epoch 26 iter 2 sum correct 0.8463541666666666 loss 1.8919100761413574
epoch 26 iter 3 sum correct 0.84716796875 loss 1.8918819427490234
epoch 26 iter 4 sum correct 0.846875 loss 1.891903281211853
epoch 26 iter 5 sum correct 0.841796875 loss 1.892256736755371
epoch 26 iter 6 sum correct 0.8395647321428571 loss 1.892412543296814
epoch 26 iter 7 sum correct 0.84033203125 loss 1.8923463821411133
epoch 26 iter 8 sum correct 0.8331163194444444 loss 1.8929144144058228
epoch 26 iter 9 sum correct 0.8330078125 loss 1.8929413557052612
epoch 26 iter 10 sum correct 0.8293678977272727 loss 1.8932158946990967
epoch 26 iter 11 sum correct 0.8271484375 loss 1.893385887145996
epoch 26 iter 12 sum correct 0.8278245192307693 loss 1.8933427333831787
epoch 26 iter 13 sum correct 0.8284040178571429 loss 1.8932957649230957
epoch 26 iter 14 sum correct 0.8286458333333333 loss 1.8932857513427734
epoch 26 iter 15 sum correct 0.829833984375 loss 1.893197774887085
epoch 26 iter 16 sum correct 0.8286994485294118 loss 1.893292784690857
epoch 26 iter 17 sum correct 0.8286675347222222 loss 1.8932993412017822
epoch 26 iter 18 sum correct 0.8294613486842105 loss 1.8932605981826782
epoch 26 iter 19 sum correct 0.83154296875 loss 1.893108606338501
epoch 26 iter 20 sum correct 0.8326822916666666 loss 1.893009066581726
epoch 26 iter 21 sum correct 0.8323863636363636 loss 1.8930219411849976
epoch 26 iter 22 sum correct 0.8325407608695652 loss 1.8929921388626099
epoch 26 iter 23 sum correct 0.8333333333333334 loss 1.8929427862167358
epoch 26 iter 24 sum correct 0.83203125 loss 1.8930326700210571
epoch 26 iter 25 sum correct 0.8319561298076923 loss 1.8930343389511108
epoch 26 iter 26 sum correct 0.8330439814814815 loss 1.8929508924484253
epoch 26 iter 27 sum correct 0.8332170758928571 loss 1.8929426670074463
epoch 26 iter 28 sum correct 0.8333108836206896 loss 1.8929312229156494
epoch 26 iter 29 sum correct 0.8330729166666667 loss 1.8929556608200073
epoch 26 iter 30 sum correct 0.8331653225806451 loss 1.8929500579833984
epoch 26 iter 31 sum correct 0.83282470703125 loss 1.892983317375183
epoch 26 iter 32 sum correct 0.8322679924242424 loss 1.8930325508117676
epoch 26 iter 33 sum correct 0.8324908088235294 loss 1.893004298210144
epoch 26 iter 34 sum correct 0.8327566964285714 loss 1.8929855823516846
epoch 26 iter 35 sum correct 0.8333875868055556 loss 1.892926812171936
epoch 26 iter 36 sum correct 0.8335620777027027 loss 1.892906665802002
epoch 26 iter 37 sum correct 0.8334703947368421 loss 1.8929129838943481
epoch 26 iter 38 sum correct 0.8333834134615384 loss 1.8929164409637451
epoch 26 iter 39 sum correct 0.83369140625 loss 1.8928899765014648
epoch 26 iter 40 sum correct 0.8338414634146342 loss 1.8928730487823486
epoch 26 iter 41 sum correct 0.8338448660714286 loss 1.8928738832473755
epoch 26 iter 42 sum correct 0.8337118459302325 loss 1.8928781747817993
epoch 26 iter 43 sum correct 0.8339399857954546 loss 1.8928606510162354
epoch 26 iter 44 sum correct 0.8340711805555555 loss 1.8928450345993042
epoch 26 iter 45 sum correct 0.8340268342391305 loss 1.8928500413894653
epoch 26 iter 46 sum correct 0.833984375 loss 1.8928598165512085
epoch 26 iter 47 sum correct 0.8341878255208334 loss 1.8928364515304565
epoch 26 iter 48 sum correct 0.8344228316326531 loss 1.892815113067627
epoch 26 iter 49 sum correct 0.8348046875 loss 1.892788290977478
epoch 26 iter 50 sum correct 0.8349034926470589 loss 1.8927817344665527
epoch 26 iter 51 sum correct 0.8351111778846154 loss 1.8927658796310425
epoch 26 iter 52 sum correct 0.8350899174528302 loss 1.8927605152130127
epoch 26 iter 53 sum correct 0.8346715856481481 loss 1.8927910327911377
epoch 26 iter 54 sum correct 0.8346235795454545 loss 1.8927946090698242
epoch 26 iter 55 sum correct 0.8346121651785714 loss 1.8927888870239258
epoch 26 iter 56 sum correct 0.8209978070175439 loss 1.89283287525177
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.51it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.06it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.27it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.58it/s]8it [00:01,  5.48it/s]                                 macro  0.5189291581266162
micro  0.6216216216216216
[[175   0  46  29  80  28 109]
 [ 14   0  13   5  11   1  12]
 [ 25   0 216  16 105  57  77]
 [  9   0  16 729  22  25  94]
 [ 15   0  81  32 330  20 175]
 [  3   0  22  14  10 348  18]
 [  4   0  27  37  90  16 433]]
              precision    recall  f1-score   support

           0       0.71      0.37      0.49       467
           1       0.00      0.00      0.00        56
           2       0.51      0.44      0.47       496
           3       0.85      0.81      0.83       895
           4       0.51      0.51      0.51       653
           5       0.70      0.84      0.76       415
           6       0.47      0.71      0.57       607

    accuracy                           0.62      3589
   macro avg       0.54      0.53      0.52      3589
weighted avg       0.63      0.62      0.61      3589

correct 0.6216216216216216 
f1 0.5189291581266162 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.80it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.51it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.79it/s] 86%|████████▌ | 6/7.009765625 [00:01<00:00,  6.53it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.13it/s]8it [00:01,  5.90it/s]                                 macro  0.5258675553807592
micro  0.6310950125383115
[[188   0  61  23  71  25 123]
 [  9   0  20   2  14   2   8]
 [ 33   0 237  23  93  61  81]
 [  7   0  20 747  28  19  58]
 [ 12   0  74  29 299   9 171]
 [  3   0  24  22  11 330  26]
 [  7   0  32  21  87  15 464]]
              precision    recall  f1-score   support

           0       0.73      0.38      0.50       491
           1       0.00      0.00      0.00        55
           2       0.51      0.45      0.48       528
           3       0.86      0.85      0.86       879
           4       0.50      0.50      0.50       594
           5       0.72      0.79      0.75       416
           6       0.50      0.74      0.60       626

    accuracy                           0.63      3589
   macro avg       0.54      0.53      0.53      3589
weighted avg       0.64      0.63      0.62      3589

correct 0.6310950125383115 
f1 0.5258675553807592 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:38,  1.43it/s]  4%|▎         | 2/56.072265625 [00:00<00:22,  2.35it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.87it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.31it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.46it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.71it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.77it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.93it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.92it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.04it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  4.01it/s] 21%|██▏       | 12/56.072265625 [00:03<00:11,  3.85it/s] 23%|██▎       | 13/56.072265625 [00:03<00:11,  3.88it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  3.99it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  3.96it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.05it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.01it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.09it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.04it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.12it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.06it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.12it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.06it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.13it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.05it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.13it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.06it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.12it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.05it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.12it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.05it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.12it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.06it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.13it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.07it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.13it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.07it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.13it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  4.06it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.13it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.06it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.13it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  4.07it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.13it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.06it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.12it/s] 84%|████████▍ | 47/56.072265625 [00:11<00:02,  4.04it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.11it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.03it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.11it/s] 91%|█████████ | 51/56.072265625 [00:12<00:01,  4.05it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.13it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.08it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.14it/s] 98%|█████████▊| 55/56.072265625 [00:13<00:00,  4.08it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.14it/s]57it [00:14,  4.96it/s]                                  57it [00:14,  3.97it/s]epoch 27 iter 0 sum correct 0.833984375 loss 1.8928241729736328
epoch 27 iter 1 sum correct 0.8447265625 loss 1.8919634819030762
epoch 27 iter 2 sum correct 0.845703125 loss 1.8918101787567139
epoch 27 iter 3 sum correct 0.84033203125 loss 1.892171859741211
epoch 27 iter 4 sum correct 0.834765625 loss 1.8925819396972656
epoch 27 iter 5 sum correct 0.8365885416666666 loss 1.8924870491027832
epoch 27 iter 6 sum correct 0.8387276785714286 loss 1.892331838607788
epoch 27 iter 7 sum correct 0.834716796875 loss 1.892666220664978
epoch 27 iter 8 sum correct 0.8376736111111112 loss 1.8924427032470703
epoch 27 iter 9 sum correct 0.83671875 loss 1.892533302307129
epoch 27 iter 10 sum correct 0.8366477272727273 loss 1.8925285339355469
epoch 27 iter 11 sum correct 0.8346354166666666 loss 1.8926763534545898
epoch 27 iter 12 sum correct 0.837890625 loss 1.8924357891082764
epoch 27 iter 13 sum correct 0.8370535714285714 loss 1.8925013542175293
epoch 27 iter 14 sum correct 0.8375 loss 1.8924649953842163
epoch 27 iter 15 sum correct 0.837158203125 loss 1.892497181892395
epoch 27 iter 16 sum correct 0.8377757352941176 loss 1.8924415111541748
epoch 27 iter 17 sum correct 0.8372395833333334 loss 1.892486572265625
epoch 27 iter 18 sum correct 0.8376850328947368 loss 1.8924448490142822
epoch 27 iter 19 sum correct 0.8375 loss 1.8924559354782104
epoch 27 iter 20 sum correct 0.8367745535714286 loss 1.8925070762634277
epoch 27 iter 21 sum correct 0.8371803977272727 loss 1.892470359802246
epoch 27 iter 22 sum correct 0.8368716032608695 loss 1.8924813270568848
epoch 27 iter 23 sum correct 0.8370768229166666 loss 1.8924667835235596
epoch 27 iter 24 sum correct 0.836875 loss 1.892474889755249
epoch 27 iter 25 sum correct 0.8365384615384616 loss 1.8924970626831055
epoch 27 iter 26 sum correct 0.8368055555555556 loss 1.8924839496612549
epoch 27 iter 27 sum correct 0.8376813616071429 loss 1.8924148082733154
epoch 27 iter 28 sum correct 0.8372171336206896 loss 1.8924496173858643
epoch 27 iter 29 sum correct 0.8375 loss 1.892417073249817
epoch 27 iter 30 sum correct 0.8381426411290323 loss 1.8923633098602295
epoch 27 iter 31 sum correct 0.838134765625 loss 1.892362356185913
epoch 27 iter 32 sum correct 0.838186553030303 loss 1.8923512697219849
epoch 27 iter 33 sum correct 0.8395565257352942 loss 1.8922464847564697
epoch 27 iter 34 sum correct 0.8389508928571429 loss 1.8922877311706543
epoch 27 iter 35 sum correct 0.8395724826388888 loss 1.8922367095947266
epoch 27 iter 36 sum correct 0.8400548986486487 loss 1.8922053575515747
epoch 27 iter 37 sum correct 0.8406661184210527 loss 1.8921667337417603
epoch 27 iter 38 sum correct 0.840645032051282 loss 1.8921692371368408
epoch 27 iter 39 sum correct 0.840966796875 loss 1.8921558856964111
epoch 27 iter 40 sum correct 0.8413681402439024 loss 1.8921267986297607
epoch 27 iter 41 sum correct 0.8416108630952381 loss 1.892110824584961
epoch 27 iter 42 sum correct 0.8419785610465116 loss 1.892086148262024
epoch 27 iter 43 sum correct 0.8422407670454546 loss 1.892059326171875
epoch 27 iter 44 sum correct 0.8421875 loss 1.892065167427063
epoch 27 iter 45 sum correct 0.8426885190217391 loss 1.8920265436172485
epoch 27 iter 46 sum correct 0.842295545212766 loss 1.8920564651489258
epoch 27 iter 47 sum correct 0.8422444661458334 loss 1.8920648097991943
epoch 27 iter 48 sum correct 0.8427136479591837 loss 1.8920303583145142
epoch 27 iter 49 sum correct 0.8426171875 loss 1.8920321464538574
epoch 27 iter 50 sum correct 0.8428691789215687 loss 1.8920109272003174
epoch 27 iter 51 sum correct 0.8429987980769231 loss 1.8920035362243652
epoch 27 iter 52 sum correct 0.8427181603773585 loss 1.892024278640747
epoch 27 iter 53 sum correct 0.8429181134259259 loss 1.8920050859451294
epoch 27 iter 54 sum correct 0.8427556818181818 loss 1.8920179605484009
epoch 27 iter 55 sum correct 0.8428780691964286 loss 1.8920074701309204
epoch 27 iter 56 sum correct 0.8292557565789473 loss 1.8918925523757935
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.61it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.09it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.49it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.82it/s]8it [00:01,  5.80it/s]                                 macro  0.5072612789615188
micro  0.6076901643911953
[[268   0  51  22  49  28  49]
 [ 26   0   8   5   8   2   7]
 [ 62   0 228  11  77  70  48]
 [ 32   0  17 730  21  43  52]
 [ 83   0 111  25 289  39 106]
 [ 10   0  29  15   6 351   4]
 [ 63   0  52  50  92  35 315]]
              precision    recall  f1-score   support

           0       0.49      0.57      0.53       467
           1       0.00      0.00      0.00        56
           2       0.46      0.46      0.46       496
           3       0.85      0.82      0.83       895
           4       0.53      0.44      0.48       653
           5       0.62      0.85      0.71       415
           6       0.54      0.52      0.53       607

    accuracy                           0.61      3589
   macro avg       0.50      0.52      0.51      3589
weighted avg       0.60      0.61      0.60      3589

correct 0.6076901643911953 
f1 0.5072612789615188 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.78it/s] 29%|██▊       | 2/7.009765625 [00:00<00:01,  3.44it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.69it/s] 71%|███████▏  | 5/7.009765625 [00:00<00:00,  6.55it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.82it/s]8it [00:01,  6.21it/s]                                 macro  0.5267515249643921
micro  0.6305377542490944
[[287   0  78  24  32  18  52]
 [ 28   0  13   5   3   2   4]
 [ 71   0 247  18  74  82  36]
 [ 22   0  19 742  28  30  38]
 [ 73   0 107  27 251  22 114]
 [ 12   0  25  18   2 351   8]
 [ 56   0  57  33  62  33 385]]
              precision    recall  f1-score   support

           0       0.52      0.58      0.55       491
           1       0.00      0.00      0.00        55
           2       0.45      0.47      0.46       528
           3       0.86      0.84      0.85       879
           4       0.56      0.42      0.48       594
           5       0.65      0.84      0.74       416
           6       0.60      0.62      0.61       626

    accuracy                           0.63      3589
   macro avg       0.52      0.54      0.53      3589
weighted avg       0.62      0.63      0.62      3589

correct 0.6305377542490944 
f1 0.5267515249643921 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:38,  1.45it/s]  4%|▎         | 2/56.072265625 [00:00<00:22,  2.36it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.87it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.31it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.46it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.71it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.74it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.91it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.88it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.01it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.95it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.05it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  3.98it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.07it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.00it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.08it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.01it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.09it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  3.94it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.03it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  3.98it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.08it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.00it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.09it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.01it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.09it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.01it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.10it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.02it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.10it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  3.97it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.06it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  3.99it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.08it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.01it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.09it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.00it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.08it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  4.01it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.09it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.01it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.09it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  3.98it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.08it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  3.98it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.07it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  3.94it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.05it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  3.97it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.06it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  4.01it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.10it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.02it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.10it/s] 98%|█████████▊| 55/56.072265625 [00:14<00:00,  4.00it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.09it/s]57it [00:14,  4.89it/s]                                  57it [00:14,  3.94it/s]epoch 28 iter 0 sum correct 0.85546875 loss 1.8909543752670288
epoch 28 iter 1 sum correct 0.8447265625 loss 1.8917605876922607
epoch 28 iter 2 sum correct 0.8450520833333334 loss 1.891722559928894
epoch 28 iter 3 sum correct 0.8466796875 loss 1.8915904760360718
epoch 28 iter 4 sum correct 0.84140625 loss 1.8919991254806519
epoch 28 iter 5 sum correct 0.8388671875 loss 1.8921998739242554
epoch 28 iter 6 sum correct 0.8323102678571429 loss 1.8926938772201538
epoch 28 iter 7 sum correct 0.831298828125 loss 1.8928056955337524
epoch 28 iter 8 sum correct 0.8344184027777778 loss 1.8925853967666626
epoch 28 iter 9 sum correct 0.836328125 loss 1.8924916982650757
epoch 28 iter 10 sum correct 0.8357599431818182 loss 1.892524242401123
epoch 28 iter 11 sum correct 0.8346354166666666 loss 1.8926061391830444
epoch 28 iter 12 sum correct 0.8354867788461539 loss 1.892513632774353
epoch 28 iter 13 sum correct 0.8346819196428571 loss 1.8925738334655762
epoch 28 iter 14 sum correct 0.8337239583333333 loss 1.8926423788070679
epoch 28 iter 15 sum correct 0.8331298828125 loss 1.8927050828933716
epoch 28 iter 16 sum correct 0.8326056985294118 loss 1.8927528858184814
epoch 28 iter 17 sum correct 0.8310546875 loss 1.8928735256195068
epoch 28 iter 18 sum correct 0.8302837171052632 loss 1.8929499387741089
epoch 28 iter 19 sum correct 0.82919921875 loss 1.8930330276489258
epoch 28 iter 20 sum correct 0.8272879464285714 loss 1.8931807279586792
epoch 28 iter 21 sum correct 0.8265269886363636 loss 1.8932523727416992
epoch 28 iter 22 sum correct 0.8257472826086957 loss 1.8933053016662598
epoch 28 iter 23 sum correct 0.825927734375 loss 1.8932946920394897
epoch 28 iter 24 sum correct 0.826796875 loss 1.8932440280914307
epoch 28 iter 25 sum correct 0.8269230769230769 loss 1.8932276964187622
epoch 28 iter 26 sum correct 0.8284866898148148 loss 1.893111228942871
epoch 28 iter 27 sum correct 0.8287527901785714 loss 1.893089771270752
epoch 28 iter 28 sum correct 0.8283270474137931 loss 1.8931257724761963
epoch 28 iter 29 sum correct 0.828125 loss 1.893132209777832
epoch 28 iter 30 sum correct 0.8285660282258065 loss 1.8930981159210205
epoch 28 iter 31 sum correct 0.8287353515625 loss 1.893085241317749
epoch 28 iter 32 sum correct 0.8282433712121212 loss 1.8931243419647217
epoch 28 iter 33 sum correct 0.8281824448529411 loss 1.893133521080017
epoch 28 iter 34 sum correct 0.8280691964285715 loss 1.8931376934051514
epoch 28 iter 35 sum correct 0.8278537326388888 loss 1.89315927028656
epoch 28 iter 36 sum correct 0.8279666385135135 loss 1.8931491374969482
epoch 28 iter 37 sum correct 0.8276110197368421 loss 1.893172264099121
epoch 28 iter 38 sum correct 0.8282251602564102 loss 1.8931224346160889
epoch 28 iter 39 sum correct 0.828662109375 loss 1.893082857131958
epoch 28 iter 40 sum correct 0.8289824695121951 loss 1.8930538892745972
epoch 28 iter 41 sum correct 0.8283110119047619 loss 1.8930968046188354
epoch 28 iter 42 sum correct 0.8289880087209303 loss 1.8930439949035645
epoch 28 iter 43 sum correct 0.8288796164772727 loss 1.8930457830429077
epoch 28 iter 44 sum correct 0.8292100694444444 loss 1.8930270671844482
epoch 28 iter 45 sum correct 0.8290166440217391 loss 1.8930386304855347
epoch 28 iter 46 sum correct 0.829330119680851 loss 1.8930132389068604
epoch 28 iter 47 sum correct 0.8296712239583334 loss 1.8929848670959473
epoch 28 iter 48 sum correct 0.8299186862244898 loss 1.8929667472839355
epoch 28 iter 49 sum correct 0.8295703125 loss 1.8929901123046875
epoch 28 iter 50 sum correct 0.8299249387254902 loss 1.892959475517273
epoch 28 iter 51 sum correct 0.8302659254807693 loss 1.8929318189620972
epoch 28 iter 52 sum correct 0.8303360849056604 loss 1.8929290771484375
epoch 28 iter 53 sum correct 0.8304759837962963 loss 1.8929115533828735
epoch 28 iter 54 sum correct 0.8302556818181818 loss 1.8929269313812256
epoch 28 iter 55 sum correct 0.8302873883928571 loss 1.8929234743118286
epoch 28 iter 56 sum correct 0.8168174342105263 loss 1.8928842544555664
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.59it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.08it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.68it/s] 86%|████████▌ | 6/7.009765625 [00:01<00:00,  6.42it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.04it/s]8it [00:01,  5.72it/s]                                 macro  0.5194265036593421
micro  0.6227361382000557
[[252   0  32  28  95  19  41]
 [ 27   0   4   6  14   0   5]
 [ 58   0 168  27 145  51  47]
 [ 19   0  13 752  43  25  43]
 [ 67   0  46  43 401  12  84]
 [ 16   0  24  16  16 340   3]
 [ 46   0  22  59 143  15 322]]
              precision    recall  f1-score   support

           0       0.52      0.54      0.53       467
           1       0.00      0.00      0.00        56
           2       0.54      0.34      0.42       496
           3       0.81      0.84      0.82       895
           4       0.47      0.61      0.53       653
           5       0.74      0.82      0.78       415
           6       0.59      0.53      0.56       607

    accuracy                           0.62      3589
   macro avg       0.52      0.53      0.52      3589
weighted avg       0.61      0.62      0.61      3589

correct 0.6227361382000557 
f1 0.5194265036593421 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.82it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.55it/s] 71%|███████▏  | 5/7.009765625 [00:00<00:00,  5.94it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.19it/s]8it [00:01,  6.12it/s]                                 macro  0.530868076616539
micro  0.6327667874059627
[[279   0  38  27  96   8  43]
 [ 24   0   4   7  12   2   6]
 [ 71   0 203  30 133  51  40]
 [ 16   0  14 765  42  17  25]
 [ 61   0  56  33 352   8  84]
 [ 13   0  28  26  18 321  10]
 [ 43   0  27  45 138  22 351]]
              precision    recall  f1-score   support

           0       0.55      0.57      0.56       491
           1       0.00      0.00      0.00        55
           2       0.55      0.38      0.45       528
           3       0.82      0.87      0.84       879
           4       0.45      0.59      0.51       594
           5       0.75      0.77      0.76       416
           6       0.63      0.56      0.59       626

    accuracy                           0.63      3589
   macro avg       0.53      0.54      0.53      3589
weighted avg       0.63      0.63      0.63      3589

correct 0.6327667874059627 
f1 0.530868076616539 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:38,  1.44it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.34it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.81it/s]  7%|▋         | 4/56.072265625 [00:01<00:16,  3.22it/s]  9%|▉         | 5/56.072265625 [00:01<00:15,  3.40it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.64it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.71it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.88it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.88it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.00it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.98it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.08it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  4.04it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.12it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.05it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.13it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.00it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.09it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.03it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.11it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.05it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.13it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.07it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.14it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  3.94it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  3.78it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  3.80it/s] 50%|████▉     | 28/56.072265625 [00:07<00:07,  3.94it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:07,  3.82it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  3.95it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  3.94it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.04it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  3.99it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.07it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.03it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.11it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.00it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.09it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  3.97it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.07it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  3.97it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.07it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  3.97it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.06it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.03it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.11it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  4.05it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.12it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.06it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.14it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  4.10it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.15it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.09it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.17it/s] 98%|█████████▊| 55/56.072265625 [00:14<00:00,  4.11it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.17it/s]57it [00:14,  5.05it/s]                                  57it [00:14,  3.94it/s]epoch 29 iter 0 sum correct 0.8515625 loss 1.8912360668182373
epoch 29 iter 1 sum correct 0.837890625 loss 1.89235520362854
epoch 29 iter 2 sum correct 0.8346354166666666 loss 1.8925249576568604
epoch 29 iter 3 sum correct 0.833984375 loss 1.8925670385360718
epoch 29 iter 4 sum correct 0.833984375 loss 1.8925975561141968
epoch 29 iter 5 sum correct 0.8382161458333334 loss 1.8922332525253296
epoch 29 iter 6 sum correct 0.837890625 loss 1.8922405242919922
epoch 29 iter 7 sum correct 0.837890625 loss 1.8922275304794312
epoch 29 iter 8 sum correct 0.8365885416666666 loss 1.892343521118164
epoch 29 iter 9 sum correct 0.836328125 loss 1.8924047946929932
epoch 29 iter 10 sum correct 0.8373579545454546 loss 1.892329216003418
epoch 29 iter 11 sum correct 0.83740234375 loss 1.8923171758651733
epoch 29 iter 12 sum correct 0.8383413461538461 loss 1.892244815826416
epoch 29 iter 13 sum correct 0.8384486607142857 loss 1.8922456502914429
epoch 29 iter 14 sum correct 0.8397135416666667 loss 1.8921688795089722
epoch 29 iter 15 sum correct 0.8397216796875 loss 1.89217209815979
epoch 29 iter 16 sum correct 0.8386948529411765 loss 1.892252802848816
epoch 29 iter 17 sum correct 0.8387586805555556 loss 1.8922334909439087
epoch 29 iter 18 sum correct 0.8396381578947368 loss 1.8921749591827393
epoch 29 iter 19 sum correct 0.84052734375 loss 1.892098307609558
epoch 29 iter 20 sum correct 0.8395647321428571 loss 1.8921698331832886
epoch 29 iter 21 sum correct 0.8403764204545454 loss 1.8921020030975342
epoch 29 iter 22 sum correct 0.8406929347826086 loss 1.8920745849609375
epoch 29 iter 23 sum correct 0.8421223958333334 loss 1.891954779624939
epoch 29 iter 24 sum correct 0.843671875 loss 1.8918367624282837
epoch 29 iter 25 sum correct 0.8444260817307693 loss 1.8917863368988037
epoch 29 iter 26 sum correct 0.8439670138888888 loss 1.8918172121047974
epoch 29 iter 27 sum correct 0.8440290178571429 loss 1.8918153047561646
epoch 29 iter 28 sum correct 0.8444234913793104 loss 1.89177405834198
epoch 29 iter 29 sum correct 0.84453125 loss 1.8917659521102905
epoch 29 iter 30 sum correct 0.8450100806451613 loss 1.8917372226715088
epoch 29 iter 31 sum correct 0.8448486328125 loss 1.8917465209960938
epoch 29 iter 32 sum correct 0.8443418560606061 loss 1.8917852640151978
epoch 29 iter 33 sum correct 0.8442095588235294 loss 1.8918037414550781
epoch 29 iter 34 sum correct 0.8441964285714286 loss 1.8918075561523438
epoch 29 iter 35 sum correct 0.8446180555555556 loss 1.8917694091796875
epoch 29 iter 36 sum correct 0.8450696790540541 loss 1.89173424243927
epoch 29 iter 37 sum correct 0.8440583881578947 loss 1.8918061256408691
epoch 29 iter 38 sum correct 0.8452023237179487 loss 1.8917168378829956
epoch 29 iter 39 sum correct 0.845458984375 loss 1.8917006254196167
epoch 29 iter 40 sum correct 0.8458460365853658 loss 1.891669750213623
epoch 29 iter 41 sum correct 0.8461681547619048 loss 1.8916375637054443
epoch 29 iter 42 sum correct 0.8467478197674418 loss 1.891586422920227
epoch 29 iter 43 sum correct 0.8472123579545454 loss 1.8915408849716187
epoch 29 iter 44 sum correct 0.84765625 loss 1.8914997577667236
epoch 29 iter 45 sum correct 0.848038383152174 loss 1.8914650678634644
epoch 29 iter 46 sum correct 0.847905585106383 loss 1.8914763927459717
epoch 29 iter 47 sum correct 0.8479410807291666 loss 1.8914697170257568
epoch 29 iter 48 sum correct 0.8473772321428571 loss 1.8915106058120728
epoch 29 iter 49 sum correct 0.8468359375 loss 1.8915454149246216
epoch 29 iter 50 sum correct 0.8470052083333334 loss 1.8915302753448486
epoch 29 iter 51 sum correct 0.8469801682692307 loss 1.8915263414382935
epoch 29 iter 52 sum correct 0.8470666273584906 loss 1.8915154933929443
epoch 29 iter 53 sum correct 0.8467158564814815 loss 1.8915444612503052
epoch 29 iter 54 sum correct 0.8468039772727273 loss 1.8915364742279053
epoch 29 iter 55 sum correct 0.8465053013392857 loss 1.8915627002716064
epoch 29 iter 56 sum correct 0.8326137609649122 loss 1.8916974067687988
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.62it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.25it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.64it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.93it/s]8it [00:01,  5.82it/s]                                 macro  0.5319199509091533
micro  0.6327667874059627
[[255   0  35  45  64  10  58]
 [ 19   0  11   9  11   0   6]
 [ 59   0 234  30  89  23  61]
 [ 30   0  12 791  14  12  36]
 [ 56   0  72  63 331   6 125]
 [ 17   0  48  22   8 292  28]
 [ 35   0  43  72  88   1 368]]
              precision    recall  f1-score   support

           0       0.54      0.55      0.54       467
           1       0.00      0.00      0.00        56
           2       0.51      0.47      0.49       496
           3       0.77      0.88      0.82       895
           4       0.55      0.51      0.53       653
           5       0.85      0.70      0.77       415
           6       0.54      0.61      0.57       607

    accuracy                           0.63      3589
   macro avg       0.54      0.53      0.53      3589
weighted avg       0.62      0.63      0.62      3589

correct 0.6327667874059627 
f1 0.5319199509091533 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.82it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.56it/s] 71%|███████▏  | 5/7.009765625 [00:00<00:00,  5.98it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.18it/s]8it [00:01,  6.17it/s]                                 macro  0.540355917576263
micro  0.6441905823349122
[[286   0  53  29  60   2  61]
 [ 23   0  10  11   8   0   3]
 [ 76   0 246  29  89  29  59]
 [ 17   0  14 795  21  11  21]
 [ 46   0  64  64 305   7 108]
 [ 12   0  55  27  13 279  30]
 [ 35   0  44  74  65   7 401]]
              precision    recall  f1-score   support

           0       0.58      0.58      0.58       491
           1       0.00      0.00      0.00        55
           2       0.51      0.47      0.49       528
           3       0.77      0.90      0.83       879
           4       0.54      0.51      0.53       594
           5       0.83      0.67      0.74       416
           6       0.59      0.64      0.61       626

    accuracy                           0.64      3589
   macro avg       0.55      0.54      0.54      3589
weighted avg       0.63      0.64      0.64      3589

correct 0.6441905823349122 
f1 0.540355917576263 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:38,  1.43it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.35it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.80it/s]  7%|▋         | 4/56.072265625 [00:01<00:16,  3.20it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.41it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.66it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.73it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.89it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.89it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.01it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.99it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.08it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  4.04it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.12it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.07it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.14it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.08it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.13it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.05it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.11it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.06it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.13it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.07it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.14it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.08it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.15it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.05it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.13it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.06it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.12it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.04it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.12it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.06it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.13it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.08it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.15it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.08it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.15it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  4.08it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.15it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.08it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.15it/s] 77%|███████▋  | 43/56.072265625 [00:10<00:03,  4.06it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.13it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.08it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.15it/s] 84%|████████▍ | 47/56.072265625 [00:11<00:02,  4.08it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.15it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.08it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.15it/s] 91%|█████████ | 51/56.072265625 [00:12<00:01,  4.10it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.16it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.11it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.18it/s] 98%|█████████▊| 55/56.072265625 [00:13<00:00,  4.12it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.18it/s]57it [00:14,  5.05it/s]                                  57it [00:14,  3.99it/s]epoch 30 iter 0 sum correct 0.89453125 loss 1.8876292705535889
epoch 30 iter 1 sum correct 0.8798828125 loss 1.8887362480163574
epoch 30 iter 2 sum correct 0.865234375 loss 1.8899950981140137
epoch 30 iter 3 sum correct 0.857421875 loss 1.8906179666519165
epoch 30 iter 4 sum correct 0.851171875 loss 1.8911503553390503
epoch 30 iter 5 sum correct 0.84375 loss 1.8917044401168823
epoch 30 iter 6 sum correct 0.8451450892857143 loss 1.8916089534759521
epoch 30 iter 7 sum correct 0.84423828125 loss 1.891677975654602
epoch 30 iter 8 sum correct 0.84375 loss 1.891715168952942
epoch 30 iter 9 sum correct 0.84453125 loss 1.8916778564453125
epoch 30 iter 10 sum correct 0.8448153409090909 loss 1.8916311264038086
epoch 30 iter 11 sum correct 0.84521484375 loss 1.8915952444076538
epoch 30 iter 12 sum correct 0.8461538461538461 loss 1.8915140628814697
epoch 30 iter 13 sum correct 0.8473772321428571 loss 1.8914167881011963
epoch 30 iter 14 sum correct 0.847265625 loss 1.8914039134979248
epoch 30 iter 15 sum correct 0.846923828125 loss 1.8914378881454468
epoch 30 iter 16 sum correct 0.8451286764705882 loss 1.891597032546997
epoch 30 iter 17 sum correct 0.8460286458333334 loss 1.8915189504623413
epoch 30 iter 18 sum correct 0.8465254934210527 loss 1.8914868831634521
epoch 30 iter 19 sum correct 0.84814453125 loss 1.8913716077804565
epoch 30 iter 20 sum correct 0.8475632440476191 loss 1.8914062976837158
epoch 30 iter 21 sum correct 0.8487215909090909 loss 1.8913142681121826
epoch 30 iter 22 sum correct 0.8485903532608695 loss 1.8913192749023438
epoch 30 iter 23 sum correct 0.8489583333333334 loss 1.8912885189056396
epoch 30 iter 24 sum correct 0.848671875 loss 1.8913103342056274
epoch 30 iter 25 sum correct 0.8480318509615384 loss 1.8913565874099731
epoch 30 iter 26 sum correct 0.8492476851851852 loss 1.89125657081604
epoch 30 iter 27 sum correct 0.8503766741071429 loss 1.8911768198013306
epoch 30 iter 28 sum correct 0.8508890086206896 loss 1.8911359310150146
epoch 30 iter 29 sum correct 0.8513671875 loss 1.891097903251648
epoch 30 iter 30 sum correct 0.8512474798387096 loss 1.8911046981811523
epoch 30 iter 31 sum correct 0.851318359375 loss 1.89110267162323
epoch 30 iter 32 sum correct 0.8509706439393939 loss 1.891130805015564
epoch 30 iter 33 sum correct 0.8511603860294118 loss 1.8911148309707642
epoch 30 iter 34 sum correct 0.8509486607142858 loss 1.8911324739456177
epoch 30 iter 35 sum correct 0.8503146701388888 loss 1.8911798000335693
epoch 30 iter 36 sum correct 0.8504539695945946 loss 1.8911710977554321
epoch 30 iter 37 sum correct 0.8505859375 loss 1.8911620378494263
epoch 30 iter 38 sum correct 0.8501602564102564 loss 1.891196846961975
epoch 30 iter 39 sum correct 0.85078125 loss 1.8911561965942383
epoch 30 iter 40 sum correct 0.8511814024390244 loss 1.8911219835281372
epoch 30 iter 41 sum correct 0.8511904761904762 loss 1.891129493713379
epoch 30 iter 42 sum correct 0.8519258720930233 loss 1.8910737037658691
epoch 30 iter 43 sum correct 0.85205078125 loss 1.891056776046753
epoch 30 iter 44 sum correct 0.8524305555555556 loss 1.8910239934921265
epoch 30 iter 45 sum correct 0.8523692255434783 loss 1.891021728515625
epoch 30 iter 46 sum correct 0.8526845079787234 loss 1.890997052192688
epoch 30 iter 47 sum correct 0.8529866536458334 loss 1.8909721374511719
epoch 30 iter 48 sum correct 0.8532366071428571 loss 1.890946626663208
epoch 30 iter 49 sum correct 0.85390625 loss 1.8908934593200684
epoch 30 iter 50 sum correct 0.8541666666666666 loss 1.8908737897872925
epoch 30 iter 51 sum correct 0.8543419471153846 loss 1.8908623456954956
epoch 30 iter 52 sum correct 0.8543263561320755 loss 1.8908592462539673
epoch 30 iter 53 sum correct 0.8546368634259259 loss 1.890837550163269
epoch 30 iter 54 sum correct 0.854794034090909 loss 1.890831470489502
epoch 30 iter 55 sum correct 0.8550502232142857 loss 1.890812635421753
epoch 30 iter 56 sum correct 0.8411458333333334 loss 1.8907831907272339
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.59it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.21it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.34it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.57it/s]8it [00:01,  5.59it/s]                                 macro  0.5251077626617799
micro  0.6174421844524938
[[290   0  63  23  56  10  25]
 [ 25   0  16   1  10   0   4]
 [ 64   0 277  18  77  32  28]
 [ 36   0  43 728  24  17  47]
 [ 91   0 147  32 291   6  86]
 [ 15   0  45  13   7 330   5]
 [ 76   0  93  47  77  14 300]]
              precision    recall  f1-score   support

           0       0.49      0.62      0.55       467
           1       0.00      0.00      0.00        56
           2       0.40      0.56      0.47       496
           3       0.84      0.81      0.83       895
           4       0.54      0.45      0.49       653
           5       0.81      0.80      0.80       415
           6       0.61      0.49      0.54       607

    accuracy                           0.62      3589
   macro avg       0.53      0.53      0.53      3589
weighted avg       0.62      0.62      0.62      3589

correct 0.6174421844524938 
f1 0.5251077626617799 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.82it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.47it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.91it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.21it/s]8it [00:01,  6.14it/s]                                 macro  0.5336064227351831
micro  0.63137364168292
[[311   0  76  15  56   5  28]
 [ 32   0  12   3   5   2   1]
 [ 78   0 287  12  82  39  30]
 [ 32   0  32 756  26  15  18]
 [ 85   0 139  28 265   9  68]
 [ 11   0  44  22   8 320  11]
 [ 86   0  77  35  80  21 327]]
              precision    recall  f1-score   support

           0       0.49      0.63      0.55       491
           1       0.00      0.00      0.00        55
           2       0.43      0.54      0.48       528
           3       0.87      0.86      0.86       879
           4       0.51      0.45      0.47       594
           5       0.78      0.77      0.77       416
           6       0.68      0.52      0.59       626

    accuracy                           0.63      3589
   macro avg       0.54      0.54      0.53      3589
weighted avg       0.64      0.63      0.63      3589

correct 0.63137364168292 
f1 0.5336064227351831 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:39,  1.41it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.33it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.81it/s]  7%|▋         | 4/56.072265625 [00:01<00:16,  3.25it/s]  9%|▉         | 5/56.072265625 [00:01<00:15,  3.40it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.67it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.75it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.91it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.83it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  3.92it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.87it/s] 21%|██▏       | 12/56.072265625 [00:03<00:11,  3.94it/s] 23%|██▎       | 13/56.072265625 [00:03<00:11,  3.87it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  3.94it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  3.86it/s] 29%|██▊       | 16/56.072265625 [00:04<00:10,  3.94it/s] 30%|███       | 17/56.072265625 [00:04<00:10,  3.88it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.00it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  3.97it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.01it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  3.98it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.07it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.02it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.11it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  3.92it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  3.97it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  3.90it/s] 50%|████▉     | 28/56.072265625 [00:07<00:07,  3.96it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  3.90it/s] 54%|█████▎    | 30/56.072265625 [00:08<00:06,  3.96it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  3.88it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:06,  3.95it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  3.90it/s] 61%|██████    | 34/56.072265625 [00:09<00:05,  3.97it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  3.88it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:05,  3.95it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  3.89it/s] 68%|██████▊   | 38/56.072265625 [00:10<00:04,  4.00it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  3.97it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:04,  4.01it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  3.94it/s] 75%|███████▍  | 42/56.072265625 [00:11<00:03,  3.99it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  3.90it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:03,  3.96it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  3.87it/s] 82%|████████▏ | 46/56.072265625 [00:12<00:02,  3.94it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  3.89it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:02,  3.95it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  3.89it/s] 89%|████████▉ | 50/56.072265625 [00:13<00:01,  4.00it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  3.99it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:01,  3.90it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  3.93it/s] 96%|█████████▋| 54/56.072265625 [00:14<00:00,  4.05it/s] 98%|█████████▊| 55/56.072265625 [00:14<00:00,  4.04it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.12it/s]57it [00:14,  5.00it/s]                                  57it [00:14,  3.87it/s]epoch 31 iter 0 sum correct 0.890625 loss 1.8881349563598633
epoch 31 iter 1 sum correct 0.8818359375 loss 1.8887364864349365
epoch 31 iter 2 sum correct 0.8795572916666666 loss 1.8888872861862183
epoch 31 iter 3 sum correct 0.8740234375 loss 1.889326572418213
epoch 31 iter 4 sum correct 0.871875 loss 1.889499306678772
epoch 31 iter 5 sum correct 0.8649088541666666 loss 1.8900835514068604
epoch 31 iter 6 sum correct 0.8627232142857143 loss 1.890303611755371
epoch 31 iter 7 sum correct 0.859619140625 loss 1.8904920816421509
epoch 31 iter 8 sum correct 0.8565538194444444 loss 1.890725016593933
epoch 31 iter 9 sum correct 0.8546875 loss 1.890853762626648
epoch 31 iter 10 sum correct 0.8540482954545454 loss 1.8909051418304443
epoch 31 iter 11 sum correct 0.8518880208333334 loss 1.891071081161499
epoch 31 iter 12 sum correct 0.8494591346153846 loss 1.8912532329559326
epoch 31 iter 13 sum correct 0.8490513392857143 loss 1.891278624534607
epoch 31 iter 14 sum correct 0.8467447916666667 loss 1.8914597034454346
epoch 31 iter 15 sum correct 0.848388671875 loss 1.8913356065750122
epoch 31 iter 16 sum correct 0.849609375 loss 1.8912369012832642
epoch 31 iter 17 sum correct 0.8484157986111112 loss 1.8913273811340332
epoch 31 iter 18 sum correct 0.8506373355263158 loss 1.8911569118499756
epoch 31 iter 19 sum correct 0.849609375 loss 1.8912365436553955
epoch 31 iter 20 sum correct 0.8501674107142857 loss 1.8911914825439453
epoch 31 iter 21 sum correct 0.8505859375 loss 1.8911594152450562
epoch 31 iter 22 sum correct 0.8507982336956522 loss 1.8911317586898804
epoch 31 iter 23 sum correct 0.8504231770833334 loss 1.8911542892456055
epoch 31 iter 24 sum correct 0.85109375 loss 1.8910961151123047
epoch 31 iter 25 sum correct 0.8507361778846154 loss 1.8911221027374268
epoch 31 iter 26 sum correct 0.8510561342592593 loss 1.891091227531433
epoch 31 iter 27 sum correct 0.8501674107142857 loss 1.891156554222107
epoch 31 iter 28 sum correct 0.8498787715517241 loss 1.8911793231964111
epoch 31 iter 29 sum correct 0.8502604166666666 loss 1.891146183013916
epoch 31 iter 30 sum correct 0.8506804435483871 loss 1.8911137580871582
epoch 31 iter 31 sum correct 0.850341796875 loss 1.8911384344100952
epoch 31 iter 32 sum correct 0.8506747159090909 loss 1.8911181688308716
epoch 31 iter 33 sum correct 0.8507582720588235 loss 1.8911124467849731
epoch 31 iter 34 sum correct 0.850390625 loss 1.8911396265029907
epoch 31 iter 35 sum correct 0.8505859375 loss 1.891127109527588
epoch 31 iter 36 sum correct 0.8509290540540541 loss 1.8911011219024658
epoch 31 iter 37 sum correct 0.8508943256578947 loss 1.8910980224609375
epoch 31 iter 38 sum correct 0.8509615384615384 loss 1.891090750694275
epoch 31 iter 39 sum correct 0.851123046875 loss 1.891079306602478
epoch 31 iter 40 sum correct 0.8512290396341463 loss 1.8910659551620483
epoch 31 iter 41 sum correct 0.8515625 loss 1.891040325164795
epoch 31 iter 42 sum correct 0.8516987645348837 loss 1.891026496887207
epoch 31 iter 43 sum correct 0.8516512784090909 loss 1.8910223245620728
epoch 31 iter 44 sum correct 0.8524305555555556 loss 1.890960931777954
epoch 31 iter 45 sum correct 0.8523267663043478 loss 1.8909677267074585
epoch 31 iter 46 sum correct 0.8522273936170213 loss 1.8909698724746704
epoch 31 iter 47 sum correct 0.8520100911458334 loss 1.8909839391708374
epoch 31 iter 48 sum correct 0.8527184311224489 loss 1.890925645828247
epoch 31 iter 49 sum correct 0.8528515625 loss 1.890909194946289
epoch 31 iter 50 sum correct 0.8530943627450981 loss 1.890890121459961
epoch 31 iter 51 sum correct 0.8525015024038461 loss 1.8909363746643066
epoch 31 iter 52 sum correct 0.8522258254716981 loss 1.8909536600112915
epoch 31 iter 53 sum correct 0.8527560763888888 loss 1.8909105062484741
epoch 31 iter 54 sum correct 0.8533025568181818 loss 1.8908703327178955
epoch 31 iter 55 sum correct 0.8528878348214286 loss 1.8908991813659668
epoch 31 iter 56 sum correct 0.8390213815789473 loss 1.8908929824829102
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.51it/s] 43%|████▎     | 3/7.009765625 [00:00<00:01,  3.88it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.50it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.73it/s]8it [00:01,  5.63it/s]                                 macro  0.5341311021415962
micro  0.6372248537196991
[[278   0  19  28  93  18  31]
 [ 26   0   3   4  17   0   6]
 [ 76   0 176  19 147  40  38]
 [ 28   0   4 766  39  10  48]
 [ 70   0  40  35 426  11  71]
 [ 15   0  17  23  16 333  11]
 [ 47   0  14  52 174  12 308]]
              precision    recall  f1-score   support

           0       0.51      0.60      0.55       467
           1       0.00      0.00      0.00        56
           2       0.64      0.35      0.46       496
           3       0.83      0.86      0.84       895
           4       0.47      0.65      0.54       653
           5       0.79      0.80      0.79       415
           6       0.60      0.51      0.55       607

    accuracy                           0.64      3589
   macro avg       0.55      0.54      0.53      3589
weighted avg       0.64      0.64      0.63      3589

correct 0.6372248537196991 
f1 0.5341311021415962 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.72it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.39it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.41it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.70it/s]8it [00:01,  5.76it/s]                                 macro  0.5261601467667727
micro  0.63137364168292
[[290   0  30  24  94  11  42]
 [ 30   0   5   5  11   2   2]
 [ 93   0 164  24 150  57  40]
 [ 28   0  10 763  38  11  29]
 [ 60   0  35  36 384  10  69]
 [ 12   0  12  41  22 320   9]
 [ 50   0  15  49 150  17 345]]
              precision    recall  f1-score   support

           0       0.52      0.59      0.55       491
           1       0.00      0.00      0.00        55
           2       0.61      0.31      0.41       528
           3       0.81      0.87      0.84       879
           4       0.45      0.65      0.53       594
           5       0.75      0.77      0.76       416
           6       0.64      0.55      0.59       626

    accuracy                           0.63      3589
   macro avg       0.54      0.53      0.53      3589
weighted avg       0.63      0.63      0.62      3589

correct 0.63137364168292 
f1 0.5261601467667727 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:38,  1.42it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.33it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.84it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.28it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.43it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.70it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.72it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.90it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.86it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  3.94it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.86it/s] 21%|██▏       | 12/56.072265625 [00:03<00:11,  3.97it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  3.94it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.03it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  3.99it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.07it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.00it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.08it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.02it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.08it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.05it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.11it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.06it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.13it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.06it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.13it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.06it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.13it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.07it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.12it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.04it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.11it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.04it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.10it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.03it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.11it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.06it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.12it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  4.06it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.13it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.07it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.13it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  4.07it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.14it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.06it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.12it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  4.04it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.12it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.07it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.12it/s] 91%|█████████ | 51/56.072265625 [00:12<00:01,  4.08it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.15it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.09it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.15it/s] 98%|█████████▊| 55/56.072265625 [00:13<00:00,  4.09it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.16it/s]57it [00:14,  3.97it/s]                                  epoch 32 iter 0 sum correct 0.84375 loss 1.8913284540176392
epoch 32 iter 1 sum correct 0.84375 loss 1.891455888748169
epoch 32 iter 2 sum correct 0.8483072916666666 loss 1.8910305500030518
epoch 32 iter 3 sum correct 0.85546875 loss 1.8904969692230225
epoch 32 iter 4 sum correct 0.861328125 loss 1.8900648355484009
epoch 32 iter 5 sum correct 0.8587239583333334 loss 1.8902742862701416
epoch 32 iter 6 sum correct 0.8588169642857143 loss 1.890264630317688
epoch 32 iter 7 sum correct 0.858154296875 loss 1.890337586402893
epoch 32 iter 8 sum correct 0.8591579861111112 loss 1.8902745246887207
epoch 32 iter 9 sum correct 0.858984375 loss 1.8902902603149414
epoch 32 iter 10 sum correct 0.8588423295454546 loss 1.8902994394302368
epoch 32 iter 11 sum correct 0.8600260416666666 loss 1.8902133703231812
epoch 32 iter 12 sum correct 0.8595252403846154 loss 1.8902643918991089
epoch 32 iter 13 sum correct 0.8599330357142857 loss 1.890238881111145
epoch 32 iter 14 sum correct 0.8604166666666667 loss 1.8901959657669067
epoch 32 iter 15 sum correct 0.85986328125 loss 1.890251874923706
epoch 32 iter 16 sum correct 0.8586856617647058 loss 1.8903335332870483
epoch 32 iter 17 sum correct 0.8582899305555556 loss 1.8903642892837524
epoch 32 iter 18 sum correct 0.8587582236842105 loss 1.8903411626815796
epoch 32 iter 19 sum correct 0.85947265625 loss 1.8903003931045532
epoch 32 iter 20 sum correct 0.8583519345238095 loss 1.890381932258606
epoch 32 iter 21 sum correct 0.8587535511363636 loss 1.8903493881225586
epoch 32 iter 22 sum correct 0.858780570652174 loss 1.8903446197509766
epoch 32 iter 23 sum correct 0.85986328125 loss 1.8902640342712402
epoch 32 iter 24 sum correct 0.8596875 loss 1.8902828693389893
epoch 32 iter 25 sum correct 0.8606520432692307 loss 1.8902090787887573
epoch 32 iter 26 sum correct 0.8608940972222222 loss 1.8901915550231934
epoch 32 iter 27 sum correct 0.8609095982142857 loss 1.8901922702789307
epoch 32 iter 28 sum correct 0.861260775862069 loss 1.890166997909546
epoch 32 iter 29 sum correct 0.8617838541666667 loss 1.8901335000991821
epoch 32 iter 30 sum correct 0.8622101814516129 loss 1.8901081085205078
epoch 32 iter 31 sum correct 0.86090087890625 loss 1.8902078866958618
epoch 32 iter 32 sum correct 0.8605587121212122 loss 1.890236496925354
epoch 32 iter 33 sum correct 0.8605238970588235 loss 1.8902345895767212
epoch 32 iter 34 sum correct 0.8603236607142857 loss 1.8902455568313599
epoch 32 iter 35 sum correct 0.8595920138888888 loss 1.890299677848816
epoch 32 iter 36 sum correct 0.859058277027027 loss 1.8903398513793945
epoch 32 iter 37 sum correct 0.8588610197368421 loss 1.8903506994247437
epoch 32 iter 38 sum correct 0.8589242788461539 loss 1.8903462886810303
epoch 32 iter 39 sum correct 0.859033203125 loss 1.8903350830078125
epoch 32 iter 40 sum correct 0.8589939024390244 loss 1.8903295993804932
epoch 32 iter 41 sum correct 0.8593284970238095 loss 1.8903030157089233
epoch 32 iter 42 sum correct 0.8597837936046512 loss 1.8902649879455566
epoch 32 iter 43 sum correct 0.8596413352272727 loss 1.890268325805664
epoch 32 iter 44 sum correct 0.8598090277777778 loss 1.8902559280395508
epoch 32 iter 45 sum correct 0.8590777853260869 loss 1.890305757522583
epoch 32 iter 46 sum correct 0.858876329787234 loss 1.8903213739395142
epoch 32 iter 47 sum correct 0.8592936197916666 loss 1.8902891874313354
epoch 32 iter 48 sum correct 0.8596938775510204 loss 1.890260934829712
epoch 32 iter 49 sum correct 0.859140625 loss 1.890305757522583
epoch 32 iter 50 sum correct 0.859375 loss 1.8902884721755981
epoch 32 iter 51 sum correct 0.8591496394230769 loss 1.890304446220398
epoch 32 iter 52 sum correct 0.8596698113207547 loss 1.8902651071548462
epoch 32 iter 53 sum correct 0.8595920138888888 loss 1.8902677297592163
epoch 32 iter 54 sum correct 0.8593394886363637 loss 1.8902885913848877
epoch 32 iter 55 sum correct 0.8596888950892857 loss 1.890265703201294
epoch 32 iter 56 sum correct 0.8457373903508771 loss 1.890241026878357
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.64it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.26it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.52it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.80it/s]8it [00:01,  5.78it/s]                                 macro  0.5282898424873096
micro  0.6308163833937029
[[226   0  61  48  60  17  55]
 [ 16   0  10   8  12   0  10]
 [ 48   0 222  23 101  35  67]
 [ 15   0  10 781  13  17  59]
 [ 75   0  61  53 322   5 137]
 [ 13   0  26  21   7 321  27]
 [ 28   0  39  59  85   4 392]]
              precision    recall  f1-score   support

           0       0.54      0.48      0.51       467
           1       0.00      0.00      0.00        56
           2       0.52      0.45      0.48       496
           3       0.79      0.87      0.83       895
           4       0.54      0.49      0.51       653
           5       0.80      0.77      0.79       415
           6       0.52      0.65      0.58       607

    accuracy                           0.63      3589
   macro avg       0.53      0.53      0.53      3589
weighted avg       0.62      0.63      0.62      3589

correct 0.6308163833937029 
f1 0.5282898424873096 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.71it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.40it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.65it/s] 86%|████████▌ | 6/7.009765625 [00:01<00:00,  6.39it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.75it/s]8it [00:01,  5.78it/s]                                 macro  0.5382156635290148
micro  0.6447478406241293
[[250   0  70  30  56  15  70]
 [ 24   0   9   6   9   2   5]
 [ 46   0 246  32  90  51  63]
 [ 10   0  21 791  19  12  26]
 [ 53   0  66  49 289   8 129]
 [  6   0  36  28  12 311  23]
 [ 35   0  34  51  70   9 427]]
              precision    recall  f1-score   support

           0       0.59      0.51      0.55       491
           1       0.00      0.00      0.00        55
           2       0.51      0.47      0.49       528
           3       0.80      0.90      0.85       879
           4       0.53      0.49      0.51       594
           5       0.76      0.75      0.75       416
           6       0.57      0.68      0.62       626

    accuracy                           0.64      3589
   macro avg       0.54      0.54      0.54      3589
weighted avg       0.63      0.64      0.63      3589

correct 0.6447478406241293 
f1 0.5382156635290148 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:37,  1.45it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.35it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.86it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.28it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.46it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.70it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.75it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.91it/s] 16%|█▌        | 9/56.072265625 [00:02<00:11,  3.92it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.02it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.98it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.07it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  4.04it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.10it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.05it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.13it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.07it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.13it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.04it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.10it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.03it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.10it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.05it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.13it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.06it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.13it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.08it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.14it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.06it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.12it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.04it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.12it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.06it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.14it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.09it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.16it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.07it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.14it/s] 70%|██████▉   | 39/56.072265625 [00:09<00:04,  4.09it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.15it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.09it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.14it/s] 77%|███████▋  | 43/56.072265625 [00:10<00:03,  4.07it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.15it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.07it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.13it/s] 84%|████████▍ | 47/56.072265625 [00:11<00:02,  4.07it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.15it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.08it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.14it/s] 91%|█████████ | 51/56.072265625 [00:12<00:01,  4.08it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.16it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.11it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.16it/s] 98%|█████████▊| 55/56.072265625 [00:13<00:00,  4.09it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.16it/s]57it [00:14,  3.99it/s]                                  epoch 33 iter 0 sum correct 0.859375 loss 1.8901422023773193
epoch 33 iter 1 sum correct 0.8603515625 loss 1.890040636062622
epoch 33 iter 2 sum correct 0.8567708333333334 loss 1.8903591632843018
epoch 33 iter 3 sum correct 0.8486328125 loss 1.890993356704712
epoch 33 iter 4 sum correct 0.85078125 loss 1.8908761739730835
epoch 33 iter 5 sum correct 0.849609375 loss 1.8909730911254883
epoch 33 iter 6 sum correct 0.8501674107142857 loss 1.8909900188446045
epoch 33 iter 7 sum correct 0.84716796875 loss 1.891211748123169
epoch 33 iter 8 sum correct 0.8435329861111112 loss 1.8914655447006226
epoch 33 iter 9 sum correct 0.8447265625 loss 1.891396403312683
epoch 33 iter 10 sum correct 0.8446377840909091 loss 1.891389012336731
epoch 33 iter 11 sum correct 0.8439127604166666 loss 1.8914498090744019
epoch 33 iter 12 sum correct 0.8461538461538461 loss 1.8913010358810425
epoch 33 iter 13 sum correct 0.8464006696428571 loss 1.891274094581604
epoch 33 iter 14 sum correct 0.8481770833333333 loss 1.8911254405975342
epoch 33 iter 15 sum correct 0.8492431640625 loss 1.8910430669784546
epoch 33 iter 16 sum correct 0.8499540441176471 loss 1.8909963369369507
epoch 33 iter 17 sum correct 0.8498263888888888 loss 1.8910115957260132
epoch 33 iter 18 sum correct 0.8501233552631579 loss 1.8909832239151
epoch 33 iter 19 sum correct 0.84892578125 loss 1.8910754919052124
epoch 33 iter 20 sum correct 0.84765625 loss 1.891175389289856
epoch 33 iter 21 sum correct 0.8469460227272727 loss 1.8912336826324463
epoch 33 iter 22 sum correct 0.8479110054347826 loss 1.8911592960357666
epoch 33 iter 23 sum correct 0.847900390625 loss 1.8911570310592651
epoch 33 iter 24 sum correct 0.8475 loss 1.8911848068237305
epoch 33 iter 25 sum correct 0.8475811298076923 loss 1.8911871910095215
epoch 33 iter 26 sum correct 0.8470052083333334 loss 1.8912432193756104
epoch 33 iter 27 sum correct 0.8475864955357143 loss 1.8911986351013184
epoch 33 iter 28 sum correct 0.847588900862069 loss 1.8912012577056885
epoch 33 iter 29 sum correct 0.848046875 loss 1.8911727666854858
epoch 33 iter 30 sum correct 0.8478452620967742 loss 1.8911892175674438
epoch 33 iter 31 sum correct 0.847900390625 loss 1.8911739587783813
epoch 33 iter 32 sum correct 0.8480705492424242 loss 1.8911597728729248
epoch 33 iter 33 sum correct 0.8485179227941176 loss 1.8911283016204834
epoch 33 iter 34 sum correct 0.8491629464285714 loss 1.8910653591156006
epoch 33 iter 35 sum correct 0.8490668402777778 loss 1.8910672664642334
epoch 33 iter 36 sum correct 0.8490287162162162 loss 1.8910728693008423
epoch 33 iter 37 sum correct 0.8492495888157895 loss 1.8910508155822754
epoch 33 iter 38 sum correct 0.8500100160256411 loss 1.8909938335418701
epoch 33 iter 39 sum correct 0.850732421875 loss 1.8909367322921753
epoch 33 iter 40 sum correct 0.8497999237804879 loss 1.8910002708435059
epoch 33 iter 41 sum correct 0.8499813988095238 loss 1.8909915685653687
epoch 33 iter 42 sum correct 0.8503815406976745 loss 1.890958547592163
epoch 33 iter 43 sum correct 0.8507634943181818 loss 1.8909215927124023
epoch 33 iter 44 sum correct 0.8513888888888889 loss 1.8908723592758179
epoch 33 iter 45 sum correct 0.8511379076086957 loss 1.8908922672271729
epoch 33 iter 46 sum correct 0.8513962765957447 loss 1.8908652067184448
epoch 33 iter 47 sum correct 0.8507486979166666 loss 1.8909152746200562
epoch 33 iter 48 sum correct 0.8509247448979592 loss 1.8909013271331787
epoch 33 iter 49 sum correct 0.8512109375 loss 1.8908790349960327
epoch 33 iter 50 sum correct 0.8513710171568627 loss 1.8908649682998657
epoch 33 iter 51 sum correct 0.8516751802884616 loss 1.8908413648605347
epoch 33 iter 52 sum correct 0.8521521226415094 loss 1.89080011844635
epoch 33 iter 53 sum correct 0.8524305555555556 loss 1.89077627658844
epoch 33 iter 54 sum correct 0.8528764204545455 loss 1.8907381296157837
epoch 33 iter 55 sum correct 0.8526088169642857 loss 1.8907582759857178
epoch 33 iter 56 sum correct 0.8388157894736842 loss 1.8906711339950562
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.64it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.22it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.44it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.67it/s]8it [00:01,  5.73it/s]                                 macro  0.5309461256087988
micro  0.6280300919476177
[[252   0  58  31  62  12  52]
 [ 20   0  16   5  12   0   3]
 [ 50   0 253  16  90  26  61]
 [ 21   0  27 753  27  15  52]
 [ 59   0 104  44 317   4 125]
 [ 12   0  48  20  11 293  31]
 [ 32   0  49  58  80   2 386]]
              precision    recall  f1-score   support

           0       0.57      0.54      0.55       467
           1       0.00      0.00      0.00        56
           2       0.46      0.51      0.48       496
           3       0.81      0.84      0.83       895
           4       0.53      0.49      0.51       653
           5       0.83      0.71      0.76       415
           6       0.54      0.64      0.59       607

    accuracy                           0.63      3589
   macro avg       0.53      0.53      0.53      3589
weighted avg       0.62      0.63      0.62      3589

correct 0.6280300919476177 
f1 0.5309461256087988 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.90it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.67it/s] 71%|███████▏  | 5/7.009765625 [00:00<00:00,  6.08it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.27it/s]8it [00:01,  6.26it/s]                                 macro  0.5333434735531585
micro  0.6349958205628309
[[272   0  71  21  69   3  55]
 [ 23   0  15   5   9   0   3]
 [ 54   0 281  23  76  26  68]
 [ 16   0  28 780  23  13  19]
 [ 45   0 111  34 267   4 133]
 [  9   0  63  26  11 270  37]
 [ 41   0  41  48  78   9 409]]
              precision    recall  f1-score   support

           0       0.59      0.55      0.57       491
           1       0.00      0.00      0.00        55
           2       0.46      0.53      0.49       528
           3       0.83      0.89      0.86       879
           4       0.50      0.45      0.47       594
           5       0.83      0.65      0.73       416
           6       0.56      0.65      0.61       626

    accuracy                           0.63      3589
   macro avg       0.54      0.53      0.53      3589
weighted avg       0.63      0.63      0.63      3589

correct 0.6349958205628309 
f1 0.5333434735531585 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:39,  1.41it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.34it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.85it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.26it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.41it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.68it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.74it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.91it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.89it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.02it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.96it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.07it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  4.01it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.09it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.01it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.09it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.01it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.10it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.03it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.10it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.02it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  3.87it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  3.86it/s] 43%|████▎     | 24/56.072265625 [00:06<00:08,  3.98it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  3.93it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.04it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  3.98it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.06it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  3.98it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.08it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.03it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.10it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.02it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.11it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.05it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.11it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.03it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.12it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  4.05it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.11it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.03it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.11it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  4.04it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.11it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.01it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.09it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  4.02it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.10it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.01it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.11it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  4.06it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.12it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.03it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.12it/s] 98%|█████████▊| 55/56.072265625 [00:14<00:00,  4.07it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.13it/s]57it [00:14,  3.94it/s]                                  epoch 34 iter 0 sum correct 0.861328125 loss 1.8898650407791138
epoch 34 iter 1 sum correct 0.8701171875 loss 1.8893537521362305
epoch 34 iter 2 sum correct 0.8782552083333334 loss 1.8886984586715698
epoch 34 iter 3 sum correct 0.875 loss 1.8889176845550537
epoch 34 iter 4 sum correct 0.87578125 loss 1.888863444328308
epoch 34 iter 5 sum correct 0.8723958333333334 loss 1.889189600944519
epoch 34 iter 6 sum correct 0.87109375 loss 1.8893054723739624
epoch 34 iter 7 sum correct 0.86962890625 loss 1.8894100189208984
epoch 34 iter 8 sum correct 0.8693576388888888 loss 1.8894153833389282
epoch 34 iter 9 sum correct 0.8666015625 loss 1.8896636962890625
epoch 34 iter 10 sum correct 0.8682528409090909 loss 1.8895487785339355
epoch 34 iter 11 sum correct 0.8671875 loss 1.8896244764328003
epoch 34 iter 12 sum correct 0.865234375 loss 1.8897780179977417
epoch 34 iter 13 sum correct 0.8650948660714286 loss 1.8898001909255981
epoch 34 iter 14 sum correct 0.866796875 loss 1.8896660804748535
epoch 34 iter 15 sum correct 0.866943359375 loss 1.8896619081497192
epoch 34 iter 16 sum correct 0.8668428308823529 loss 1.8896690607070923
epoch 34 iter 17 sum correct 0.8689236111111112 loss 1.8895028829574585
epoch 34 iter 18 sum correct 0.8686266447368421 loss 1.8895143270492554
epoch 34 iter 19 sum correct 0.87021484375 loss 1.889388918876648
epoch 34 iter 20 sum correct 0.8695126488095238 loss 1.889446496963501
epoch 34 iter 21 sum correct 0.8694069602272727 loss 1.8894582986831665
epoch 34 iter 22 sum correct 0.8695652173913043 loss 1.8894424438476562
epoch 34 iter 23 sum correct 0.868896484375 loss 1.8894922733306885
epoch 34 iter 24 sum correct 0.869921875 loss 1.889412522315979
epoch 34 iter 25 sum correct 0.8693659855769231 loss 1.8894423246383667
epoch 34 iter 26 sum correct 0.8693576388888888 loss 1.889450192451477
epoch 34 iter 27 sum correct 0.86962890625 loss 1.8894249200820923
epoch 34 iter 28 sum correct 0.8683324353448276 loss 1.8895217180252075
epoch 34 iter 29 sum correct 0.8684895833333334 loss 1.889510154724121
epoch 34 iter 30 sum correct 0.8690146169354839 loss 1.8894710540771484
epoch 34 iter 31 sum correct 0.86883544921875 loss 1.8894848823547363
epoch 34 iter 32 sum correct 0.8689038825757576 loss 1.8894814252853394
epoch 34 iter 33 sum correct 0.8694278492647058 loss 1.8894426822662354
epoch 34 iter 34 sum correct 0.8694196428571429 loss 1.8894426822662354
epoch 34 iter 35 sum correct 0.8689778645833334 loss 1.889477014541626
epoch 34 iter 36 sum correct 0.869140625 loss 1.8894621133804321
epoch 34 iter 37 sum correct 0.8685752467105263 loss 1.8895022869110107
epoch 34 iter 38 sum correct 0.8681390224358975 loss 1.8895314931869507
epoch 34 iter 39 sum correct 0.868017578125 loss 1.8895409107208252
epoch 34 iter 40 sum correct 0.8680449695121951 loss 1.889539361000061
epoch 34 iter 41 sum correct 0.8683965773809523 loss 1.8895055055618286
epoch 34 iter 42 sum correct 0.868186773255814 loss 1.889519214630127
epoch 34 iter 43 sum correct 0.8678977272727273 loss 1.8895394802093506
epoch 34 iter 44 sum correct 0.8671875 loss 1.8895905017852783
epoch 34 iter 45 sum correct 0.8671450407608695 loss 1.8895931243896484
epoch 34 iter 46 sum correct 0.8673537234042553 loss 1.8895785808563232
epoch 34 iter 47 sum correct 0.8676350911458334 loss 1.8895583152770996
epoch 34 iter 48 sum correct 0.8675063775510204 loss 1.889563798904419
epoch 34 iter 49 sum correct 0.86796875 loss 1.889530897140503
epoch 34 iter 50 sum correct 0.8674172794117647 loss 1.889575481414795
epoch 34 iter 51 sum correct 0.8671875 loss 1.8895955085754395
epoch 34 iter 52 sum correct 0.8668558372641509 loss 1.889617919921875
epoch 34 iter 53 sum correct 0.8667173032407407 loss 1.8896291255950928
epoch 34 iter 54 sum correct 0.8663707386363636 loss 1.8896530866622925
epoch 34 iter 55 sum correct 0.8669782366071429 loss 1.8896023035049438
epoch 34 iter 56 sum correct 0.8528645833333334 loss 1.889595866203308
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.61it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.22it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.70it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.01it/s]8it [00:01,  5.89it/s]                                 macro  0.5389726883422039
micro  0.6388966285873502
[[252   0  36  27  63  12  77]
 [ 23   0   7   7  15   0   4]
 [ 51   0 216  21  98  38  72]
 [ 29   0   8 748  27  16  67]
 [ 68   0  62  37 340   5 141]
 [ 16   0  23  22  13 319  22]
 [ 22   0  31  48  86   2 418]]
              precision    recall  f1-score   support

           0       0.55      0.54      0.54       467
           1       0.00      0.00      0.00        56
           2       0.56      0.44      0.49       496
           3       0.82      0.84      0.83       895
           4       0.53      0.52      0.53       653
           5       0.81      0.77      0.79       415
           6       0.52      0.69      0.59       607

    accuracy                           0.64      3589
   macro avg       0.54      0.54      0.54      3589
weighted avg       0.63      0.64      0.63      3589

correct 0.6388966285873502 
f1 0.5389726883422039 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.76it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.44it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.92it/s] 86%|████████▌ | 6/7.009765625 [00:01<00:00,  6.63it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.16it/s]8it [00:01,  5.89it/s]                                 macro  0.5422683164790638
micro  0.6475341320702146
[[272   0  51  27  55   6  80]
 [ 26   0   6   6  11   2   4]
 [ 63   0 226  22  98  46  73]
 [ 23   0  13 771  22  11  39]
 [ 57   0  50  25 311   9 142]
 [  8   0  32  28  18 303  27]
 [ 33   0  26  35  78  13 441]]
              precision    recall  f1-score   support

           0       0.56      0.55      0.56       491
           1       0.00      0.00      0.00        55
           2       0.56      0.43      0.48       528
           3       0.84      0.88      0.86       879
           4       0.52      0.52      0.52       594
           5       0.78      0.73      0.75       416
           6       0.55      0.70      0.62       626

    accuracy                           0.65      3589
   macro avg       0.55      0.55      0.54      3589
weighted avg       0.64      0.65      0.64      3589

correct 0.6475341320702146 
f1 0.5422683164790638 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:38,  1.41it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.31it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.81it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.27it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.46it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.72it/s] 12%|█▏        | 7/56.072265625 [00:02<00:12,  3.78it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.94it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.91it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.03it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.97it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.07it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  4.04it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.11it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.06it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.14it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.08it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.15it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.08it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.14it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  3.95it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  3.98it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  3.93it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.03it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  3.98it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.07it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.00it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.09it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.02it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.11it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  3.99it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.07it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.00it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.09it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.02it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.10it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.02it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.10it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  4.04it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.11it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.03it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.11it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  4.03it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.11it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.03it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.10it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  4.05it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.11it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  3.95it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.05it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  4.02it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.10it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.06it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.13it/s] 98%|█████████▊| 55/56.072265625 [00:13<00:00,  4.08it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.14it/s]57it [00:14,  4.94it/s]                                  57it [00:14,  3.95it/s]epoch 35 iter 0 sum correct 0.884765625 loss 1.888224482536316
epoch 35 iter 1 sum correct 0.8818359375 loss 1.8882923126220703
epoch 35 iter 2 sum correct 0.8834635416666666 loss 1.8882238864898682
epoch 35 iter 3 sum correct 0.876953125 loss 1.888716697692871
epoch 35 iter 4 sum correct 0.869140625 loss 1.8892580270767212
epoch 35 iter 5 sum correct 0.8688151041666666 loss 1.8893150091171265
epoch 35 iter 6 sum correct 0.8657924107142857 loss 1.8895509243011475
epoch 35 iter 7 sum correct 0.8642578125 loss 1.8896549940109253
epoch 35 iter 8 sum correct 0.8634982638888888 loss 1.8897312879562378
epoch 35 iter 9 sum correct 0.863671875 loss 1.889750361442566
epoch 35 iter 10 sum correct 0.8634588068181818 loss 1.8897851705551147
epoch 35 iter 11 sum correct 0.8636067708333334 loss 1.8897852897644043
epoch 35 iter 12 sum correct 0.8641826923076923 loss 1.889743685722351
epoch 35 iter 13 sum correct 0.8635602678571429 loss 1.889791488647461
epoch 35 iter 14 sum correct 0.8619791666666666 loss 1.8899089097976685
epoch 35 iter 15 sum correct 0.861572265625 loss 1.8899470567703247
epoch 35 iter 16 sum correct 0.8624770220588235 loss 1.8898719549179077
epoch 35 iter 17 sum correct 0.8625217013888888 loss 1.8898617029190063
epoch 35 iter 18 sum correct 0.8621504934210527 loss 1.8898931741714478
epoch 35 iter 19 sum correct 0.86259765625 loss 1.8898543119430542
epoch 35 iter 20 sum correct 0.8636532738095238 loss 1.8897703886032104
epoch 35 iter 21 sum correct 0.8630149147727273 loss 1.8898165225982666
epoch 35 iter 22 sum correct 0.8627717391304348 loss 1.8898347616195679
epoch 35 iter 23 sum correct 0.8639322916666666 loss 1.8897576332092285
epoch 35 iter 24 sum correct 0.862734375 loss 1.8898519277572632
epoch 35 iter 25 sum correct 0.8628305288461539 loss 1.8898459672927856
epoch 35 iter 26 sum correct 0.8629918981481481 loss 1.8898407220840454
epoch 35 iter 27 sum correct 0.8629324776785714 loss 1.88983952999115
epoch 35 iter 28 sum correct 0.8622710129310345 loss 1.8898810148239136
epoch 35 iter 29 sum correct 0.8625 loss 1.889876127243042
epoch 35 iter 30 sum correct 0.8623991935483871 loss 1.8898853063583374
epoch 35 iter 31 sum correct 0.86224365234375 loss 1.8898981809616089
epoch 35 iter 32 sum correct 0.8626893939393939 loss 1.8898653984069824
epoch 35 iter 33 sum correct 0.8625344669117647 loss 1.8898780345916748
epoch 35 iter 34 sum correct 0.8629464285714286 loss 1.8898448944091797
epoch 35 iter 35 sum correct 0.8624131944444444 loss 1.889890432357788
epoch 35 iter 36 sum correct 0.8630173141891891 loss 1.8898476362228394
epoch 35 iter 37 sum correct 0.8632298519736842 loss 1.8898319005966187
epoch 35 iter 38 sum correct 0.8635817307692307 loss 1.889809250831604
epoch 35 iter 39 sum correct 0.8638671875 loss 1.8897861242294312
epoch 35 iter 40 sum correct 0.8641387195121951 loss 1.889763355255127
epoch 35 iter 41 sum correct 0.8643973214285714 loss 1.889739990234375
epoch 35 iter 42 sum correct 0.864734738372093 loss 1.8897168636322021
epoch 35 iter 43 sum correct 0.8655894886363636 loss 1.8896503448486328
epoch 35 iter 44 sum correct 0.8653645833333333 loss 1.8896664381027222
epoch 35 iter 45 sum correct 0.8654891304347826 loss 1.8896541595458984
epoch 35 iter 46 sum correct 0.8651928191489362 loss 1.8896688222885132
epoch 35 iter 47 sum correct 0.8660481770833334 loss 1.889602541923523
epoch 35 iter 48 sum correct 0.8662308673469388 loss 1.889582872390747
epoch 35 iter 49 sum correct 0.8660546875 loss 1.8895963430404663
epoch 35 iter 50 sum correct 0.866000306372549 loss 1.8896028995513916
epoch 35 iter 51 sum correct 0.8653470552884616 loss 1.889648675918579
epoch 35 iter 52 sum correct 0.8654923349056604 loss 1.8896377086639404
epoch 35 iter 53 sum correct 0.8652705439814815 loss 1.8896551132202148
epoch 35 iter 54 sum correct 0.8656605113636363 loss 1.8896251916885376
epoch 35 iter 55 sum correct 0.8659319196428571 loss 1.8896068334579468
epoch 35 iter 56 sum correct 0.8518366228070176 loss 1.8896044492721558
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.56it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.14it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.61it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.82it/s]8it [00:01,  5.74it/s]                                 macro  0.5170623098161066
micro  0.620228475898579
[[217   0  63  27  80  28  52]
 [ 19   0   9   6  16   2   4]
 [ 43   0 214  16 110  67  46]
 [ 15   0  20 739  35  36  50]
 [ 53   0  74  37 369  25  95]
 [ 13   0  26  12  12 345   7]
 [ 29   0  41  43 125  27 342]]
              precision    recall  f1-score   support

           0       0.56      0.46      0.51       467
           1       0.00      0.00      0.00        56
           2       0.48      0.43      0.45       496
           3       0.84      0.83      0.83       895
           4       0.49      0.57      0.53       653
           5       0.65      0.83      0.73       415
           6       0.57      0.56      0.57       607

    accuracy                           0.62      3589
   macro avg       0.51      0.53      0.52      3589
weighted avg       0.61      0.62      0.61      3589

correct 0.620228475898579 
f1 0.5170623098161066 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.77it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.47it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.78it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.07it/s]8it [00:01,  5.95it/s]                                 macro  0.5278947425590742
micro  0.6322095291167457
[[240   0  78  21  79  18  55]
 [ 18   0  10   6  17   2   2]
 [ 48   0 237  18 107  79  39]
 [ 13   0  26 753  31  27  29]
 [ 48   0  73  26 326  15 106]
 [  4   0  26  18  20 337  11]
 [ 43   0  37  40  92  38 376]]
              precision    recall  f1-score   support

           0       0.58      0.49      0.53       491
           1       0.00      0.00      0.00        55
           2       0.49      0.45      0.47       528
           3       0.85      0.86      0.86       879
           4       0.49      0.55      0.52       594
           5       0.65      0.81      0.72       416
           6       0.61      0.60      0.60       626

    accuracy                           0.63      3589
   macro avg       0.52      0.54      0.53      3589
weighted avg       0.62      0.63      0.63      3589

correct 0.6322095291167457 
f1 0.5278947425590742 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:37,  1.48it/s]  4%|▎         | 2/56.072265625 [00:00<00:22,  2.37it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.86it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.29it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.49it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.71it/s] 12%|█▏        | 7/56.072265625 [00:02<00:12,  3.79it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.95it/s] 16%|█▌        | 9/56.072265625 [00:02<00:11,  3.95it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.06it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  4.02it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.11it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  4.06it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.13it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.06it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.14it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.09it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.16it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.09it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.15it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.07it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.14it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.06it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.12it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.06it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.14it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.08it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.14it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.07it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.15it/s] 55%|█████▌    | 31/56.072265625 [00:07<00:06,  4.06it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.12it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.05it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.12it/s] 62%|██████▏   | 35/56.072265625 [00:08<00:05,  4.07it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.13it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.06it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.14it/s] 70%|██████▉   | 39/56.072265625 [00:09<00:04,  4.05it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.12it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.03it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.11it/s] 77%|███████▋  | 43/56.072265625 [00:10<00:03,  4.06it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.12it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.06it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.14it/s] 84%|████████▍ | 47/56.072265625 [00:11<00:02,  4.08it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:02,  3.92it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  3.91it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.03it/s] 91%|█████████ | 51/56.072265625 [00:12<00:01,  4.01it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.09it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.04it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.13it/s] 98%|█████████▊| 55/56.072265625 [00:13<00:00,  4.06it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.13it/s]57it [00:14,  4.98it/s]                                  57it [00:14,  3.98it/s]epoch 36 iter 0 sum correct 0.8515625 loss 1.8905792236328125
epoch 36 iter 1 sum correct 0.865234375 loss 1.8896501064300537
epoch 36 iter 2 sum correct 0.8619791666666666 loss 1.8899459838867188
epoch 36 iter 3 sum correct 0.85595703125 loss 1.8904485702514648
epoch 36 iter 4 sum correct 0.849609375 loss 1.8909310102462769
epoch 36 iter 5 sum correct 0.8479817708333334 loss 1.8910486698150635
epoch 36 iter 6 sum correct 0.8445870535714286 loss 1.8912997245788574
epoch 36 iter 7 sum correct 0.84375 loss 1.8913497924804688
epoch 36 iter 8 sum correct 0.8426649305555556 loss 1.8914231061935425
epoch 36 iter 9 sum correct 0.83984375 loss 1.8916199207305908
epoch 36 iter 10 sum correct 0.8405539772727273 loss 1.8915629386901855
epoch 36 iter 11 sum correct 0.8372395833333334 loss 1.8918497562408447
epoch 36 iter 12 sum correct 0.8345853365384616 loss 1.8920674324035645
epoch 36 iter 13 sum correct 0.8318917410714286 loss 1.8922616243362427
epoch 36 iter 14 sum correct 0.832421875 loss 1.8922162055969238
epoch 36 iter 15 sum correct 0.8333740234375 loss 1.8921386003494263
epoch 36 iter 16 sum correct 0.8342141544117647 loss 1.892081379890442
epoch 36 iter 17 sum correct 0.8337673611111112 loss 1.8920996189117432
epoch 36 iter 18 sum correct 0.8334703947368421 loss 1.8921282291412354
epoch 36 iter 19 sum correct 0.832421875 loss 1.892199158668518
epoch 36 iter 20 sum correct 0.8319382440476191 loss 1.8922358751296997
epoch 36 iter 21 sum correct 0.8338068181818182 loss 1.8921031951904297
epoch 36 iter 22 sum correct 0.8340692934782609 loss 1.8920875787734985
epoch 36 iter 23 sum correct 0.8350423177083334 loss 1.892015814781189
epoch 36 iter 24 sum correct 0.835625 loss 1.8919596672058105
epoch 36 iter 25 sum correct 0.8356370192307693 loss 1.891965627670288
epoch 36 iter 26 sum correct 0.8358651620370371 loss 1.8919392824172974
epoch 36 iter 27 sum correct 0.8353794642857143 loss 1.8919804096221924
epoch 36 iter 28 sum correct 0.8356007543103449 loss 1.8919650316238403
epoch 36 iter 29 sum correct 0.8359375 loss 1.8919247388839722
epoch 36 iter 30 sum correct 0.8358744959677419 loss 1.8919179439544678
epoch 36 iter 31 sum correct 0.83660888671875 loss 1.8918522596359253
epoch 36 iter 32 sum correct 0.8367069128787878 loss 1.8918482065200806
epoch 36 iter 33 sum correct 0.8374310661764706 loss 1.891795039176941
epoch 36 iter 34 sum correct 0.8376116071428571 loss 1.891784906387329
epoch 36 iter 35 sum correct 0.8379991319444444 loss 1.8917533159255981
epoch 36 iter 36 sum correct 0.8374155405405406 loss 1.8917970657348633
epoch 36 iter 37 sum correct 0.8374794407894737 loss 1.8917945623397827
epoch 36 iter 38 sum correct 0.8383413461538461 loss 1.8917326927185059
epoch 36 iter 39 sum correct 0.83818359375 loss 1.8917402029037476
epoch 36 iter 40 sum correct 0.8378429878048781 loss 1.8917688131332397
epoch 36 iter 41 sum correct 0.8380301339285714 loss 1.8917467594146729
epoch 36 iter 42 sum correct 0.8384811046511628 loss 1.8917156457901
epoch 36 iter 43 sum correct 0.8385120738636364 loss 1.8917200565338135
epoch 36 iter 44 sum correct 0.8390190972222222 loss 1.8916817903518677
epoch 36 iter 45 sum correct 0.839461616847826 loss 1.89164400100708
epoch 36 iter 46 sum correct 0.840093085106383 loss 1.8915950059890747
epoch 36 iter 47 sum correct 0.8399658203125 loss 1.8915989398956299
epoch 36 iter 48 sum correct 0.8403220663265306 loss 1.8915705680847168
epoch 36 iter 49 sum correct 0.8401953125 loss 1.891584873199463
epoch 36 iter 50 sum correct 0.8408011642156863 loss 1.891538143157959
epoch 36 iter 51 sum correct 0.8405949519230769 loss 1.8915584087371826
epoch 36 iter 52 sum correct 0.8410966981132075 loss 1.8915208578109741
epoch 36 iter 53 sum correct 0.8412905092592593 loss 1.8915060758590698
epoch 36 iter 54 sum correct 0.8419034090909091 loss 1.8914613723754883
epoch 36 iter 55 sum correct 0.8423897879464286 loss 1.891425609588623
epoch 36 iter 56 sum correct 0.8285704495614035 loss 1.891525387763977
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.59it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.13it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.64it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.84it/s]8it [00:01,  5.69it/s]                                 macro  0.5016798600498199
micro  0.6046252438005015
[[311   0  25  23  35  26  47]
 [ 32   0   6   7   8   0   3]
 [ 91   0 161  15  92  88  49]
 [ 53   0   7 691  11  69  64]
 [127   0  50  36 285  39 116]
 [ 20   0  15   8   8 360   4]
 [ 80   0  19  38  72  36 362]]
              precision    recall  f1-score   support

           0       0.44      0.67      0.53       467
           1       0.00      0.00      0.00        56
           2       0.57      0.32      0.41       496
           3       0.84      0.77      0.81       895
           4       0.56      0.44      0.49       653
           5       0.58      0.87      0.70       415
           6       0.56      0.60      0.58       607

    accuracy                           0.60      3589
   macro avg       0.51      0.52      0.50      3589
weighted avg       0.61      0.60      0.59      3589

correct 0.6046252438005015 
f1 0.5016798600498199 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.82it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.55it/s] 71%|███████▏  | 5/7.009765625 [00:00<00:00,  5.94it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.21it/s]8it [00:01,  6.20it/s]                                 macro  0.5079480432147979
micro  0.6177208135971023
[[334   0  23  18  45  23  48]
 [ 30   0   3   8   9   2   3]
 [129   0 148  18  68 108  57]
 [ 46   0  11 720  19  45  38]
 [120   0  37  36 260  22 119]
 [ 19   0  15  10  10 348  14]
 [ 75   0  12  25  62  45 407]]
              precision    recall  f1-score   support

           0       0.44      0.68      0.54       491
           1       0.00      0.00      0.00        55
           2       0.59      0.28      0.38       528
           3       0.86      0.82      0.84       879
           4       0.55      0.44      0.49       594
           5       0.59      0.84      0.69       416
           6       0.59      0.65      0.62       626

    accuracy                           0.62      3589
   macro avg       0.52      0.53      0.51      3589
weighted avg       0.62      0.62      0.60      3589

correct 0.6177208135971023 
f1 0.5079480432147979 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:38,  1.45it/s]  4%|▎         | 2/56.072265625 [00:00<00:22,  2.37it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.86it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.30it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.47it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.72it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.77it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.91it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.86it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.00it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.95it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.05it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  3.99it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.08it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.04it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.08it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.01it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.10it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.04it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.12it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.05it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.11it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.07it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.14it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.06it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.12it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.07it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.14it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.06it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.14it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.06it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.13it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.05it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.12it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.07it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.14it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.08it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.15it/s] 70%|██████▉   | 39/56.072265625 [00:09<00:04,  4.09it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.16it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.07it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.14it/s] 77%|███████▋  | 43/56.072265625 [00:10<00:03,  4.07it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.14it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.06it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.10it/s] 84%|████████▍ | 47/56.072265625 [00:11<00:02,  4.03it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.11it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.06it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.14it/s] 91%|█████████ | 51/56.072265625 [00:12<00:01,  4.09it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.17it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.12it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.19it/s] 98%|█████████▊| 55/56.072265625 [00:13<00:00,  4.13it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.19it/s]57it [00:14,  3.99it/s]                                  epoch 37 iter 0 sum correct 0.869140625 loss 1.8893473148345947
epoch 37 iter 1 sum correct 0.880859375 loss 1.888392686843872
epoch 37 iter 2 sum correct 0.8763020833333334 loss 1.8887470960617065
epoch 37 iter 3 sum correct 0.87548828125 loss 1.8887603282928467
epoch 37 iter 4 sum correct 0.869921875 loss 1.8892143964767456
epoch 37 iter 5 sum correct 0.8717447916666666 loss 1.889032244682312
epoch 37 iter 6 sum correct 0.8736049107142857 loss 1.8889434337615967
epoch 37 iter 7 sum correct 0.868896484375 loss 1.8893247842788696
epoch 37 iter 8 sum correct 0.8650173611111112 loss 1.8896111249923706
epoch 37 iter 9 sum correct 0.8650390625 loss 1.8895829916000366
epoch 37 iter 10 sum correct 0.8654119318181818 loss 1.889549970626831
epoch 37 iter 11 sum correct 0.865234375 loss 1.8895738124847412
epoch 37 iter 12 sum correct 0.8647836538461539 loss 1.8896125555038452
epoch 37 iter 13 sum correct 0.8638392857142857 loss 1.8896751403808594
epoch 37 iter 14 sum correct 0.8614583333333333 loss 1.8898751735687256
epoch 37 iter 15 sum correct 0.8624267578125 loss 1.8897932767868042
epoch 37 iter 16 sum correct 0.8617876838235294 loss 1.8898358345031738
epoch 37 iter 17 sum correct 0.8614366319444444 loss 1.8898775577545166
epoch 37 iter 18 sum correct 0.859375 loss 1.8900349140167236
epoch 37 iter 19 sum correct 0.86005859375 loss 1.8899919986724854
epoch 37 iter 20 sum correct 0.8600260416666666 loss 1.8899749517440796
epoch 37 iter 21 sum correct 0.8592862215909091 loss 1.890036702156067
epoch 37 iter 22 sum correct 0.8584408967391305 loss 1.8900952339172363
epoch 37 iter 23 sum correct 0.857666015625 loss 1.890157699584961
epoch 37 iter 24 sum correct 0.856875 loss 1.8902106285095215
epoch 37 iter 25 sum correct 0.8576472355769231 loss 1.8901554346084595
epoch 37 iter 26 sum correct 0.8569155092592593 loss 1.89020836353302
epoch 37 iter 27 sum correct 0.8574916294642857 loss 1.8901677131652832
epoch 37 iter 28 sum correct 0.8577586206896551 loss 1.890155553817749
epoch 37 iter 29 sum correct 0.8579427083333333 loss 1.8901418447494507
epoch 37 iter 30 sum correct 0.8575478830645161 loss 1.8901751041412354
epoch 37 iter 31 sum correct 0.85723876953125 loss 1.8901952505111694
epoch 37 iter 32 sum correct 0.8576586174242424 loss 1.890156626701355
epoch 37 iter 33 sum correct 0.8578239889705882 loss 1.890141248703003
epoch 37 iter 34 sum correct 0.8578683035714286 loss 1.890141248703003
epoch 37 iter 35 sum correct 0.8581271701388888 loss 1.8901182413101196
epoch 37 iter 36 sum correct 0.8590054898648649 loss 1.8900482654571533
epoch 37 iter 37 sum correct 0.8591694078947368 loss 1.8900359869003296
epoch 37 iter 38 sum correct 0.8590244391025641 loss 1.8900469541549683
epoch 37 iter 39 sum correct 0.859619140625 loss 1.8899967670440674
epoch 37 iter 40 sum correct 0.8594702743902439 loss 1.890005111694336
epoch 37 iter 41 sum correct 0.8592819940476191 loss 1.890018105506897
epoch 37 iter 42 sum correct 0.8591933139534884 loss 1.8900249004364014
epoch 37 iter 43 sum correct 0.8595969460227273 loss 1.8899942636489868
epoch 37 iter 44 sum correct 0.8595052083333333 loss 1.8899980783462524
epoch 37 iter 45 sum correct 0.8597146739130435 loss 1.8899853229522705
epoch 37 iter 46 sum correct 0.8601645611702128 loss 1.889951467514038
epoch 37 iter 47 sum correct 0.8602294921875 loss 1.8899500370025635
epoch 37 iter 48 sum correct 0.8600127551020408 loss 1.8899649381637573
epoch 37 iter 49 sum correct 0.86046875 loss 1.8899281024932861
epoch 37 iter 50 sum correct 0.8602941176470589 loss 1.8899415731430054
epoch 37 iter 51 sum correct 0.8602013221153846 loss 1.8899480104446411
epoch 37 iter 52 sum correct 0.8606279481132075 loss 1.8899121284484863
epoch 37 iter 53 sum correct 0.8604962384259259 loss 1.8899253606796265
epoch 37 iter 54 sum correct 0.8612926136363637 loss 1.8898674249649048
epoch 37 iter 55 sum correct 0.8609793526785714 loss 1.8898911476135254
epoch 37 iter 56 sum correct 0.8469024122807017 loss 1.8899587392807007
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.65it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.35it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.57it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.91it/s]8it [00:01,  5.89it/s]                                 macro  0.534292211407372
micro  0.64084703259961
[[253   0  39  35  69  23  48]
 [ 28   0   6   5  14   0   3]
 [ 60   0 204  19 106  47  60]
 [ 19   0   6 785  22  19  44]
 [ 73   0  43  44 359  21 113]
 [ 12   0  28  22  10 325  18]
 [ 45   0  25  69  85   9 374]]
              precision    recall  f1-score   support

           0       0.52      0.54      0.53       467
           1       0.00      0.00      0.00        56
           2       0.58      0.41      0.48       496
           3       0.80      0.88      0.84       895
           4       0.54      0.55      0.54       653
           5       0.73      0.78      0.76       415
           6       0.57      0.62      0.59       607

    accuracy                           0.64      3589
   macro avg       0.53      0.54      0.53      3589
weighted avg       0.63      0.64      0.63      3589

correct 0.64084703259961 
f1 0.534292211407372 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.74it/s] 29%|██▊       | 2/7.009765625 [00:00<00:01,  3.38it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.66it/s] 57%|█████▋    | 4/7.009765625 [00:00<00:00,  5.84it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  6.24it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.46it/s]8it [00:01,  5.87it/s]                                 macro  0.5362073499503589
micro  0.6453050989133463
[[284   0  52  29  67  11  48]
 [ 26   0   9   6  12   2   0]
 [ 66   0 202  27 111  62  60]
 [ 11   0  10 795  21  20  22]
 [ 68   0  38  41 318  15 114]
 [  7   0  35  27  10 317  20]
 [ 51   0  18  56  84  17 400]]
              precision    recall  f1-score   support

           0       0.55      0.58      0.57       491
           1       0.00      0.00      0.00        55
           2       0.55      0.38      0.45       528
           3       0.81      0.90      0.85       879
           4       0.51      0.54      0.52       594
           5       0.71      0.76      0.74       416
           6       0.60      0.64      0.62       626

    accuracy                           0.65      3589
   macro avg       0.54      0.54      0.54      3589
weighted avg       0.63      0.65      0.63      3589

correct 0.6453050989133463 
f1 0.5362073499503589 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:38,  1.43it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.34it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.86it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.31it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.48it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.72it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.77it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.92it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.90it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.00it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.95it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.05it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  4.00it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.07it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.00it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.09it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.03it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.11it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.07it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.13it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.08it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.15it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.06it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.14it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.08it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.15it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.07it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.13it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.07it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.14it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.04it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.12it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.05it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.12it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.07it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.14it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.08it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.15it/s] 70%|██████▉   | 39/56.072265625 [00:09<00:04,  4.09it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.16it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.07it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.14it/s] 77%|███████▋  | 43/56.072265625 [00:10<00:03,  4.06it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.13it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.08it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.14it/s] 84%|████████▍ | 47/56.072265625 [00:11<00:02,  4.06it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.13it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.07it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.15it/s] 91%|█████████ | 51/56.072265625 [00:12<00:01,  4.06it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.14it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.07it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.15it/s] 98%|█████████▊| 55/56.072265625 [00:13<00:00,  4.07it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.14it/s]57it [00:14,  4.96it/s]                                  57it [00:14,  3.98it/s]epoch 38 iter 0 sum correct 0.8828125 loss 1.8884241580963135
epoch 38 iter 1 sum correct 0.8720703125 loss 1.8891212940216064
epoch 38 iter 2 sum correct 0.865234375 loss 1.889650821685791
epoch 38 iter 3 sum correct 0.8701171875 loss 1.8892462253570557
epoch 38 iter 4 sum correct 0.8734375 loss 1.8889309167861938
epoch 38 iter 5 sum correct 0.8717447916666666 loss 1.889028787612915
epoch 38 iter 6 sum correct 0.8708147321428571 loss 1.889050006866455
epoch 38 iter 7 sum correct 0.871337890625 loss 1.888988733291626
epoch 38 iter 8 sum correct 0.8726128472222222 loss 1.888877511024475
epoch 38 iter 9 sum correct 0.87109375 loss 1.889007806777954
epoch 38 iter 10 sum correct 0.8705610795454546 loss 1.8890607357025146
epoch 38 iter 11 sum correct 0.8717447916666666 loss 1.8889802694320679
epoch 38 iter 12 sum correct 0.8724459134615384 loss 1.8889333009719849
epoch 38 iter 13 sum correct 0.8736049107142857 loss 1.8888391256332397
epoch 38 iter 14 sum correct 0.8725260416666667 loss 1.8889310359954834
epoch 38 iter 15 sum correct 0.8721923828125 loss 1.8889702558517456
epoch 38 iter 16 sum correct 0.8714384191176471 loss 1.8890304565429688
epoch 38 iter 17 sum correct 0.8719618055555556 loss 1.888979434967041
epoch 38 iter 18 sum correct 0.8738692434210527 loss 1.8888399600982666
epoch 38 iter 19 sum correct 0.8744140625 loss 1.8888040781021118
epoch 38 iter 20 sum correct 0.8727678571428571 loss 1.8889274597167969
epoch 38 iter 21 sum correct 0.8714488636363636 loss 1.8890234231948853
epoch 38 iter 22 sum correct 0.8719429347826086 loss 1.8889946937561035
epoch 38 iter 23 sum correct 0.873046875 loss 1.8889076709747314
epoch 38 iter 24 sum correct 0.872734375 loss 1.8889217376708984
epoch 38 iter 25 sum correct 0.8726712740384616 loss 1.8889259099960327
epoch 38 iter 26 sum correct 0.8727575231481481 loss 1.8889168500900269
epoch 38 iter 27 sum correct 0.8732561383928571 loss 1.8888753652572632
epoch 38 iter 28 sum correct 0.8731815732758621 loss 1.88887619972229
epoch 38 iter 29 sum correct 0.8728515625 loss 1.8889013528823853
epoch 38 iter 30 sum correct 0.8727948588709677 loss 1.8889081478118896
epoch 38 iter 31 sum correct 0.87261962890625 loss 1.8889195919036865
epoch 38 iter 32 sum correct 0.8721590909090909 loss 1.8889528512954712
epoch 38 iter 33 sum correct 0.8727596507352942 loss 1.8889117240905762
epoch 38 iter 34 sum correct 0.873046875 loss 1.888883113861084
epoch 38 iter 35 sum correct 0.8738606770833334 loss 1.888826847076416
epoch 38 iter 36 sum correct 0.8736275337837838 loss 1.8888416290283203
epoch 38 iter 37 sum correct 0.8736636513157895 loss 1.888832449913025
epoch 38 iter 38 sum correct 0.8732972756410257 loss 1.8888663053512573
epoch 38 iter 39 sum correct 0.873095703125 loss 1.8888756036758423
epoch 38 iter 40 sum correct 0.8731897865853658 loss 1.8888720273971558
epoch 38 iter 41 sum correct 0.8728608630952381 loss 1.888898253440857
epoch 38 iter 42 sum correct 0.8728197674418605 loss 1.8889061212539673
epoch 38 iter 43 sum correct 0.8724698153409091 loss 1.8889281749725342
epoch 38 iter 44 sum correct 0.8723090277777777 loss 1.8889427185058594
epoch 38 iter 45 sum correct 0.8727072010869565 loss 1.8889117240905762
epoch 38 iter 46 sum correct 0.8725897606382979 loss 1.8889212608337402
epoch 38 iter 47 sum correct 0.8727213541666666 loss 1.8889093399047852
epoch 38 iter 48 sum correct 0.8728077168367347 loss 1.8889005184173584
epoch 38 iter 49 sum correct 0.87296875 loss 1.8888888359069824
epoch 38 iter 50 sum correct 0.8731617647058824 loss 1.888870120048523
epoch 38 iter 51 sum correct 0.8733473557692307 loss 1.888853907585144
epoch 38 iter 52 sum correct 0.8734153891509434 loss 1.888850450515747
epoch 38 iter 53 sum correct 0.8730830439814815 loss 1.8888771533966064
epoch 38 iter 54 sum correct 0.8732244318181818 loss 1.8888636827468872
epoch 38 iter 55 sum correct 0.8733258928571429 loss 1.8888581991195679
epoch 38 iter 56 sum correct 0.8590666118421053 loss 1.888917088508606
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.54it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.09it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.58it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.92it/s]8it [00:01,  5.78it/s]                                 macro  0.5370093915097166
micro  0.6358317079966564
[[268   0  39  29  82   8  41]
 [ 23   0  10   6  13   0   4]
 [ 68   0 228  22 110  32  36]
 [ 26   0  10 768  36  10  45]
 [ 63   0  77  44 383   5  81]
 [ 25   0  30  30  15 303  12]
 [ 47   0  35  56 136   1 332]]
              precision    recall  f1-score   support

           0       0.52      0.57      0.54       467
           1       0.00      0.00      0.00        56
           2       0.53      0.46      0.49       496
           3       0.80      0.86      0.83       895
           4       0.49      0.59      0.54       653
           5       0.84      0.73      0.78       415
           6       0.60      0.55      0.57       607

    accuracy                           0.64      3589
   macro avg       0.54      0.54      0.54      3589
weighted avg       0.63      0.64      0.63      3589

correct 0.6358317079966564 
f1 0.5370093915097166 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.74it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.36it/s] 57%|█████▋    | 4/7.009765625 [00:00<00:00,  5.41it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.60it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.23it/s]8it [00:01,  5.83it/s]                                 macro  0.5401582539219155
micro  0.6427974366118696
[[274   0  57  20  91   6  43]
 [ 26   0   7   9   8   2   3]
 [ 83   0 225  27 111  35  47]
 [ 23   0  17 779  33   7  20]
 [ 46   0  67  30 352   5  94]
 [ 22   0  37  31  18 297  11]
 [ 43   0  24  41 128  10 380]]
              precision    recall  f1-score   support

           0       0.53      0.56      0.54       491
           1       0.00      0.00      0.00        55
           2       0.52      0.43      0.47       528
           3       0.83      0.89      0.86       879
           4       0.48      0.59      0.53       594
           5       0.82      0.71      0.76       416
           6       0.64      0.61      0.62       626

    accuracy                           0.64      3589
   macro avg       0.54      0.54      0.54      3589
weighted avg       0.64      0.64      0.64      3589

correct 0.6427974366118696 
f1 0.5401582539219155 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:38,  1.42it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.33it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.83it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.28it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.48it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.72it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.77it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.94it/s] 16%|█▌        | 9/56.072265625 [00:02<00:11,  3.93it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  3.99it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.96it/s] 21%|██▏       | 12/56.072265625 [00:03<00:11,  3.81it/s] 23%|██▎       | 13/56.072265625 [00:03<00:11,  3.83it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  3.96it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  3.96it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.06it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.02it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.09it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.02it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.10it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.05it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.10it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.03it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.11it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.06it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.11it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.04it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.11it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.06it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.12it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.04it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.13it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.06it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.12it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.05it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.12it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.06it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.12it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  4.06it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.13it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.08it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.13it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  4.06it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.14it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.08it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.12it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  4.04it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.11it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.05it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.11it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  4.05it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.14it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.07it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.14it/s] 98%|█████████▊| 55/56.072265625 [00:13<00:00,  4.08it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.15it/s]57it [00:14,  3.96it/s]                                  epoch 39 iter 0 sum correct 0.890625 loss 1.8873319625854492
epoch 39 iter 1 sum correct 0.876953125 loss 1.8884997367858887
epoch 39 iter 2 sum correct 0.8782552083333334 loss 1.888427495956421
epoch 39 iter 3 sum correct 0.87255859375 loss 1.8888626098632812
epoch 39 iter 4 sum correct 0.8671875 loss 1.8892993927001953
epoch 39 iter 5 sum correct 0.8671875 loss 1.8893427848815918
epoch 39 iter 6 sum correct 0.8674665178571429 loss 1.8892987966537476
epoch 39 iter 7 sum correct 0.86376953125 loss 1.889556884765625
epoch 39 iter 8 sum correct 0.8637152777777778 loss 1.8895879983901978
epoch 39 iter 9 sum correct 0.86015625 loss 1.889884352684021
epoch 39 iter 10 sum correct 0.8586647727272727 loss 1.8900141716003418
epoch 39 iter 11 sum correct 0.85693359375 loss 1.890134334564209
epoch 39 iter 12 sum correct 0.8527644230769231 loss 1.8904517889022827
epoch 39 iter 13 sum correct 0.8505859375 loss 1.8906188011169434
epoch 39 iter 14 sum correct 0.8484375 loss 1.8907867670059204
epoch 39 iter 15 sum correct 0.8486328125 loss 1.8907819986343384
epoch 39 iter 16 sum correct 0.8483455882352942 loss 1.890797734260559
epoch 39 iter 17 sum correct 0.8492838541666666 loss 1.8907190561294556
epoch 39 iter 18 sum correct 0.850328947368421 loss 1.8906446695327759
epoch 39 iter 19 sum correct 0.8513671875 loss 1.8905643224716187
epoch 39 iter 20 sum correct 0.8511904761904762 loss 1.8905726671218872
epoch 39 iter 21 sum correct 0.8516512784090909 loss 1.8905360698699951
epoch 39 iter 22 sum correct 0.8512228260869565 loss 1.8905664682388306
epoch 39 iter 23 sum correct 0.8522135416666666 loss 1.8904826641082764
epoch 39 iter 24 sum correct 0.85265625 loss 1.8904482126235962
epoch 39 iter 25 sum correct 0.8524639423076923 loss 1.8904643058776855
epoch 39 iter 26 sum correct 0.8520688657407407 loss 1.8904876708984375
epoch 39 iter 27 sum correct 0.8521902901785714 loss 1.8904716968536377
epoch 39 iter 28 sum correct 0.8524380387931034 loss 1.8904544115066528
epoch 39 iter 29 sum correct 0.8520182291666667 loss 1.8904837369918823
epoch 39 iter 30 sum correct 0.8520035282258065 loss 1.8904930353164673
epoch 39 iter 31 sum correct 0.851806640625 loss 1.890515685081482
epoch 39 iter 32 sum correct 0.8519176136363636 loss 1.8905075788497925
epoch 39 iter 33 sum correct 0.8523667279411765 loss 1.8904728889465332
epoch 39 iter 34 sum correct 0.8530691964285714 loss 1.8904176950454712
epoch 39 iter 35 sum correct 0.8531358506944444 loss 1.8904154300689697
epoch 39 iter 36 sum correct 0.853515625 loss 1.8903881311416626
epoch 39 iter 37 sum correct 0.8538240131578947 loss 1.8903605937957764
epoch 39 iter 38 sum correct 0.8537660256410257 loss 1.8903652429580688
epoch 39 iter 39 sum correct 0.853466796875 loss 1.8903859853744507
epoch 39 iter 40 sum correct 0.8537538109756098 loss 1.8903571367263794
epoch 39 iter 41 sum correct 0.8541201636904762 loss 1.8903300762176514
epoch 39 iter 42 sum correct 0.8542423691860465 loss 1.890321969985962
epoch 39 iter 43 sum correct 0.8536931818181818 loss 1.8903635740280151
epoch 39 iter 44 sum correct 0.85390625 loss 1.8903541564941406
epoch 39 iter 45 sum correct 0.8539402173913043 loss 1.8903535604476929
epoch 39 iter 46 sum correct 0.8540558510638298 loss 1.8903424739837646
epoch 39 iter 47 sum correct 0.8544921875 loss 1.890305995941162
epoch 39 iter 48 sum correct 0.8541135204081632 loss 1.8903361558914185
epoch 39 iter 49 sum correct 0.85390625 loss 1.8903498649597168
epoch 39 iter 50 sum correct 0.8543198529411765 loss 1.8903224468231201
epoch 39 iter 51 sum correct 0.8547551081730769 loss 1.8902889490127563
epoch 39 iter 52 sum correct 0.855247641509434 loss 1.8902515172958374
epoch 39 iter 53 sum correct 0.8551432291666666 loss 1.8902586698532104
epoch 39 iter 54 sum correct 0.8553267045454546 loss 1.8902411460876465
epoch 39 iter 55 sum correct 0.8556431361607143 loss 1.890215277671814
epoch 39 iter 56 sum correct 0.8416255482456141 loss 1.8903275728225708
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.68it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.32it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.77it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.05it/s]8it [00:01,  5.88it/s]                                 macro  0.5278030172657867
micro  0.6185567010309279
[[275   0  40  14  64  20  54]
 [ 32   0  10   2   7   0   5]
 [ 66   0 237   6  97  27  63]
 [ 41   0  20 676  34  20 104]
 [ 66   0  87  20 339  12 129]
 [ 20   0  37  13  11 317  17]
 [ 47   0  46  30 100   8 376]]
              precision    recall  f1-score   support

           0       0.50      0.59      0.54       467
           1       0.00      0.00      0.00        56
           2       0.50      0.48      0.49       496
           3       0.89      0.76      0.82       895
           4       0.52      0.52      0.52       653
           5       0.78      0.76      0.77       415
           6       0.50      0.62      0.55       607

    accuracy                           0.62      3589
   macro avg       0.53      0.53      0.53      3589
weighted avg       0.63      0.62      0.62      3589

correct 0.6185567010309279 
f1 0.5278030172657867 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.82it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.57it/s] 71%|███████▏  | 5/7.009765625 [00:00<00:00,  6.01it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.22it/s]8it [00:01,  6.18it/s]                                 macro  0.539411304200521
micro  0.6355530788520479
[[298   0  50   9  69   7  58]
 [ 23   0  12   2  14   2   2]
 [ 90   0 236  11  99  33  59]
 [ 41   0  30 691  28  14  75]
 [ 69   0  70  19 311   7 118]
 [ 13   0  45  21  13 303  21]
 [ 41   0  26  10  93  14 442]]
              precision    recall  f1-score   support

           0       0.52      0.61      0.56       491
           1       0.00      0.00      0.00        55
           2       0.50      0.45      0.47       528
           3       0.91      0.79      0.84       879
           4       0.50      0.52      0.51       594
           5       0.80      0.73      0.76       416
           6       0.57      0.71      0.63       626

    accuracy                           0.64      3589
   macro avg       0.54      0.54      0.54      3589
weighted avg       0.64      0.64      0.63      3589

correct 0.6355530788520479 
f1 0.539411304200521 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:39,  1.40it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.33it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.83it/s]  7%|▋         | 4/56.072265625 [00:01<00:16,  3.21it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.41it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.68it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.72it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.89it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.87it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.00it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.94it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.02it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  3.95it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.04it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  3.97it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.04it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  3.96it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  3.98it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  3.90it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.01it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  3.96it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.06it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  3.98it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.08it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.01it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.09it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.01it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.10it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  3.97it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.06it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.00it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.10it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.02it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.09it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  3.96it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.05it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  3.89it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  3.94it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  3.86it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:04,  3.98it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  3.96it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.01it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  3.93it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.04it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  3.90it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.01it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  3.93it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.04it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  3.98it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.07it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  4.01it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.10it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.05it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.13it/s] 98%|█████████▊| 55/56.072265625 [00:14<00:00,  4.07it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.15it/s]57it [00:14,  5.03it/s]                                  57it [00:14,  3.91it/s]epoch 40 iter 0 sum correct 0.86328125 loss 1.8896561861038208
epoch 40 iter 1 sum correct 0.841796875 loss 1.8915460109710693
epoch 40 iter 2 sum correct 0.8255208333333334 loss 1.8926843404769897
epoch 40 iter 3 sum correct 0.82275390625 loss 1.8928207159042358
epoch 40 iter 4 sum correct 0.814453125 loss 1.8935339450836182
epoch 40 iter 5 sum correct 0.80859375 loss 1.893961787223816
epoch 40 iter 6 sum correct 0.798828125 loss 1.8947402238845825
epoch 40 iter 7 sum correct 0.7978515625 loss 1.894790768623352
epoch 40 iter 8 sum correct 0.7897135416666666 loss 1.8954548835754395
epoch 40 iter 9 sum correct 0.7884765625 loss 1.8955566883087158
epoch 40 iter 10 sum correct 0.7881747159090909 loss 1.8955886363983154
epoch 40 iter 11 sum correct 0.7859700520833334 loss 1.8957661390304565
epoch 40 iter 12 sum correct 0.7866586538461539 loss 1.8957109451293945
epoch 40 iter 13 sum correct 0.7844587053571429 loss 1.8958686590194702
epoch 40 iter 14 sum correct 0.78671875 loss 1.895686388015747
epoch 40 iter 15 sum correct 0.78857421875 loss 1.8955514430999756
epoch 40 iter 16 sum correct 0.7899816176470589 loss 1.8954660892486572
epoch 40 iter 17 sum correct 0.7893880208333334 loss 1.895508885383606
epoch 40 iter 18 sum correct 0.7911184210526315 loss 1.8953683376312256
epoch 40 iter 19 sum correct 0.7900390625 loss 1.895466685295105
epoch 40 iter 20 sum correct 0.7917596726190477 loss 1.8953142166137695
epoch 40 iter 21 sum correct 0.7934126420454546 loss 1.8951772451400757
epoch 40 iter 22 sum correct 0.793563179347826 loss 1.89516282081604
epoch 40 iter 23 sum correct 0.7935384114583334 loss 1.895151972770691
epoch 40 iter 24 sum correct 0.794140625 loss 1.89510977268219
epoch 40 iter 25 sum correct 0.7938701923076923 loss 1.8951340913772583
epoch 40 iter 26 sum correct 0.7939814814814815 loss 1.8951239585876465
epoch 40 iter 27 sum correct 0.7955496651785714 loss 1.8949929475784302
epoch 40 iter 28 sum correct 0.7959321120689655 loss 1.894957423210144
epoch 40 iter 29 sum correct 0.7965494791666666 loss 1.8949048519134521
epoch 40 iter 30 sum correct 0.7981980846774194 loss 1.8947721719741821
epoch 40 iter 31 sum correct 0.7991943359375 loss 1.8946913480758667
epoch 40 iter 32 sum correct 0.7996567234848485 loss 1.8946492671966553
epoch 40 iter 33 sum correct 0.80078125 loss 1.8945602178573608
epoch 40 iter 34 sum correct 0.8014508928571429 loss 1.894505262374878
epoch 40 iter 35 sum correct 0.8014865451388888 loss 1.8944956064224243
epoch 40 iter 36 sum correct 0.8033678209459459 loss 1.8943455219268799
epoch 40 iter 37 sum correct 0.8038137335526315 loss 1.8943126201629639
epoch 40 iter 38 sum correct 0.8051382211538461 loss 1.8942162990570068
epoch 40 iter 39 sum correct 0.80576171875 loss 1.894161581993103
epoch 40 iter 40 sum correct 0.806640625 loss 1.8940918445587158
epoch 40 iter 41 sum correct 0.8078497023809523 loss 1.893995761871338
epoch 40 iter 42 sum correct 0.8083666424418605 loss 1.8939540386199951
epoch 40 iter 43 sum correct 0.8084605823863636 loss 1.8939460515975952
epoch 40 iter 44 sum correct 0.8088107638888888 loss 1.8939175605773926
epoch 40 iter 45 sum correct 0.8092306385869565 loss 1.893883228302002
epoch 40 iter 46 sum correct 0.8104222074468085 loss 1.8937922716140747
epoch 40 iter 47 sum correct 0.8114827473958334 loss 1.8937134742736816
epoch 40 iter 48 sum correct 0.8126992984693877 loss 1.8936219215393066
epoch 40 iter 49 sum correct 0.8140234375 loss 1.8935211896896362
epoch 40 iter 50 sum correct 0.8152573529411765 loss 1.8934301137924194
epoch 40 iter 51 sum correct 0.8157677283653846 loss 1.893398404121399
epoch 40 iter 52 sum correct 0.8167747641509434 loss 1.8933179378509521
epoch 40 iter 53 sum correct 0.8173466435185185 loss 1.8932640552520752
epoch 40 iter 54 sum correct 0.8181463068181818 loss 1.8932031393051147
epoch 40 iter 55 sum correct 0.8187430245535714 loss 1.8931479454040527
epoch 40 iter 56 sum correct 0.8054756030701754 loss 1.893085241317749
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.69it/s] 29%|██▊       | 2/7.009765625 [00:00<00:01,  3.21it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.43it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  6.06it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.21it/s]8it [00:01,  5.76it/s]                                 macro  0.5274328505330607
micro  0.6274728336584007
[[204   0  69  28 100  17  49]
 [ 10   0  16   7  18   0   5]
 [ 31   0 241  22 134  29  39]
 [ 14   0  13 766  39  16  47]
 [ 31   0  65  38 418  10  91]
 [ 13   0  43  26  11 317   5]
 [ 23   0  40  70 162   6 306]]
              precision    recall  f1-score   support

           0       0.63      0.44      0.51       467
           1       0.00      0.00      0.00        56
           2       0.49      0.49      0.49       496
           3       0.80      0.86      0.83       895
           4       0.47      0.64      0.54       653
           5       0.80      0.76      0.78       415
           6       0.56      0.50      0.53       607

    accuracy                           0.63      3589
   macro avg       0.54      0.53      0.53      3589
weighted avg       0.62      0.63      0.62      3589

correct 0.6274728336584007 
f1 0.5274328505330607 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.73it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.32it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.75it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.05it/s]8it [00:01,  5.95it/s]                                 macro  0.532033090685855
micro  0.6341599331290053
[[204   0 100  21 103   5  58]
 [  9   0  15   5  18   1   7]
 [ 34   0 240  28 146  37  43]
 [  4   0  15 772  42  12  34]
 [ 26   0  51  39 382   6  90]
 [  4   0  43  27  23 306  13]
 [ 27   0  44  38 130  15 372]]
              precision    recall  f1-score   support

           0       0.66      0.42      0.51       491
           1       0.00      0.00      0.00        55
           2       0.47      0.45      0.46       528
           3       0.83      0.88      0.85       879
           4       0.45      0.64      0.53       594
           5       0.80      0.74      0.77       416
           6       0.60      0.59      0.60       626

    accuracy                           0.63      3589
   macro avg       0.55      0.53      0.53      3589
weighted avg       0.64      0.63      0.63      3589

correct 0.6341599331290053 
f1 0.532033090685855 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:38,  1.42it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.31it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.81it/s]  7%|▋         | 4/56.072265625 [00:01<00:16,  3.20it/s]  9%|▉         | 5/56.072265625 [00:01<00:15,  3.38it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.60it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.64it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.82it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.81it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  3.90it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.85it/s] 21%|██▏       | 12/56.072265625 [00:03<00:11,  3.93it/s] 23%|██▎       | 13/56.072265625 [00:03<00:11,  3.87it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  3.99it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  3.89it/s] 29%|██▊       | 16/56.072265625 [00:04<00:10,  4.00it/s] 30%|███       | 17/56.072265625 [00:04<00:10,  3.85it/s] 32%|███▏      | 18/56.072265625 [00:05<00:09,  3.95it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  3.91it/s] 36%|███▌      | 20/56.072265625 [00:05<00:09,  3.95it/s] 37%|███▋      | 21/56.072265625 [00:05<00:09,  3.87it/s] 39%|███▉      | 22/56.072265625 [00:06<00:08,  3.94it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  3.88it/s] 43%|████▎     | 24/56.072265625 [00:06<00:08,  3.95it/s] 45%|████▍     | 25/56.072265625 [00:06<00:08,  3.87it/s] 46%|████▋     | 26/56.072265625 [00:07<00:08,  3.67it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  3.70it/s] 50%|████▉     | 28/56.072265625 [00:07<00:07,  3.81it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:07,  3.77it/s] 54%|█████▎    | 30/56.072265625 [00:08<00:06,  3.86it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  3.81it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:06,  3.90it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:06,  3.83it/s] 61%|██████    | 34/56.072265625 [00:09<00:05,  3.91it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  3.84it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:05,  3.92it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  3.86it/s] 68%|██████▊   | 38/56.072265625 [00:10<00:04,  3.93it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  3.87it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:04,  3.95it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  3.89it/s] 75%|███████▍  | 42/56.072265625 [00:11<00:03,  3.96it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  3.88it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:03,  3.95it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  3.90it/s] 82%|████████▏ | 46/56.072265625 [00:12<00:02,  3.96it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  3.89it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:02,  3.92it/s] 87%|████████▋ | 49/56.072265625 [00:13<00:01,  3.80it/s] 89%|████████▉ | 50/56.072265625 [00:13<00:01,  3.94it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  3.95it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:01,  4.05it/s] 95%|█████████▍| 53/56.072265625 [00:14<00:00,  4.03it/s] 96%|█████████▋| 54/56.072265625 [00:14<00:00,  4.12it/s] 98%|█████████▊| 55/56.072265625 [00:14<00:00,  4.08it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.14it/s]57it [00:14,  4.95it/s]                                  57it [00:14,  3.82it/s]epoch 41 iter 0 sum correct 0.853515625 loss 1.8902981281280518
epoch 41 iter 1 sum correct 0.8671875 loss 1.8893640041351318
epoch 41 iter 2 sum correct 0.8626302083333334 loss 1.889638900756836
epoch 41 iter 3 sum correct 0.8564453125 loss 1.8901149034500122
epoch 41 iter 4 sum correct 0.851953125 loss 1.8904510736465454
epoch 41 iter 5 sum correct 0.8509114583333334 loss 1.8905575275421143
epoch 41 iter 6 sum correct 0.849609375 loss 1.8906686305999756
epoch 41 iter 7 sum correct 0.842529296875 loss 1.8912297487258911
epoch 41 iter 8 sum correct 0.8348524305555556 loss 1.891764521598816
epoch 41 iter 9 sum correct 0.8294921875 loss 1.8922131061553955
epoch 41 iter 10 sum correct 0.8293678977272727 loss 1.8922204971313477
epoch 41 iter 11 sum correct 0.8277994791666666 loss 1.892374873161316
epoch 41 iter 12 sum correct 0.8255709134615384 loss 1.8925598859786987
epoch 41 iter 13 sum correct 0.8253348214285714 loss 1.8925776481628418
epoch 41 iter 14 sum correct 0.825390625 loss 1.8925832509994507
epoch 41 iter 15 sum correct 0.82568359375 loss 1.8925716876983643
epoch 41 iter 16 sum correct 0.8257123161764706 loss 1.8925743103027344
epoch 41 iter 17 sum correct 0.8262803819444444 loss 1.8925094604492188
epoch 41 iter 18 sum correct 0.826171875 loss 1.8925448656082153
epoch 41 iter 19 sum correct 0.82470703125 loss 1.8926514387130737
epoch 41 iter 20 sum correct 0.8243117559523809 loss 1.8926866054534912
epoch 41 iter 21 sum correct 0.8250177556818182 loss 1.8926329612731934
epoch 41 iter 22 sum correct 0.8256623641304348 loss 1.8925836086273193
epoch 41 iter 23 sum correct 0.827392578125 loss 1.89243745803833
epoch 41 iter 24 sum correct 0.828046875 loss 1.8923847675323486
epoch 41 iter 25 sum correct 0.8282001201923077 loss 1.8923730850219727
epoch 41 iter 26 sum correct 0.8283420138888888 loss 1.8923660516738892
epoch 41 iter 27 sum correct 0.8285435267857143 loss 1.8923536539077759
epoch 41 iter 28 sum correct 0.828057650862069 loss 1.8923954963684082
epoch 41 iter 29 sum correct 0.8283203125 loss 1.892372965812683
epoch 41 iter 30 sum correct 0.8286290322580645 loss 1.8923413753509521
epoch 41 iter 31 sum correct 0.829345703125 loss 1.8922761678695679
epoch 41 iter 32 sum correct 0.8290127840909091 loss 1.8922971487045288
epoch 41 iter 33 sum correct 0.8289292279411765 loss 1.8922991752624512
epoch 41 iter 34 sum correct 0.8300223214285715 loss 1.8922113180160522
epoch 41 iter 35 sum correct 0.8307834201388888 loss 1.892142415046692
epoch 41 iter 36 sum correct 0.8316089527027027 loss 1.892081618309021
epoch 41 iter 37 sum correct 0.8318770559210527 loss 1.8920615911483765
epoch 41 iter 38 sum correct 0.8323818108974359 loss 1.8920221328735352
epoch 41 iter 39 sum correct 0.83291015625 loss 1.8919776678085327
epoch 41 iter 40 sum correct 0.8337938262195121 loss 1.891911506652832
epoch 41 iter 41 sum correct 0.833984375 loss 1.891897201538086
epoch 41 iter 42 sum correct 0.8346202761627907 loss 1.8918474912643433
epoch 41 iter 43 sum correct 0.8351384943181818 loss 1.891810417175293
epoch 41 iter 44 sum correct 0.8352430555555556 loss 1.891798973083496
epoch 41 iter 45 sum correct 0.8358101222826086 loss 1.891754388809204
epoch 41 iter 46 sum correct 0.8364777260638298 loss 1.8916990756988525
epoch 41 iter 47 sum correct 0.8369954427083334 loss 1.8916538953781128
epoch 41 iter 48 sum correct 0.8379304846938775 loss 1.8915791511535645
epoch 41 iter 49 sum correct 0.8381640625 loss 1.8915597200393677
epoch 41 iter 50 sum correct 0.8386948529411765 loss 1.891519546508789
epoch 41 iter 51 sum correct 0.8391676682692307 loss 1.8914841413497925
epoch 41 iter 52 sum correct 0.8391804245283019 loss 1.891475796699524
epoch 41 iter 53 sum correct 0.8393735532407407 loss 1.8914568424224854
epoch 41 iter 54 sum correct 0.8394886363636364 loss 1.8914467096328735
epoch 41 iter 55 sum correct 0.8394601004464286 loss 1.8914449214935303
epoch 41 iter 56 sum correct 0.8258634868421053 loss 1.8913788795471191
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.52it/s] 29%|██▊       | 2/7.009765625 [00:00<00:01,  3.04it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.16it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  6.19it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.47it/s]8it [00:01,  5.71it/s]                                 macro  0.5260325143541774
micro  0.614934522151017
[[261   0  60  21  64  10  51]
 [ 26   0  11   4   8   0   7]
 [ 53   0 243  11  94  38  57]
 [ 25   0  31 685  27  15 112]
 [ 63   0 110  21 325   5 129]
 [ 14   0  30  14  11 320  26]
 [ 43   0  36  34 120   1 373]]
              precision    recall  f1-score   support

           0       0.54      0.56      0.55       467
           1       0.00      0.00      0.00        56
           2       0.47      0.49      0.48       496
           3       0.87      0.77      0.81       895
           4       0.50      0.50      0.50       653
           5       0.82      0.77      0.80       415
           6       0.49      0.61      0.55       607

    accuracy                           0.61      3589
   macro avg       0.53      0.53      0.53      3589
weighted avg       0.62      0.61      0.62      3589

correct 0.614934522151017 
f1 0.5260325143541774 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.78it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.36it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.84it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.03it/s]8it [00:01,  5.99it/s]                                 macro  0.524512302858953
micro  0.6188353301755364
[[283   0  76  13  54   7  58]
 [ 21   0  15   3  10   2   4]
 [ 64   0 245  20  92  43  64]
 [ 25   0  25 704  42  14  69]
 [ 61   0 115  17 274   8 119]
 [ 11   0  50  18   9 292  36]
 [ 39   0  42  18  99   5 423]]
              precision    recall  f1-score   support

           0       0.56      0.58      0.57       491
           1       0.00      0.00      0.00        55
           2       0.43      0.46      0.45       528
           3       0.89      0.80      0.84       879
           4       0.47      0.46      0.47       594
           5       0.79      0.70      0.74       416
           6       0.55      0.68      0.60       626

    accuracy                           0.62      3589
   macro avg       0.53      0.53      0.52      3589
weighted avg       0.62      0.62      0.62      3589

correct 0.6188353301755364 
f1 0.524512302858953 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:39,  1.38it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.29it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.82it/s]  7%|▋         | 4/56.072265625 [00:01<00:16,  3.25it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.44it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.71it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.75it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.91it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.89it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.02it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.98it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.06it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  4.00it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.09it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.01it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.08it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.02it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.10it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.02it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.03it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  3.98it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.08it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.01it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.08it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.01it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.09it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.03it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.09it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.03it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.10it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.03it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.10it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.03it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.11it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.02it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.09it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.02it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.10it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  4.03it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.09it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.02it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.11it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  4.04it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.06it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.00it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.09it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  4.01it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.07it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  3.99it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.08it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  4.04it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.10it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.04it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.12it/s] 98%|█████████▊| 55/56.072265625 [00:14<00:00,  4.07it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.12it/s]57it [00:14,  4.91it/s]                                  57it [00:14,  3.94it/s]epoch 42 iter 0 sum correct 0.8671875 loss 1.8889284133911133
epoch 42 iter 1 sum correct 0.8720703125 loss 1.8887357711791992
epoch 42 iter 2 sum correct 0.8763020833333334 loss 1.8884309530258179
epoch 42 iter 3 sum correct 0.8740234375 loss 1.888643503189087
epoch 42 iter 4 sum correct 0.866015625 loss 1.8892831802368164
epoch 42 iter 5 sum correct 0.8626302083333334 loss 1.8895444869995117
epoch 42 iter 6 sum correct 0.8618861607142857 loss 1.8896218538284302
epoch 42 iter 7 sum correct 0.861328125 loss 1.889670968055725
epoch 42 iter 8 sum correct 0.8587239583333334 loss 1.8898667097091675
epoch 42 iter 9 sum correct 0.854296875 loss 1.8901783227920532
epoch 42 iter 10 sum correct 0.8517400568181818 loss 1.8903864622116089
epoch 42 iter 11 sum correct 0.8486328125 loss 1.8906410932540894
epoch 42 iter 12 sum correct 0.8493088942307693 loss 1.8906062841415405
epoch 42 iter 13 sum correct 0.8501674107142857 loss 1.8905471563339233
epoch 42 iter 14 sum correct 0.8505208333333333 loss 1.890521764755249
epoch 42 iter 15 sum correct 0.8489990234375 loss 1.890655755996704
epoch 42 iter 16 sum correct 0.8483455882352942 loss 1.8907058238983154
epoch 42 iter 17 sum correct 0.8479817708333334 loss 1.8907405138015747
epoch 42 iter 18 sum correct 0.8481702302631579 loss 1.8907157182693481
epoch 42 iter 19 sum correct 0.84775390625 loss 1.890743613243103
epoch 42 iter 20 sum correct 0.8475632440476191 loss 1.8907487392425537
epoch 42 iter 21 sum correct 0.8473011363636364 loss 1.890764832496643
epoch 42 iter 22 sum correct 0.8469769021739131 loss 1.8907837867736816
epoch 42 iter 23 sum correct 0.8467610677083334 loss 1.8908030986785889
epoch 42 iter 24 sum correct 0.846953125 loss 1.8907792568206787
epoch 42 iter 25 sum correct 0.8475811298076923 loss 1.8907335996627808
epoch 42 iter 26 sum correct 0.8478732638888888 loss 1.890709638595581
epoch 42 iter 27 sum correct 0.8482142857142857 loss 1.890671730041504
epoch 42 iter 28 sum correct 0.8482623922413793 loss 1.8906644582748413
epoch 42 iter 29 sum correct 0.8477864583333333 loss 1.890708327293396
epoch 42 iter 30 sum correct 0.8481602822580645 loss 1.890684723854065
epoch 42 iter 31 sum correct 0.8487548828125 loss 1.8906325101852417
epoch 42 iter 32 sum correct 0.8489583333333334 loss 1.8906108140945435
epoch 42 iter 33 sum correct 0.8493795955882353 loss 1.8905761241912842
epoch 42 iter 34 sum correct 0.8484933035714286 loss 1.8906383514404297
epoch 42 iter 35 sum correct 0.8483615451388888 loss 1.8906540870666504
epoch 42 iter 36 sum correct 0.8493982263513513 loss 1.890576958656311
epoch 42 iter 37 sum correct 0.849866365131579 loss 1.8905471563339233
epoch 42 iter 38 sum correct 0.8492588141025641 loss 1.89059317111969
epoch 42 iter 39 sum correct 0.849853515625 loss 1.8905457258224487
epoch 42 iter 40 sum correct 0.8499428353658537 loss 1.8905504941940308
epoch 42 iter 41 sum correct 0.8497488839285714 loss 1.8905662298202515
epoch 42 iter 42 sum correct 0.8506540697674418 loss 1.890498161315918
epoch 42 iter 43 sum correct 0.8509410511363636 loss 1.8904750347137451
epoch 42 iter 44 sum correct 0.8516493055555555 loss 1.8904201984405518
epoch 42 iter 45 sum correct 0.8513502038043478 loss 1.8904379606246948
epoch 42 iter 46 sum correct 0.8517702792553191 loss 1.8904054164886475
epoch 42 iter 47 sum correct 0.8515625 loss 1.8904204368591309
epoch 42 iter 48 sum correct 0.851921237244898 loss 1.8903915882110596
epoch 42 iter 49 sum correct 0.8523046875 loss 1.89035964012146
epoch 42 iter 50 sum correct 0.852749693627451 loss 1.890320897102356
epoch 42 iter 51 sum correct 0.8529897836538461 loss 1.890299677848816
epoch 42 iter 52 sum correct 0.8527417452830188 loss 1.890321135520935
epoch 42 iter 53 sum correct 0.8528284143518519 loss 1.8903130292892456
epoch 42 iter 54 sum correct 0.8529474431818181 loss 1.8903024196624756
epoch 42 iter 55 sum correct 0.8531668526785714 loss 1.8902819156646729
epoch 42 iter 56 sum correct 0.8392955043859649 loss 1.8902555704116821
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.58it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.19it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.57it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.86it/s]8it [00:01,  5.78it/s]                                 macro  0.5268750455677733
micro  0.620228475898579
[[275   0  76  28  50   5  33]
 [ 18   0  17   4  12   0   5]
 [ 59   0 277  14  82  26  38]
 [ 23   0  58 741  12  17  44]
 [ 77   0 135  32 304   9  96]
 [ 15   0  61  16   6 310   7]
 [ 69   0  73  47  89  10 319]]
              precision    recall  f1-score   support

           0       0.51      0.59      0.55       467
           1       0.00      0.00      0.00        56
           2       0.40      0.56      0.46       496
           3       0.84      0.83      0.83       895
           4       0.55      0.47      0.50       653
           5       0.82      0.75      0.78       415
           6       0.59      0.53      0.56       607

    accuracy                           0.62      3589
   macro avg       0.53      0.53      0.53      3589
weighted avg       0.63      0.62      0.62      3589

correct 0.620228475898579 
f1 0.5268750455677733 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.73it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.41it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.75it/s] 86%|████████▌ | 6/7.009765625 [00:01<00:00,  6.49it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.81it/s]8it [00:01,  5.80it/s]                                 macro  0.5359296934829679
micro  0.6344385622736138
[[294   0  86  25  49   2  35]
 [ 19   0  17   8   8   2   1]
 [ 65   0 310  19  69  30  35]
 [ 21   0  36 760  23  16  23]
 [ 76   0 120  39 267   7  85]
 [ 14   0  79  15   9 289  10]
 [ 70   0  58  37  91  13 357]]
              precision    recall  f1-score   support

           0       0.53      0.60      0.56       491
           1       0.00      0.00      0.00        55
           2       0.44      0.59      0.50       528
           3       0.84      0.86      0.85       879
           4       0.52      0.45      0.48       594
           5       0.81      0.69      0.75       416
           6       0.65      0.57      0.61       626

    accuracy                           0.63      3589
   macro avg       0.54      0.54      0.54      3589
weighted avg       0.64      0.63      0.63      3589

correct 0.6344385622736138 
f1 0.5359296934829679 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:39,  1.40it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.32it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.82it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.27it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.45it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.71it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.76it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.91it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.87it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.01it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.95it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.04it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  3.96it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.07it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  3.99it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.06it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  3.97it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.07it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  3.98it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.05it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  3.99it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.08it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  3.99it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.06it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  3.97it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.07it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  3.98it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.05it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  3.99it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.08it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  3.99it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.07it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  3.98it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.08it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  3.99it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.06it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  3.98it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.08it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  3.99it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.06it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  3.98it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.08it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  4.01it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.08it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  3.98it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.08it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  4.00it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.07it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  3.98it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.09it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  4.01it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:01,  3.78it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  3.78it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  3.94it/s] 98%|█████████▊| 55/56.072265625 [00:14<00:00,  3.91it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.01it/s]57it [00:14,  4.85it/s]                                  57it [00:14,  3.91it/s]epoch 43 iter 0 sum correct 0.859375 loss 1.8896549940109253
epoch 43 iter 1 sum correct 0.8662109375 loss 1.8891681432724
epoch 43 iter 2 sum correct 0.8743489583333334 loss 1.8885606527328491
epoch 43 iter 3 sum correct 0.8740234375 loss 1.8886654376983643
epoch 43 iter 4 sum correct 0.870703125 loss 1.8889009952545166
epoch 43 iter 5 sum correct 0.8675130208333334 loss 1.8890998363494873
epoch 43 iter 6 sum correct 0.865234375 loss 1.8892648220062256
epoch 43 iter 7 sum correct 0.86279296875 loss 1.8894740343093872
epoch 43 iter 8 sum correct 0.86328125 loss 1.8894459009170532
epoch 43 iter 9 sum correct 0.8607421875 loss 1.8896583318710327
epoch 43 iter 10 sum correct 0.8572443181818182 loss 1.8899753093719482
epoch 43 iter 11 sum correct 0.85595703125 loss 1.8900727033615112
epoch 43 iter 12 sum correct 0.8551682692307693 loss 1.8901162147521973
epoch 43 iter 13 sum correct 0.8533761160714286 loss 1.890271782875061
epoch 43 iter 14 sum correct 0.8544270833333333 loss 1.8901968002319336
epoch 43 iter 15 sum correct 0.8533935546875 loss 1.8902746438980103
epoch 43 iter 16 sum correct 0.8531709558823529 loss 1.8902796506881714
epoch 43 iter 17 sum correct 0.8532986111111112 loss 1.8902661800384521
epoch 43 iter 18 sum correct 0.8520764802631579 loss 1.8903594017028809
epoch 43 iter 19 sum correct 0.85361328125 loss 1.8902353048324585
epoch 43 iter 20 sum correct 0.8544456845238095 loss 1.8901625871658325
epoch 43 iter 21 sum correct 0.8550248579545454 loss 1.890113353729248
epoch 43 iter 22 sum correct 0.8544497282608695 loss 1.8901623487472534
epoch 43 iter 23 sum correct 0.853271484375 loss 1.8902573585510254
epoch 43 iter 24 sum correct 0.853125 loss 1.8902727365493774
epoch 43 iter 25 sum correct 0.8535907451923077 loss 1.8902400732040405
epoch 43 iter 26 sum correct 0.8543836805555556 loss 1.8901762962341309
epoch 43 iter 27 sum correct 0.8547014508928571 loss 1.8901582956314087
epoch 43 iter 28 sum correct 0.8551993534482759 loss 1.8901281356811523
epoch 43 iter 29 sum correct 0.8555338541666667 loss 1.890101671218872
epoch 43 iter 30 sum correct 0.8556577620967742 loss 1.8900948762893677
epoch 43 iter 31 sum correct 0.8553466796875 loss 1.8901176452636719
epoch 43 iter 32 sum correct 0.8548768939393939 loss 1.8901621103286743
epoch 43 iter 33 sum correct 0.8551240808823529 loss 1.8901416063308716
epoch 43 iter 34 sum correct 0.8551897321428571 loss 1.8901375532150269
epoch 43 iter 35 sum correct 0.8557400173611112 loss 1.8900867700576782
epoch 43 iter 36 sum correct 0.8561549831081081 loss 1.8900580406188965
epoch 43 iter 37 sum correct 0.8560341282894737 loss 1.890064001083374
epoch 43 iter 38 sum correct 0.8569711538461539 loss 1.8899943828582764
epoch 43 iter 39 sum correct 0.856787109375 loss 1.8900102376937866
epoch 43 iter 40 sum correct 0.8571360518292683 loss 1.8899848461151123
epoch 43 iter 41 sum correct 0.857421875 loss 1.8899638652801514
epoch 43 iter 42 sum correct 0.857921511627907 loss 1.8899219036102295
epoch 43 iter 43 sum correct 0.8585316051136364 loss 1.8898733854293823
epoch 43 iter 44 sum correct 0.8586371527777777 loss 1.8898619413375854
epoch 43 iter 45 sum correct 0.8589504076086957 loss 1.8898402452468872
epoch 43 iter 46 sum correct 0.8590425531914894 loss 1.8898402452468872
epoch 43 iter 47 sum correct 0.8588053385416666 loss 1.8898557424545288
epoch 43 iter 48 sum correct 0.8589764030612245 loss 1.8898417949676514
epoch 43 iter 49 sum correct 0.8589453125 loss 1.8898406028747559
epoch 43 iter 50 sum correct 0.8589920343137255 loss 1.8898338079452515
epoch 43 iter 51 sum correct 0.8592998798076923 loss 1.8898147344589233
epoch 43 iter 52 sum correct 0.8597435141509434 loss 1.8897780179977417
epoch 43 iter 53 sum correct 0.8599537037037037 loss 1.8897606134414673
epoch 43 iter 54 sum correct 0.8601207386363636 loss 1.8897480964660645
epoch 43 iter 55 sum correct 0.8601422991071429 loss 1.889746904373169
epoch 43 iter 56 sum correct 0.8460457785087719 loss 1.8898367881774902
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.56it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.12it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.54it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.84it/s]8it [00:01,  5.65it/s]                                 macro  0.5280397549133238
micro  0.6285873502368348
[[202   0  71  47  76  10  61]
 [ 14   0  18   8  10   1   5]
 [ 38   0 237  22 104  34  61]
 [  8   0  19 770  23  19  56]
 [ 37   0  86  52 327   5 146]
 [  7   0  36  21  10 327  14]
 [ 14   0  48  69  77   6 393]]
              precision    recall  f1-score   support

           0       0.63      0.43      0.51       467
           1       0.00      0.00      0.00        56
           2       0.46      0.48      0.47       496
           3       0.78      0.86      0.82       895
           4       0.52      0.50      0.51       653
           5       0.81      0.79      0.80       415
           6       0.53      0.65      0.59       607

    accuracy                           0.63      3589
   macro avg       0.53      0.53      0.53      3589
weighted avg       0.62      0.63      0.62      3589

correct 0.6285873502368348 
f1 0.5280397549133238 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.60it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.19it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.68it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.97it/s]8it [00:01,  5.85it/s]                                 macro  0.527892621943512
micro  0.6308163833937029
[[217   0  98  39  68   5  64]
 [ 14   0  24   6   4   1   6]
 [ 36   0 261  36 102  41  52]
 [  4   0  14 786  27  21  27]
 [ 27   0  76  48 295  10 138]
 [  7   0  50  24  13 308  14]
 [ 19   0  37  62  98  13 397]]
              precision    recall  f1-score   support

           0       0.67      0.44      0.53       491
           1       0.00      0.00      0.00        55
           2       0.47      0.49      0.48       528
           3       0.79      0.89      0.84       879
           4       0.49      0.50      0.49       594
           5       0.77      0.74      0.76       416
           6       0.57      0.63      0.60       626

    accuracy                           0.63      3589
   macro avg       0.54      0.53      0.53      3589
weighted avg       0.62      0.63      0.62      3589

correct 0.6308163833937029 
f1 0.527892621943512 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:39,  1.41it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.33it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.84it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.28it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.47it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.72it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.76it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.93it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.91it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.03it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.98it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.08it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  4.04it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.12it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.04it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.12it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.06it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.13it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.04it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.12it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.05it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.12it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.03it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.11it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.04it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.12it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.03it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.11it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.04it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.11it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.04it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.12it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.02it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.10it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.01it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.09it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.01it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.10it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  4.01it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.09it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.01it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.10it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  4.01it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.09it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.00it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.10it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  4.02it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.10it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.01it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.11it/s] 91%|█████████ | 51/56.072265625 [00:12<00:01,  4.04it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.12it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.04it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.13it/s] 98%|█████████▊| 55/56.072265625 [00:13<00:00,  4.07it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.14it/s]57it [00:14,  5.01it/s]                                  57it [00:14,  3.96it/s]epoch 44 iter 0 sum correct 0.861328125 loss 1.8893909454345703
epoch 44 iter 1 sum correct 0.8662109375 loss 1.8891022205352783
epoch 44 iter 2 sum correct 0.8522135416666666 loss 1.890268087387085
epoch 44 iter 3 sum correct 0.8505859375 loss 1.8903762102127075
epoch 44 iter 4 sum correct 0.846484375 loss 1.8907991647720337
epoch 44 iter 5 sum correct 0.8382161458333334 loss 1.891479253768921
epoch 44 iter 6 sum correct 0.8387276785714286 loss 1.89143705368042
epoch 44 iter 7 sum correct 0.8349609375 loss 1.8917165994644165
epoch 44 iter 8 sum correct 0.833984375 loss 1.891771912574768
epoch 44 iter 9 sum correct 0.8361328125 loss 1.8916168212890625
epoch 44 iter 10 sum correct 0.8355823863636364 loss 1.8916585445404053
epoch 44 iter 11 sum correct 0.8351236979166666 loss 1.8916966915130615
epoch 44 iter 12 sum correct 0.8351862980769231 loss 1.8916857242584229
epoch 44 iter 13 sum correct 0.8357979910714286 loss 1.891645908355713
epoch 44 iter 14 sum correct 0.8358072916666667 loss 1.8916395902633667
epoch 44 iter 15 sum correct 0.8355712890625 loss 1.8916531801223755
epoch 44 iter 16 sum correct 0.8363970588235294 loss 1.8915910720825195
epoch 44 iter 17 sum correct 0.8365885416666666 loss 1.8915748596191406
epoch 44 iter 18 sum correct 0.8369654605263158 loss 1.8915454149246216
epoch 44 iter 19 sum correct 0.83740234375 loss 1.8915045261383057
epoch 44 iter 20 sum correct 0.8372395833333334 loss 1.8915202617645264
epoch 44 iter 21 sum correct 0.8382457386363636 loss 1.8914507627487183
epoch 44 iter 22 sum correct 0.8390794836956522 loss 1.8913915157318115
epoch 44 iter 23 sum correct 0.839599609375 loss 1.8913520574569702
epoch 44 iter 24 sum correct 0.84140625 loss 1.8912078142166138
epoch 44 iter 25 sum correct 0.8429236778846154 loss 1.891081690788269
epoch 44 iter 26 sum correct 0.8423755787037037 loss 1.891123652458191
epoch 44 iter 27 sum correct 0.8429129464285714 loss 1.8910847902297974
epoch 44 iter 28 sum correct 0.843682650862069 loss 1.8910343647003174
epoch 44 iter 29 sum correct 0.8440755208333334 loss 1.8910043239593506
epoch 44 iter 30 sum correct 0.8451360887096774 loss 1.8909227848052979
epoch 44 iter 31 sum correct 0.8463134765625 loss 1.8908319473266602
epoch 44 iter 32 sum correct 0.8465909090909091 loss 1.8908029794692993
epoch 44 iter 33 sum correct 0.8466796875 loss 1.8907946348190308
epoch 44 iter 34 sum correct 0.8468191964285714 loss 1.8907828330993652
epoch 44 iter 35 sum correct 0.8468424479166666 loss 1.890769600868225
epoch 44 iter 36 sum correct 0.8470755912162162 loss 1.8907508850097656
epoch 44 iter 37 sum correct 0.8477076480263158 loss 1.8907045125961304
epoch 44 iter 38 sum correct 0.8479567307692307 loss 1.8906811475753784
epoch 44 iter 39 sum correct 0.84765625 loss 1.8907053470611572
epoch 44 iter 40 sum correct 0.8478944359756098 loss 1.8906842470169067
epoch 44 iter 41 sum correct 0.8484468005952381 loss 1.890641212463379
epoch 44 iter 42 sum correct 0.8491551598837209 loss 1.8905763626098633
epoch 44 iter 43 sum correct 0.8503639914772727 loss 1.8904836177825928
epoch 44 iter 44 sum correct 0.8514756944444445 loss 1.8903939723968506
epoch 44 iter 45 sum correct 0.851180366847826 loss 1.8904112577438354
epoch 44 iter 46 sum correct 0.851063829787234 loss 1.8904197216033936
epoch 44 iter 47 sum correct 0.8511149088541666 loss 1.890411138534546
epoch 44 iter 48 sum correct 0.8509247448979592 loss 1.890424132347107
epoch 44 iter 49 sum correct 0.8508203125 loss 1.890429139137268
epoch 44 iter 50 sum correct 0.8507199754901961 loss 1.8904370069503784
epoch 44 iter 51 sum correct 0.8505108173076923 loss 1.890450358390808
epoch 44 iter 52 sum correct 0.8510097287735849 loss 1.8904225826263428
epoch 44 iter 53 sum correct 0.8506582754629629 loss 1.8904492855072021
epoch 44 iter 54 sum correct 0.85078125 loss 1.890437364578247
epoch 44 iter 55 sum correct 0.8511788504464286 loss 1.8904025554656982
epoch 44 iter 56 sum correct 0.8372053179824561 loss 1.8905417919158936
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:04,  1.50it/s] 43%|████▎     | 3/7.009765625 [00:00<00:01,  3.99it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.58it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.76it/s]8it [00:01,  5.62it/s]                                 macro  0.5167815326436254
micro  0.6163276678740597
[[264   0  37  31  66  24  45]
 [ 27   0   7   5  13   0   4]
 [ 69   0 210  10  87  69  51]
 [ 28   0  17 710  43  52  45]
 [ 79   0  74  26 323  30 121]
 [ 12   0  19  14   7 356   7]
 [ 43   0  33  43 110  29 349]]
              precision    recall  f1-score   support

           0       0.51      0.57      0.53       467
           1       0.00      0.00      0.00        56
           2       0.53      0.42      0.47       496
           3       0.85      0.79      0.82       895
           4       0.50      0.49      0.50       653
           5       0.64      0.86      0.73       415
           6       0.56      0.57      0.57       607

    accuracy                           0.62      3589
   macro avg       0.51      0.53      0.52      3589
weighted avg       0.61      0.62      0.61      3589

correct 0.6163276678740597 
f1 0.5167815326436254 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.76it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.43it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.97it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.21it/s]8it [00:01,  6.07it/s]                                 macro  0.5289900954949075
micro  0.6327667874059627
[[283   0  46  28  68  13  53]
 [ 30   0   8   5   6   2   4]
 [ 83   0 229  18  83  75  40]
 [ 17   0  15 749  41  35  22]
 [ 66   0  65  27 302  18 116]
 [ 13   0  19  17   8 353   6]
 [ 38   0  40  41 104  48 355]]
              precision    recall  f1-score   support

           0       0.53      0.58      0.55       491
           1       0.00      0.00      0.00        55
           2       0.54      0.43      0.48       528
           3       0.85      0.85      0.85       879
           4       0.49      0.51      0.50       594
           5       0.65      0.85      0.74       416
           6       0.60      0.57      0.58       626

    accuracy                           0.63      3589
   macro avg       0.52      0.54      0.53      3589
weighted avg       0.62      0.63      0.62      3589

correct 0.6327667874059627 
f1 0.5289900954949075 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:39,  1.41it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.31it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.84it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.28it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.49it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.73it/s] 12%|█▏        | 7/56.072265625 [00:02<00:12,  3.81it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.96it/s] 16%|█▌        | 9/56.072265625 [00:02<00:11,  3.93it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.02it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.98it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.08it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  4.03it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.10it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.04it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.12it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.07it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.14it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.08it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.14it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.06it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.14it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.07it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.14it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.08it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.16it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.09it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.16it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.09it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.16it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.09it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.15it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.08it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.15it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.02it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.05it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.02it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.10it/s] 70%|██████▉   | 39/56.072265625 [00:09<00:04,  4.03it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.11it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.04it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.12it/s] 77%|███████▋  | 43/56.072265625 [00:10<00:03,  4.04it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.09it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.02it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.10it/s] 84%|████████▍ | 47/56.072265625 [00:11<00:02,  4.04it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.12it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.04it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.13it/s] 91%|█████████ | 51/56.072265625 [00:12<00:01,  4.09it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.16it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.11it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.18it/s] 98%|█████████▊| 55/56.072265625 [00:13<00:00,  4.13it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.19it/s]57it [00:14,  3.99it/s]                                  epoch 45 iter 0 sum correct 0.890625 loss 1.8872389793395996
epoch 45 iter 1 sum correct 0.8828125 loss 1.887832522392273
epoch 45 iter 2 sum correct 0.8723958333333334 loss 1.8887161016464233
epoch 45 iter 3 sum correct 0.87109375 loss 1.8887407779693604
epoch 45 iter 4 sum correct 0.860546875 loss 1.8895419836044312
epoch 45 iter 5 sum correct 0.861328125 loss 1.8894962072372437
epoch 45 iter 6 sum correct 0.8610491071428571 loss 1.8895223140716553
epoch 45 iter 7 sum correct 0.861572265625 loss 1.8894932270050049
epoch 45 iter 8 sum correct 0.8580729166666666 loss 1.8897711038589478
epoch 45 iter 9 sum correct 0.8583984375 loss 1.889739990234375
epoch 45 iter 10 sum correct 0.8552911931818182 loss 1.8899930715560913
epoch 45 iter 11 sum correct 0.8556315104166666 loss 1.889973521232605
epoch 45 iter 12 sum correct 0.8563701923076923 loss 1.8899401426315308
epoch 45 iter 13 sum correct 0.8549107142857143 loss 1.8900758028030396
epoch 45 iter 14 sum correct 0.854296875 loss 1.8901193141937256
epoch 45 iter 15 sum correct 0.85546875 loss 1.8900384902954102
epoch 45 iter 16 sum correct 0.8562729779411765 loss 1.8899744749069214
epoch 45 iter 17 sum correct 0.8559027777777778 loss 1.8900154829025269
epoch 45 iter 18 sum correct 0.856188322368421 loss 1.8900046348571777
epoch 45 iter 19 sum correct 0.8568359375 loss 1.8899507522583008
epoch 45 iter 20 sum correct 0.8579799107142857 loss 1.8898755311965942
epoch 45 iter 21 sum correct 0.8569779829545454 loss 1.8899428844451904
epoch 45 iter 22 sum correct 0.8570822010869565 loss 1.8899366855621338
epoch 45 iter 23 sum correct 0.8567708333333334 loss 1.889951229095459
epoch 45 iter 24 sum correct 0.85671875 loss 1.8899537324905396
epoch 45 iter 25 sum correct 0.8571965144230769 loss 1.8899117708206177
epoch 45 iter 26 sum correct 0.8569878472222222 loss 1.8899290561676025
epoch 45 iter 27 sum correct 0.8574916294642857 loss 1.8898833990097046
epoch 45 iter 28 sum correct 0.8579606681034483 loss 1.889843225479126
epoch 45 iter 29 sum correct 0.857421875 loss 1.8898814916610718
epoch 45 iter 30 sum correct 0.8576108870967742 loss 1.8898652791976929
epoch 45 iter 31 sum correct 0.85797119140625 loss 1.889837622642517
epoch 45 iter 32 sum correct 0.8586055871212122 loss 1.8897991180419922
epoch 45 iter 33 sum correct 0.8592026654411765 loss 1.8897494077682495
epoch 45 iter 34 sum correct 0.8594866071428572 loss 1.8897244930267334
epoch 45 iter 35 sum correct 0.8597547743055556 loss 1.8897004127502441
epoch 45 iter 36 sum correct 0.8601140202702703 loss 1.8896690607070923
epoch 45 iter 37 sum correct 0.8599403782894737 loss 1.8896856307983398
epoch 45 iter 38 sum correct 0.8604767628205128 loss 1.8896466493606567
epoch 45 iter 39 sum correct 0.860693359375 loss 1.8896286487579346
epoch 45 iter 40 sum correct 0.8607088414634146 loss 1.8896293640136719
epoch 45 iter 41 sum correct 0.8607235863095238 loss 1.8896292448043823
epoch 45 iter 42 sum correct 0.8606468023255814 loss 1.889635682106018
epoch 45 iter 43 sum correct 0.8602627840909091 loss 1.8896634578704834
epoch 45 iter 44 sum correct 0.8604166666666667 loss 1.8896514177322388
epoch 45 iter 45 sum correct 0.8604364809782609 loss 1.8896450996398926
epoch 45 iter 46 sum correct 0.8606216755319149 loss 1.8896311521530151
epoch 45 iter 47 sum correct 0.8605550130208334 loss 1.8896335363388062
epoch 45 iter 48 sum correct 0.8606106505102041 loss 1.8896260261535645
epoch 45 iter 49 sum correct 0.860703125 loss 1.889615535736084
epoch 45 iter 50 sum correct 0.8605238970588235 loss 1.8896265029907227
epoch 45 iter 51 sum correct 0.8609149639423077 loss 1.8895938396453857
epoch 45 iter 52 sum correct 0.8614755306603774 loss 1.889553189277649
epoch 45 iter 53 sum correct 0.8614728009259259 loss 1.8895528316497803
epoch 45 iter 54 sum correct 0.8613991477272728 loss 1.8895570039749146
epoch 45 iter 55 sum correct 0.861572265625 loss 1.8895410299301147
epoch 45 iter 56 sum correct 0.8475191885964912 loss 1.889578104019165
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.55it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.07it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.67it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.84it/s]8it [00:01,  5.65it/s]                                 macro  0.5172627933020115
micro  0.6104764558372806
[[222   0 104  20  75  14  32]
 [ 24   0  21   2   5   0   4]
 [ 41   0 277  12  97  40  29]
 [ 20   0  51 735  29  18  42]
 [ 58   0 147  27 328  10  83]
 [  8   0  53  13   9 324   8]
 [ 38   0  83  45 125  11 305]]
              precision    recall  f1-score   support

           0       0.54      0.48      0.51       467
           1       0.00      0.00      0.00        56
           2       0.38      0.56      0.45       496
           3       0.86      0.82      0.84       895
           4       0.49      0.50      0.50       653
           5       0.78      0.78      0.78       415
           6       0.61      0.50      0.55       607

    accuracy                           0.61      3589
   macro avg       0.52      0.52      0.52      3589
weighted avg       0.62      0.61      0.61      3589

correct 0.6104764558372806 
f1 0.5172627933020115 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.74it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.40it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.74it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.81it/s]8it [00:01,  5.79it/s]                                 macro  0.5347654555255467
micro  0.6302591251044859
[[245   0 135   8  67   5  31]
 [ 15   0  19   5  12   1   3]
 [ 39   0 318  17  87  38  29]
 [ 17   0  49 749  26  17  21]
 [ 45   0 120  31 307   7  84]
 [ 11   0  64  14   5 310  12]
 [ 50   0  88  32 111  12 333]]
              precision    recall  f1-score   support

           0       0.58      0.50      0.54       491
           1       0.00      0.00      0.00        55
           2       0.40      0.60      0.48       528
           3       0.88      0.85      0.86       879
           4       0.50      0.52      0.51       594
           5       0.79      0.75      0.77       416
           6       0.65      0.53      0.58       626

    accuracy                           0.63      3589
   macro avg       0.54      0.54      0.53      3589
weighted avg       0.64      0.63      0.63      3589

correct 0.6302591251044859 
f1 0.5347654555255467 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:39,  1.41it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.33it/s]  5%|▌         | 3/56.072265625 [00:01<00:19,  2.79it/s]  7%|▋         | 4/56.072265625 [00:01<00:16,  3.23it/s]  9%|▉         | 5/56.072265625 [00:01<00:15,  3.35it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.63it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.73it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.91it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.91it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.03it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  4.01it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.10it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  4.05it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.12it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.05it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.13it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.07it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.14it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.07it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.14it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.08it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  3.93it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  3.94it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.05it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.00it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.10it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.04it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.12it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.05it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.13it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.01it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.04it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.01it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.10it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  3.98it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.06it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.01it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.09it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  4.04it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.11it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.03it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.11it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  4.03it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.10it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.03it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.10it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  4.02it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.10it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.03it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.11it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  4.05it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.12it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.06it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.13it/s] 98%|█████████▊| 55/56.072265625 [00:13<00:00,  4.08it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.15it/s]57it [00:14,  4.99it/s]                                  57it [00:14,  3.95it/s]epoch 46 iter 0 sum correct 0.884765625 loss 1.8877531290054321
epoch 46 iter 1 sum correct 0.880859375 loss 1.8880009651184082
epoch 46 iter 2 sum correct 0.8821614583333334 loss 1.8878655433654785
epoch 46 iter 3 sum correct 0.8759765625 loss 1.8883662223815918
epoch 46 iter 4 sum correct 0.87734375 loss 1.8882840871810913
epoch 46 iter 5 sum correct 0.8772786458333334 loss 1.8882392644882202
epoch 46 iter 6 sum correct 0.8755580357142857 loss 1.8883624076843262
epoch 46 iter 7 sum correct 0.873779296875 loss 1.8884719610214233
epoch 46 iter 8 sum correct 0.8743489583333334 loss 1.8884389400482178
epoch 46 iter 9 sum correct 0.876953125 loss 1.8882423639297485
epoch 46 iter 10 sum correct 0.8778409090909091 loss 1.888171911239624
epoch 46 iter 11 sum correct 0.8780924479166666 loss 1.888144850730896
epoch 46 iter 12 sum correct 0.8778545673076923 loss 1.8881700038909912
epoch 46 iter 13 sum correct 0.8755580357142857 loss 1.8883706331253052
epoch 46 iter 14 sum correct 0.8755208333333333 loss 1.888393759727478
epoch 46 iter 15 sum correct 0.8759765625 loss 1.8883533477783203
epoch 46 iter 16 sum correct 0.8767233455882353 loss 1.8883033990859985
epoch 46 iter 17 sum correct 0.8756510416666666 loss 1.8883825540542603
epoch 46 iter 18 sum correct 0.875719572368421 loss 1.888381004333496
epoch 46 iter 19 sum correct 0.87705078125 loss 1.8882824182510376
epoch 46 iter 20 sum correct 0.876953125 loss 1.8882904052734375
epoch 46 iter 21 sum correct 0.8762428977272727 loss 1.8883498907089233
epoch 46 iter 22 sum correct 0.8756793478260869 loss 1.8883942365646362
epoch 46 iter 23 sum correct 0.8755696614583334 loss 1.888396978378296
epoch 46 iter 24 sum correct 0.875546875 loss 1.888400673866272
epoch 46 iter 25 sum correct 0.8753756009615384 loss 1.8884212970733643
epoch 46 iter 26 sum correct 0.8754340277777778 loss 1.888426661491394
epoch 46 iter 27 sum correct 0.875 loss 1.8884589672088623
epoch 46 iter 28 sum correct 0.8745959051724138 loss 1.888488531112671
epoch 46 iter 29 sum correct 0.8736979166666666 loss 1.8885525465011597
epoch 46 iter 30 sum correct 0.8745589717741935 loss 1.8884896039962769
epoch 46 iter 31 sum correct 0.8748779296875 loss 1.8884599208831787
epoch 46 iter 32 sum correct 0.8742897727272727 loss 1.8885024785995483
epoch 46 iter 33 sum correct 0.8743106617647058 loss 1.888495683670044
epoch 46 iter 34 sum correct 0.8745535714285714 loss 1.8884795904159546
epoch 46 iter 35 sum correct 0.8747287326388888 loss 1.8884658813476562
epoch 46 iter 36 sum correct 0.8747888513513513 loss 1.8884578943252563
epoch 46 iter 37 sum correct 0.8746402138157895 loss 1.8884713649749756
epoch 46 iter 38 sum correct 0.8756510416666666 loss 1.8883981704711914
epoch 46 iter 39 sum correct 0.87587890625 loss 1.8883806467056274
epoch 46 iter 40 sum correct 0.8758098323170732 loss 1.8883787393569946
epoch 46 iter 41 sum correct 0.8756510416666666 loss 1.8883864879608154
epoch 46 iter 42 sum correct 0.8758175872093024 loss 1.888372778892517
epoch 46 iter 43 sum correct 0.8753107244318182 loss 1.8884103298187256
epoch 46 iter 44 sum correct 0.8753038194444445 loss 1.8884096145629883
epoch 46 iter 45 sum correct 0.8757218070652174 loss 1.8883728981018066
epoch 46 iter 46 sum correct 0.8756648936170213 loss 1.8883775472640991
epoch 46 iter 47 sum correct 0.8758951822916666 loss 1.8883583545684814
epoch 46 iter 48 sum correct 0.8756776147959183 loss 1.8883801698684692
epoch 46 iter 49 sum correct 0.8761328125 loss 1.888347864151001
epoch 46 iter 50 sum correct 0.8763786764705882 loss 1.8883261680603027
epoch 46 iter 51 sum correct 0.8765024038461539 loss 1.888318657875061
epoch 46 iter 52 sum correct 0.876953125 loss 1.8882778882980347
epoch 46 iter 53 sum correct 0.8766999421296297 loss 1.8882981538772583
epoch 46 iter 54 sum correct 0.8769886363636363 loss 1.8882780075073242
epoch 46 iter 55 sum correct 0.876953125 loss 1.8882794380187988
epoch 46 iter 56 sum correct 0.8624246162280702 loss 1.8885259628295898
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.66it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.31it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.72it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.87it/s]8it [00:01,  5.80it/s]                                 macro  0.5218935207146542
micro  0.6274728336584007
[[286   0  23  26  53  22  57]
 [ 34   0   2   3  12   1   4]
 [ 83   0 163  22  94  60  74]
 [ 23   0  13 748  15  31  65]
 [ 88   0  52  29 322  15 147]
 [ 14   0  14  18  10 344  15]
 [ 61   0  19  46  81  11 389]]
              precision    recall  f1-score   support

           0       0.49      0.61      0.54       467
           1       0.00      0.00      0.00        56
           2       0.57      0.33      0.42       496
           3       0.84      0.84      0.84       895
           4       0.55      0.49      0.52       653
           5       0.71      0.83      0.77       415
           6       0.52      0.64      0.57       607

    accuracy                           0.63      3589
   macro avg       0.52      0.53      0.52      3589
weighted avg       0.62      0.63      0.62      3589

correct 0.6274728336584007 
f1 0.5218935207146542 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.82it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.55it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.41it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.74it/s]8it [00:01,  5.86it/s]                                 macro  0.5404876481754307
micro  0.6486486486486487
[[314   0  38  18  55  17  49]
 [ 27   0   3  10   9   2   4]
 [ 84   0 193  26  86  65  74]
 [ 30   0   7 765  22  23  32]
 [ 89   0  38  33 291  15 128]
 [ 11   0  15  19  12 343  16]
 [ 68   0  13  39  70  14 422]]
              precision    recall  f1-score   support

           0       0.50      0.64      0.56       491
           1       0.00      0.00      0.00        55
           2       0.63      0.37      0.46       528
           3       0.84      0.87      0.86       879
           4       0.53      0.49      0.51       594
           5       0.72      0.82      0.77       416
           6       0.58      0.67      0.62       626

    accuracy                           0.65      3589
   macro avg       0.54      0.55      0.54      3589
weighted avg       0.64      0.65      0.64      3589

correct 0.6486486486486487 
f1 0.5404876481754307 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:37,  1.48it/s]  4%|▎         | 2/56.072265625 [00:00<00:22,  2.39it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.89it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.31it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.50it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.74it/s] 12%|█▏        | 7/56.072265625 [00:02<00:12,  3.80it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.95it/s] 16%|█▌        | 9/56.072265625 [00:02<00:11,  3.93it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.04it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  4.02it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.09it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  4.03it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.11it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.06it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.11it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.04it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.11it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.01it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.08it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.03it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.09it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.03it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.12it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.06it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.13it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.07it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.14it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.05it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.12it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.04it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.12it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.06it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.14it/s] 62%|██████▏   | 35/56.072265625 [00:08<00:05,  4.08it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.15it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.09it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.16it/s] 70%|██████▉   | 39/56.072265625 [00:09<00:04,  4.09it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.16it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.07it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.14it/s] 77%|███████▋  | 43/56.072265625 [00:10<00:03,  4.07it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.14it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.08it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.14it/s] 84%|████████▍ | 47/56.072265625 [00:11<00:02,  4.07it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.11it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.04it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.11it/s] 91%|█████████ | 51/56.072265625 [00:12<00:01,  4.08it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.15it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.11it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.18it/s] 98%|█████████▊| 55/56.072265625 [00:13<00:00,  4.12it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.18it/s]57it [00:14,  4.00it/s]                                  epoch 47 iter 0 sum correct 0.8984375 loss 1.886636734008789
epoch 47 iter 1 sum correct 0.8955078125 loss 1.8868370056152344
epoch 47 iter 2 sum correct 0.884765625 loss 1.8876781463623047
epoch 47 iter 3 sum correct 0.87353515625 loss 1.8885142803192139
epoch 47 iter 4 sum correct 0.87265625 loss 1.8886200189590454
epoch 47 iter 5 sum correct 0.8727213541666666 loss 1.888611078262329
epoch 47 iter 6 sum correct 0.8705357142857143 loss 1.8887763023376465
epoch 47 iter 7 sum correct 0.869873046875 loss 1.888855218887329
epoch 47 iter 8 sum correct 0.8676215277777778 loss 1.8890228271484375
epoch 47 iter 9 sum correct 0.8638671875 loss 1.8893187046051025
epoch 47 iter 10 sum correct 0.8616832386363636 loss 1.8895392417907715
epoch 47 iter 11 sum correct 0.85791015625 loss 1.889833927154541
epoch 47 iter 12 sum correct 0.857421875 loss 1.8898810148239136
epoch 47 iter 13 sum correct 0.8571428571428571 loss 1.8898869752883911
epoch 47 iter 14 sum correct 0.8572916666666667 loss 1.8898576498031616
epoch 47 iter 15 sum correct 0.855224609375 loss 1.890016794204712
epoch 47 iter 16 sum correct 0.8561580882352942 loss 1.8899271488189697
epoch 47 iter 17 sum correct 0.8560112847222222 loss 1.8899383544921875
epoch 47 iter 18 sum correct 0.8549547697368421 loss 1.8900309801101685
epoch 47 iter 19 sum correct 0.85400390625 loss 1.8901159763336182
epoch 47 iter 20 sum correct 0.8530505952380952 loss 1.89019775390625
epoch 47 iter 21 sum correct 0.8524502840909091 loss 1.890237808227539
epoch 47 iter 22 sum correct 0.8522418478260869 loss 1.8902509212493896
epoch 47 iter 23 sum correct 0.8521321614583334 loss 1.8902602195739746
epoch 47 iter 24 sum correct 0.851640625 loss 1.8902997970581055
epoch 47 iter 25 sum correct 0.8516376201923077 loss 1.8903003931045532
epoch 47 iter 26 sum correct 0.8514178240740741 loss 1.890317678451538
epoch 47 iter 27 sum correct 0.8528878348214286 loss 1.890197515487671
epoch 47 iter 28 sum correct 0.8541217672413793 loss 1.8900983333587646
epoch 47 iter 29 sum correct 0.854296875 loss 1.8900872468948364
epoch 47 iter 30 sum correct 0.8546496975806451 loss 1.8900551795959473
epoch 47 iter 31 sum correct 0.85552978515625 loss 1.8899872303009033
epoch 47 iter 32 sum correct 0.8561789772727273 loss 1.8899387121200562
epoch 47 iter 33 sum correct 0.8562155330882353 loss 1.8899343013763428
epoch 47 iter 34 sum correct 0.85703125 loss 1.8898701667785645
epoch 47 iter 35 sum correct 0.8573133680555556 loss 1.8898462057113647
epoch 47 iter 36 sum correct 0.8573690878378378 loss 1.889845609664917
epoch 47 iter 37 sum correct 0.8579358552631579 loss 1.889801263809204
epoch 47 iter 38 sum correct 0.858223157051282 loss 1.889787197113037
epoch 47 iter 39 sum correct 0.859033203125 loss 1.8897265195846558
epoch 47 iter 40 sum correct 0.8586128048780488 loss 1.8897523880004883
epoch 47 iter 41 sum correct 0.8586309523809523 loss 1.8897475004196167
epoch 47 iter 42 sum correct 0.8589207848837209 loss 1.889724850654602
epoch 47 iter 43 sum correct 0.8590198863636364 loss 1.889718770980835
epoch 47 iter 44 sum correct 0.8591579861111112 loss 1.8897068500518799
epoch 47 iter 45 sum correct 0.859375 loss 1.8896899223327637
epoch 47 iter 46 sum correct 0.8591672207446809 loss 1.8897091150283813
epoch 47 iter 47 sum correct 0.859375 loss 1.8896944522857666
epoch 47 iter 48 sum correct 0.8598533163265306 loss 1.889652132987976
epoch 47 iter 49 sum correct 0.85984375 loss 1.889646053314209
epoch 47 iter 50 sum correct 0.8596047794117647 loss 1.889664649963379
epoch 47 iter 51 sum correct 0.8596003605769231 loss 1.889658808708191
epoch 47 iter 52 sum correct 0.859153891509434 loss 1.889694333076477
epoch 47 iter 53 sum correct 0.8594473379629629 loss 1.8896721601486206
epoch 47 iter 54 sum correct 0.859375 loss 1.8896794319152832
epoch 47 iter 55 sum correct 0.8594796316964286 loss 1.8896666765213013
epoch 47 iter 56 sum correct 0.8454632675438597 loss 1.8896757364273071
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.64it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.24it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.44it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.64it/s]8it [00:01,  5.64it/s]                                 macro  0.5003492670080423
micro  0.6029534689328504
[[145   0  87  55  76  20  84]
 [  8   0  19  10  14   1   4]
 [ 16   0 246  33  85  34  82]
 [  4   0  16 768  25  28  54]
 [ 18   0 116  50 301   7 161]
 [  5   0  29  21   7 318  35]
 [  5   0  47  67  98   4 386]]
              precision    recall  f1-score   support

           0       0.72      0.31      0.43       467
           1       0.00      0.00      0.00        56
           2       0.44      0.50      0.47       496
           3       0.76      0.86      0.81       895
           4       0.50      0.46      0.48       653
           5       0.77      0.77      0.77       415
           6       0.48      0.64      0.55       607

    accuracy                           0.60      3589
   macro avg       0.52      0.50      0.50      3589
weighted avg       0.61      0.60      0.59      3589

correct 0.6029534689328504 
f1 0.5003492670080423 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.81it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.57it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.78it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.03it/s]8it [00:01,  6.09it/s]                                 macro  0.5015670214332636
micro  0.6093619392588465
[[146   0 103  47  97  13  85]
 [  6   0  17  10  13   1   8]
 [ 18   0 263  38  81  45  83]
 [  3   0  18 788  21  24  25]
 [ 14   0  91  54 280   9 146]
 [  2   0  43  22  10 299  40]
 [  6   0  31  63 105  10 411]]
              precision    recall  f1-score   support

           0       0.75      0.30      0.43       491
           1       0.00      0.00      0.00        55
           2       0.46      0.50      0.48       528
           3       0.77      0.90      0.83       879
           4       0.46      0.47      0.47       594
           5       0.75      0.72      0.73       416
           6       0.52      0.66      0.58       626

    accuracy                           0.61      3589
   macro avg       0.53      0.51      0.50      3589
weighted avg       0.61      0.61      0.59      3589

correct 0.6093619392588465 
f1 0.5015670214332636 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:38,  1.43it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.34it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.83it/s]  7%|▋         | 4/56.072265625 [00:01<00:16,  3.25it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.46it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.70it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.76it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.91it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.90it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.00it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.97it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.06it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  3.99it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.08it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.04it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.10it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.03it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.11it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.06it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.12it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.04it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.10it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.02it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.09it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.03it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.10it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.05it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.11it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.03it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.11it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.06it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.12it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.04it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.12it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.04it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.10it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.02it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.11it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  4.06it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.12it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.04it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.12it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  4.06it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.11it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.04it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.12it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  4.00it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:02,  3.72it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  3.74it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  3.90it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  3.90it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:01,  4.00it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  3.97it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.07it/s] 98%|█████████▊| 55/56.072265625 [00:14<00:00,  4.01it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.08it/s]57it [00:14,  4.87it/s]                                  57it [00:14,  3.94it/s]epoch 48 iter 0 sum correct 0.861328125 loss 1.8891819715499878
epoch 48 iter 1 sum correct 0.85546875 loss 1.889643907546997
epoch 48 iter 2 sum correct 0.8639322916666666 loss 1.889103889465332
epoch 48 iter 3 sum correct 0.8583984375 loss 1.8896026611328125
epoch 48 iter 4 sum correct 0.858984375 loss 1.8895609378814697
epoch 48 iter 5 sum correct 0.8561197916666666 loss 1.8897879123687744
epoch 48 iter 6 sum correct 0.8546316964285714 loss 1.8899433612823486
epoch 48 iter 7 sum correct 0.855224609375 loss 1.889890193939209
epoch 48 iter 8 sum correct 0.8537326388888888 loss 1.8900426626205444
epoch 48 iter 9 sum correct 0.852734375 loss 1.8901195526123047
epoch 48 iter 10 sum correct 0.8508522727272727 loss 1.890303373336792
epoch 48 iter 11 sum correct 0.8494466145833334 loss 1.890413522720337
epoch 48 iter 12 sum correct 0.8494591346153846 loss 1.8904129266738892
epoch 48 iter 13 sum correct 0.8475167410714286 loss 1.8905506134033203
epoch 48 iter 14 sum correct 0.8479166666666667 loss 1.8905075788497925
epoch 48 iter 15 sum correct 0.84716796875 loss 1.8905706405639648
epoch 48 iter 16 sum correct 0.8474264705882353 loss 1.890550971031189
epoch 48 iter 17 sum correct 0.8487413194444444 loss 1.8904550075531006
epoch 48 iter 18 sum correct 0.8502261513157895 loss 1.890356421470642
epoch 48 iter 19 sum correct 0.851171875 loss 1.8902742862701416
epoch 48 iter 20 sum correct 0.8498883928571429 loss 1.890378713607788
epoch 48 iter 21 sum correct 0.8508522727272727 loss 1.8903013467788696
epoch 48 iter 22 sum correct 0.8502887228260869 loss 1.8903502225875854
epoch 48 iter 23 sum correct 0.8500162760416666 loss 1.890357494354248
epoch 48 iter 24 sum correct 0.85078125 loss 1.89028799533844
epoch 48 iter 25 sum correct 0.8502854567307693 loss 1.8903330564498901
epoch 48 iter 26 sum correct 0.8501880787037037 loss 1.8903439044952393
epoch 48 iter 27 sum correct 0.8512137276785714 loss 1.8902689218521118
epoch 48 iter 28 sum correct 0.8517645474137931 loss 1.8902227878570557
epoch 48 iter 29 sum correct 0.8516927083333333 loss 1.8902372121810913
epoch 48 iter 30 sum correct 0.8521295362903226 loss 1.8902074098587036
epoch 48 iter 31 sum correct 0.85321044921875 loss 1.890128493309021
epoch 48 iter 32 sum correct 0.8539891098484849 loss 1.8900662660598755
epoch 48 iter 33 sum correct 0.8547219669117647 loss 1.8899980783462524
epoch 48 iter 34 sum correct 0.8547991071428571 loss 1.8899747133255005
epoch 48 iter 35 sum correct 0.85595703125 loss 1.8898893594741821
epoch 48 iter 36 sum correct 0.85546875 loss 1.8899239301681519
epoch 48 iter 37 sum correct 0.8562397203947368 loss 1.8898576498031616
epoch 48 iter 38 sum correct 0.8563201121794872 loss 1.889862060546875
epoch 48 iter 39 sum correct 0.85693359375 loss 1.889815330505371
epoch 48 iter 40 sum correct 0.8569931402439024 loss 1.8898074626922607
epoch 48 iter 41 sum correct 0.857421875 loss 1.8897725343704224
epoch 48 iter 42 sum correct 0.8571039244186046 loss 1.889794111251831
epoch 48 iter 43 sum correct 0.8575994318181818 loss 1.889763593673706
epoch 48 iter 44 sum correct 0.8581163194444444 loss 1.889724612236023
epoch 48 iter 45 sum correct 0.857804008152174 loss 1.8897428512573242
epoch 48 iter 46 sum correct 0.8578789893617021 loss 1.8897383213043213
epoch 48 iter 47 sum correct 0.8581949869791666 loss 1.8897202014923096
epoch 48 iter 48 sum correct 0.8585379464285714 loss 1.8896890878677368
epoch 48 iter 49 sum correct 0.8587890625 loss 1.8896632194519043
epoch 48 iter 50 sum correct 0.8594132965686274 loss 1.8896147012710571
epoch 48 iter 51 sum correct 0.8599008413461539 loss 1.8895787000656128
epoch 48 iter 52 sum correct 0.8593381485849056 loss 1.8896230459213257
epoch 48 iter 53 sum correct 0.8595558449074074 loss 1.8896101713180542
epoch 48 iter 54 sum correct 0.8601917613636364 loss 1.8895623683929443
epoch 48 iter 55 sum correct 0.8602818080357143 loss 1.8895530700683594
epoch 48 iter 56 sum correct 0.8462513706140351 loss 1.8895806074142456
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.61it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.20it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.42it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.71it/s]8it [00:01,  5.64it/s]                                 macro  0.5200286950553262
micro  0.6174421844524938
[[244   0  87  28  59  15  34]
 [ 18   0  19   7   8   1   3]
 [ 57   0 253  21  89  37  39]
 [ 16   0  20 765  30  19  45]
 [ 92   0 123  32 304   7  95]
 [  9   0  46  17   8 327   8]
 [ 65   0  54  54 104   7 323]]
              precision    recall  f1-score   support

           0       0.49      0.52      0.50       467
           1       0.00      0.00      0.00        56
           2       0.42      0.51      0.46       496
           3       0.83      0.85      0.84       895
           4       0.50      0.47      0.48       653
           5       0.79      0.79      0.79       415
           6       0.59      0.53      0.56       607

    accuracy                           0.62      3589
   macro avg       0.52      0.52      0.52      3589
weighted avg       0.61      0.62      0.61      3589

correct 0.6174421844524938 
f1 0.5200286950553262 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.73it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.45it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.88it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.05it/s]8it [00:01,  5.89it/s]                                 macro  0.5349967393220638
micro  0.6344385622736138
[[270   0  84  33  58   8  38]
 [ 17   0  16   8  10   1   3]
 [ 52   0 288  25  87  41  35]
 [ 11   0  21 770  41  18  18]
 [ 67   0 104  33 294   7  89]
 [  6   0  59  23   8 311   9]
 [ 83   0  57  32 100  10 344]]
              precision    recall  f1-score   support

           0       0.53      0.55      0.54       491
           1       0.00      0.00      0.00        55
           2       0.46      0.55      0.50       528
           3       0.83      0.88      0.85       879
           4       0.49      0.49      0.49       594
           5       0.79      0.75      0.77       416
           6       0.64      0.55      0.59       626

    accuracy                           0.63      3589
   macro avg       0.53      0.54      0.53      3589
weighted avg       0.63      0.63      0.63      3589

correct 0.6344385622736138 
f1 0.5349967393220638 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:38,  1.41it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.33it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.84it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.27it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.42it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.68it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.74it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.86it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.84it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  3.93it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.91it/s] 21%|██▏       | 12/56.072265625 [00:03<00:11,  3.97it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  3.94it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.04it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.00it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.08it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.01it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.09it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.03it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.11it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.02it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.10it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.03it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.10it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.00it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.09it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.03it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.11it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.04it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.12it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  3.94it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:06,  3.99it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  3.92it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  3.98it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  3.91it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:05,  3.97it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  3.91it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.03it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  4.00it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.02it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  3.94it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  3.99it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  3.94it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.04it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  3.96it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.06it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  4.01it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:02,  4.01it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  3.92it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.02it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  3.97it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:01,  4.06it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.00it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.08it/s] 98%|█████████▊| 55/56.072265625 [00:14<00:00,  4.03it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.10it/s]57it [00:14,  4.89it/s]                                  57it [00:14,  3.91it/s]epoch 49 iter 0 sum correct 0.919921875 loss 1.885042667388916
epoch 49 iter 1 sum correct 0.900390625 loss 1.88639235496521
epoch 49 iter 2 sum correct 0.900390625 loss 1.8864386081695557
epoch 49 iter 3 sum correct 0.90234375 loss 1.8862160444259644
epoch 49 iter 4 sum correct 0.899609375 loss 1.8864134550094604
epoch 49 iter 5 sum correct 0.8955078125 loss 1.8867123126983643
epoch 49 iter 6 sum correct 0.8948102678571429 loss 1.8867616653442383
epoch 49 iter 7 sum correct 0.887451171875 loss 1.8873263597488403
epoch 49 iter 8 sum correct 0.8858506944444444 loss 1.8874565362930298
epoch 49 iter 9 sum correct 0.8859375 loss 1.8874400854110718
epoch 49 iter 10 sum correct 0.8863636363636364 loss 1.8874056339263916
epoch 49 iter 11 sum correct 0.8834635416666666 loss 1.887617826461792
epoch 49 iter 12 sum correct 0.8838641826923077 loss 1.8875906467437744
epoch 49 iter 13 sum correct 0.8840680803571429 loss 1.887596607208252
epoch 49 iter 14 sum correct 0.8833333333333333 loss 1.8876612186431885
epoch 49 iter 15 sum correct 0.88427734375 loss 1.8875869512557983
epoch 49 iter 16 sum correct 0.8838465073529411 loss 1.8876177072525024
epoch 49 iter 17 sum correct 0.8819444444444444 loss 1.8877623081207275
epoch 49 iter 18 sum correct 0.8814761513157895 loss 1.8877991437911987
epoch 49 iter 19 sum correct 0.88193359375 loss 1.8877705335617065
epoch 49 iter 20 sum correct 0.8816034226190477 loss 1.8878014087677002
epoch 49 iter 21 sum correct 0.8821022727272727 loss 1.8877646923065186
epoch 49 iter 22 sum correct 0.8818783967391305 loss 1.8877862691879272
epoch 49 iter 23 sum correct 0.8826497395833334 loss 1.8877284526824951
epoch 49 iter 24 sum correct 0.882734375 loss 1.887717843055725
epoch 49 iter 25 sum correct 0.8819110576923077 loss 1.8877815008163452
epoch 49 iter 26 sum correct 0.8819444444444444 loss 1.8877781629562378
epoch 49 iter 27 sum correct 0.88134765625 loss 1.8878198862075806
epoch 49 iter 28 sum correct 0.8809940732758621 loss 1.8878417015075684
epoch 49 iter 29 sum correct 0.88125 loss 1.887816309928894
epoch 49 iter 30 sum correct 0.8809223790322581 loss 1.8878449201583862
epoch 49 iter 31 sum correct 0.88092041015625 loss 1.8878448009490967
epoch 49 iter 32 sum correct 0.8809185606060606 loss 1.887843132019043
epoch 49 iter 33 sum correct 0.8807444852941176 loss 1.8878521919250488
epoch 49 iter 34 sum correct 0.8815848214285714 loss 1.8877884149551392
epoch 49 iter 35 sum correct 0.8815646701388888 loss 1.8877898454666138
epoch 49 iter 36 sum correct 0.8811233108108109 loss 1.8878254890441895
epoch 49 iter 37 sum correct 0.8813219572368421 loss 1.887809157371521
epoch 49 iter 38 sum correct 0.8809094551282052 loss 1.8878408670425415
epoch 49 iter 39 sum correct 0.881396484375 loss 1.8878074884414673
epoch 49 iter 40 sum correct 0.8805735518292683 loss 1.887871503829956
epoch 49 iter 41 sum correct 0.8803943452380952 loss 1.8878873586654663
epoch 49 iter 42 sum correct 0.8807231104651163 loss 1.887862205505371
epoch 49 iter 43 sum correct 0.8807262073863636 loss 1.8878623247146606
epoch 49 iter 44 sum correct 0.8806423611111112 loss 1.8878716230392456
epoch 49 iter 45 sum correct 0.8814113451086957 loss 1.8878146409988403
epoch 49 iter 46 sum correct 0.881108710106383 loss 1.8878364562988281
epoch 49 iter 47 sum correct 0.8810221354166666 loss 1.8878467082977295
epoch 49 iter 48 sum correct 0.880859375 loss 1.887858271598816
epoch 49 iter 49 sum correct 0.88140625 loss 1.8878144025802612
epoch 49 iter 50 sum correct 0.8815487132352942 loss 1.8878061771392822
epoch 49 iter 51 sum correct 0.8816856971153846 loss 1.887794017791748
epoch 49 iter 52 sum correct 0.8815595518867925 loss 1.8878048658370972
epoch 49 iter 53 sum correct 0.8816912615740741 loss 1.8877968788146973
epoch 49 iter 54 sum correct 0.8818536931818182 loss 1.8877838850021362
epoch 49 iter 55 sum correct 0.8820452008928571 loss 1.8877689838409424
epoch 49 iter 56 sum correct 0.8677357456140351 loss 1.88775634765625
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.67it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.31it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.75it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.96it/s]8it [00:01,  5.87it/s]                                 macro  0.5306931027512988
micro  0.6263583170799666
[[263   0  36  17 104  14  33]
 [ 24   0   9   3  16   0   4]
 [ 72   0 205   9 143  29  38]
 [ 29   0  21 713  50  20  62]
 [ 61   0  51  16 441  11  73]
 [ 20   0  36  11  18 321   9]
 [ 52   0  19  31 195   5 305]]
              precision    recall  f1-score   support

           0       0.50      0.56      0.53       467
           1       0.00      0.00      0.00        56
           2       0.54      0.41      0.47       496
           3       0.89      0.80      0.84       895
           4       0.46      0.68      0.54       653
           5       0.80      0.77      0.79       415
           6       0.58      0.50      0.54       607

    accuracy                           0.63      3589
   macro avg       0.54      0.53      0.53      3589
weighted avg       0.64      0.63      0.63      3589

correct 0.6263583170799666 
f1 0.5306931027512988 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.89it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.57it/s] 71%|███████▏  | 5/7.009765625 [00:00<00:00,  6.16it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.36it/s]8it [00:01,  6.32it/s]                                 macro  0.5378670190031876
micro  0.6319308999721371
[[283   0  44  14 107   6  37]
 [ 21   0   8   6  16   1   3]
 [ 75   0 225  12 148  34  34]
 [ 25   0  16 726  54  12  46]
 [ 64   0  52  16 383   8  71]
 [ 16   0  31  15  25 313  16]
 [ 44   0  27  18 189  10 338]]
              precision    recall  f1-score   support

           0       0.54      0.58      0.56       491
           1       0.00      0.00      0.00        55
           2       0.56      0.43      0.48       528
           3       0.90      0.83      0.86       879
           4       0.42      0.64      0.51       594
           5       0.82      0.75      0.78       416
           6       0.62      0.54      0.58       626

    accuracy                           0.63      3589
   macro avg       0.55      0.54      0.54      3589
weighted avg       0.65      0.63      0.63      3589

correct 0.6319308999721371 
f1 0.5378670190031876 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:37,  1.46it/s]  4%|▎         | 2/56.072265625 [00:00<00:22,  2.38it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.90it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.32it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.48it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.74it/s] 12%|█▏        | 7/56.072265625 [00:02<00:12,  3.79it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.95it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.91it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.04it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.99it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.09it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  4.01it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.10it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.05it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.13it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  3.93it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  3.98it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  3.89it/s] 36%|███▌      | 20/56.072265625 [00:05<00:09,  3.99it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  3.96it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.05it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  3.94it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.05it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  3.98it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.07it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.02it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.10it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  3.92it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  3.98it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  3.94it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:06,  4.00it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  3.97it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.06it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  3.94it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:05,  4.01it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  3.94it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.04it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  3.99it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.08it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.01it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.02it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  3.97it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.07it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  3.90it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  3.96it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  3.91it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:02,  3.98it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  3.94it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.05it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  4.00it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.09it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.03it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.11it/s] 98%|█████████▊| 55/56.072265625 [00:14<00:00,  4.05it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.13it/s]57it [00:14,  4.94it/s]                                  57it [00:14,  3.92it/s]epoch 50 iter 0 sum correct 0.89453125 loss 1.8867213726043701
epoch 50 iter 1 sum correct 0.890625 loss 1.8871781826019287
epoch 50 iter 2 sum correct 0.88671875 loss 1.8874750137329102
epoch 50 iter 3 sum correct 0.884765625 loss 1.8876216411590576
epoch 50 iter 4 sum correct 0.880859375 loss 1.887992262840271
epoch 50 iter 5 sum correct 0.8766276041666666 loss 1.8882951736450195
epoch 50 iter 6 sum correct 0.8758370535714286 loss 1.888397216796875
epoch 50 iter 7 sum correct 0.870849609375 loss 1.8887786865234375
epoch 50 iter 8 sum correct 0.8680555555555556 loss 1.8889904022216797
epoch 50 iter 9 sum correct 0.8658203125 loss 1.8891719579696655
epoch 50 iter 10 sum correct 0.8643465909090909 loss 1.8892781734466553
epoch 50 iter 11 sum correct 0.86083984375 loss 1.8895409107208252
epoch 50 iter 12 sum correct 0.8607271634615384 loss 1.8895496129989624
epoch 50 iter 13 sum correct 0.8599330357142857 loss 1.8896366357803345
epoch 50 iter 14 sum correct 0.8591145833333333 loss 1.8897112607955933
epoch 50 iter 15 sum correct 0.8575439453125 loss 1.8898299932479858
epoch 50 iter 16 sum correct 0.8566176470588235 loss 1.8899091482162476
epoch 50 iter 17 sum correct 0.857421875 loss 1.8898565769195557
epoch 50 iter 18 sum correct 0.8570106907894737 loss 1.8898869752883911
epoch 50 iter 19 sum correct 0.85712890625 loss 1.88985276222229
epoch 50 iter 20 sum correct 0.8571428571428571 loss 1.8898561000823975
epoch 50 iter 21 sum correct 0.8556463068181818 loss 1.8899611234664917
epoch 50 iter 22 sum correct 0.8558084239130435 loss 1.8899372816085815
epoch 50 iter 23 sum correct 0.85595703125 loss 1.8899242877960205
epoch 50 iter 24 sum correct 0.854921875 loss 1.890000581741333
epoch 50 iter 25 sum correct 0.8550180288461539 loss 1.8899868726730347
epoch 50 iter 26 sum correct 0.8557581018518519 loss 1.8899292945861816
epoch 50 iter 27 sum correct 0.8563058035714286 loss 1.8898814916610718
epoch 50 iter 28 sum correct 0.8568157327586207 loss 1.889853596687317
epoch 50 iter 29 sum correct 0.8565755208333333 loss 1.8898682594299316
epoch 50 iter 30 sum correct 0.8571068548387096 loss 1.8898255825042725
epoch 50 iter 31 sum correct 0.8570556640625 loss 1.8898288011550903
epoch 50 iter 32 sum correct 0.857421875 loss 1.8898141384124756
epoch 50 iter 33 sum correct 0.8567325367647058 loss 1.8898645639419556
epoch 50 iter 34 sum correct 0.8564732142857143 loss 1.8898777961730957
epoch 50 iter 35 sum correct 0.8563368055555556 loss 1.8898906707763672
epoch 50 iter 36 sum correct 0.8562605574324325 loss 1.889898419380188
epoch 50 iter 37 sum correct 0.8565995065789473 loss 1.8898670673370361
epoch 50 iter 38 sum correct 0.8568209134615384 loss 1.889845609664917
epoch 50 iter 39 sum correct 0.8572265625 loss 1.8898082971572876
epoch 50 iter 40 sum correct 0.8575647865853658 loss 1.8897892236709595
epoch 50 iter 41 sum correct 0.8577938988095238 loss 1.8897696733474731
epoch 50 iter 42 sum correct 0.8571947674418605 loss 1.8898133039474487
epoch 50 iter 43 sum correct 0.8567560369318182 loss 1.8898468017578125
epoch 50 iter 44 sum correct 0.8573784722222222 loss 1.8897908926010132
epoch 50 iter 45 sum correct 0.8576341711956522 loss 1.8897660970687866
epoch 50 iter 46 sum correct 0.857920545212766 loss 1.8897396326065063
epoch 50 iter 47 sum correct 0.8582356770833334 loss 1.8897125720977783
epoch 50 iter 48 sum correct 0.8585778061224489 loss 1.8896796703338623
epoch 50 iter 49 sum correct 0.85875 loss 1.8896634578704834
epoch 50 iter 50 sum correct 0.8592984068627451 loss 1.8896247148513794
epoch 50 iter 51 sum correct 0.8601637620192307 loss 1.8895564079284668
epoch 50 iter 52 sum correct 0.8607016509433962 loss 1.889514446258545
epoch 50 iter 53 sum correct 0.8609664351851852 loss 1.8894920349121094
epoch 50 iter 54 sum correct 0.8614701704545454 loss 1.889449954032898
epoch 50 iter 55 sum correct 0.8617466517857143 loss 1.8894304037094116
epoch 50 iter 56 sum correct 0.8476219846491229 loss 1.8895151615142822
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.72it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.38it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  6.03it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.16it/s]8it [00:01,  6.06it/s]                                 macro  0.5211827779085435
micro  0.620228475898579
[[294   0  59  27  36  10  41]
 [ 34   0   7   4   7   1   3]
 [ 82   0 229  14  72  49  50]
 [ 40   0  25 742  20  23  45]
 [106   0 112  27 286  21 101]
 [ 19   0  29  17   8 331  11]
 [ 80   0  49  64  53  17 344]]
              precision    recall  f1-score   support

           0       0.45      0.63      0.52       467
           1       0.00      0.00      0.00        56
           2       0.45      0.46      0.46       496
           3       0.83      0.83      0.83       895
           4       0.59      0.44      0.50       653
           5       0.73      0.80      0.76       415
           6       0.58      0.57      0.57       607

    accuracy                           0.62      3589
   macro avg       0.52      0.53      0.52      3589
weighted avg       0.62      0.62      0.61      3589

correct 0.620228475898579 
f1 0.5211827779085435 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.76it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.49it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.58it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.83it/s]8it [00:01,  5.94it/s]                                 macro  0.5218938219521656
micro  0.6232933964892727
[[313   0  64  22  37  20  35]
 [ 34   0  11   2   3   2   3]
 [ 91   0 247  24  60  62  44]
 [ 50   0  17 745  19  18  30]
 [ 92   0 102  36 241  13 110]
 [ 16   0  27  23  12 323  15]
 [ 87   0  49  36  66  20 368]]
              precision    recall  f1-score   support

           0       0.46      0.64      0.53       491
           1       0.00      0.00      0.00        55
           2       0.48      0.47      0.47       528
           3       0.84      0.85      0.84       879
           4       0.55      0.41      0.47       594
           5       0.71      0.78      0.74       416
           6       0.61      0.59      0.60       626

    accuracy                           0.62      3589
   macro avg       0.52      0.53      0.52      3589
weighted avg       0.62      0.62      0.62      3589

correct 0.6232933964892727 
f1 0.5218938219521656 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:40,  1.36it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.28it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.82it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.27it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.44it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.69it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.73it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.91it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.89it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.01it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.95it/s] 21%|██▏       | 12/56.072265625 [00:03<00:11,  3.76it/s] 23%|██▎       | 13/56.072265625 [00:03<00:11,  3.79it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  3.94it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  3.92it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.04it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  3.99it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.08it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.00it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.09it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.02it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.11it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.01it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.10it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.01it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.10it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.01it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.10it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.01it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.10it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.01it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.10it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.01it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.09it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.00it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.09it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.02it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.10it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  4.02it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.10it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.02it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.11it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  4.02it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.11it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.03it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.11it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  4.03it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.11it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.04it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.12it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  4.06it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.14it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.05it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.14it/s] 98%|█████████▊| 55/56.072265625 [00:14<00:00,  4.07it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.14it/s]57it [00:14,  4.99it/s]                                  57it [00:14,  3.94it/s]epoch 51 iter 0 sum correct 0.87890625 loss 1.887948989868164
epoch 51 iter 1 sum correct 0.873046875 loss 1.8884656429290771
epoch 51 iter 2 sum correct 0.8619791666666666 loss 1.8893163204193115
epoch 51 iter 3 sum correct 0.86083984375 loss 1.8893632888793945
epoch 51 iter 4 sum correct 0.85 loss 1.8902419805526733
epoch 51 iter 5 sum correct 0.8466796875 loss 1.8905694484710693
epoch 51 iter 6 sum correct 0.849609375 loss 1.89035165309906
epoch 51 iter 7 sum correct 0.849609375 loss 1.8903223276138306
epoch 51 iter 8 sum correct 0.8504774305555556 loss 1.8902571201324463
epoch 51 iter 9 sum correct 0.8474609375 loss 1.8905067443847656
epoch 51 iter 10 sum correct 0.8467684659090909 loss 1.8905729055404663
epoch 51 iter 11 sum correct 0.8429361979166666 loss 1.8908766508102417
epoch 51 iter 12 sum correct 0.8420973557692307 loss 1.8909450769424438
epoch 51 iter 13 sum correct 0.8423549107142857 loss 1.890946388244629
epoch 51 iter 14 sum correct 0.8393229166666667 loss 1.8911943435668945
epoch 51 iter 15 sum correct 0.83935546875 loss 1.8911828994750977
epoch 51 iter 16 sum correct 0.8399586397058824 loss 1.8911346197128296
epoch 51 iter 17 sum correct 0.8407118055555556 loss 1.891074538230896
epoch 51 iter 18 sum correct 0.8409745065789473 loss 1.8910480737686157
epoch 51 iter 19 sum correct 0.84189453125 loss 1.8909803628921509
epoch 51 iter 20 sum correct 0.8418898809523809 loss 1.8909778594970703
epoch 51 iter 21 sum correct 0.8440163352272727 loss 1.8908107280731201
epoch 51 iter 22 sum correct 0.845703125 loss 1.8906779289245605
epoch 51 iter 23 sum correct 0.8475748697916666 loss 1.8905357122421265
epoch 51 iter 24 sum correct 0.8475 loss 1.8905324935913086
epoch 51 iter 25 sum correct 0.8481820913461539 loss 1.890474796295166
epoch 51 iter 26 sum correct 0.8494646990740741 loss 1.8903679847717285
epoch 51 iter 27 sum correct 0.8497488839285714 loss 1.8903539180755615
epoch 51 iter 28 sum correct 0.8500134698275862 loss 1.890323281288147
epoch 51 iter 29 sum correct 0.8506510416666667 loss 1.8902671337127686
epoch 51 iter 30 sum correct 0.8509954637096774 loss 1.8902393579483032
epoch 51 iter 31 sum correct 0.85205078125 loss 1.890160322189331
epoch 51 iter 32 sum correct 0.8525094696969697 loss 1.8901188373565674
epoch 51 iter 33 sum correct 0.8528262867647058 loss 1.890094518661499
epoch 51 iter 34 sum correct 0.8532924107142857 loss 1.8900635242462158
epoch 51 iter 35 sum correct 0.8531358506944444 loss 1.8900707960128784
epoch 51 iter 36 sum correct 0.8536211993243243 loss 1.890031337738037
epoch 51 iter 37 sum correct 0.8533100328947368 loss 1.8900542259216309
epoch 51 iter 38 sum correct 0.8539162660256411 loss 1.8900097608566284
epoch 51 iter 39 sum correct 0.8546875 loss 1.889952540397644
epoch 51 iter 40 sum correct 0.8557545731707317 loss 1.8898634910583496
epoch 51 iter 41 sum correct 0.8562593005952381 loss 1.8898193836212158
epoch 51 iter 42 sum correct 0.8562863372093024 loss 1.8898210525512695
epoch 51 iter 43 sum correct 0.8566228693181818 loss 1.8897972106933594
epoch 51 iter 44 sum correct 0.8575954861111111 loss 1.8897186517715454
epoch 51 iter 45 sum correct 0.8577615489130435 loss 1.8897043466567993
epoch 51 iter 46 sum correct 0.8572140957446809 loss 1.889743447303772
epoch 51 iter 47 sum correct 0.8571370442708334 loss 1.889746069908142
epoch 51 iter 48 sum correct 0.8575414540816326 loss 1.8897162675857544
epoch 51 iter 49 sum correct 0.85796875 loss 1.889681339263916
epoch 51 iter 50 sum correct 0.858609068627451 loss 1.8896251916885376
epoch 51 iter 51 sum correct 0.8586613581730769 loss 1.8896206617355347
epoch 51 iter 52 sum correct 0.8592644457547169 loss 1.889571189880371
epoch 51 iter 53 sum correct 0.8597728587962963 loss 1.8895306587219238
epoch 51 iter 54 sum correct 0.8602272727272727 loss 1.8894972801208496
epoch 51 iter 55 sum correct 0.8605259486607143 loss 1.8894715309143066
epoch 51 iter 56 sum correct 0.846422697368421 loss 1.8895721435546875
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.69it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.34it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.82it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.07it/s]8it [00:01,  6.04it/s]                                 macro  0.5328424611050087
micro  0.6305377542490944
[[251   0  54  21  69  19  53]
 [ 17   0  18   2  11   0   8]
 [ 63   0 220   9  99  40  65]
 [ 26   0  28 712  23  23  83]
 [ 70   0  61  19 352  15 136]
 [ 11   0  28  11  10 339  16]
 [ 54   0  31  36  88   9 389]]
              precision    recall  f1-score   support

           0       0.51      0.54      0.52       467
           1       0.00      0.00      0.00        56
           2       0.50      0.44      0.47       496
           3       0.88      0.80      0.84       895
           4       0.54      0.54      0.54       653
           5       0.76      0.82      0.79       415
           6       0.52      0.64      0.57       607

    accuracy                           0.63      3589
   macro avg       0.53      0.54      0.53      3589
weighted avg       0.63      0.63      0.63      3589

correct 0.6305377542490944 
f1 0.5328424611050087 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.89it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.64it/s] 71%|███████▏  | 5/7.009765625 [00:00<00:00,  6.18it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.35it/s]8it [00:01,  6.31it/s]                                 macro  0.5320067004355036
micro  0.6310950125383115
[[258   0  85  16  63  11  58]
 [ 22   0  12   5   7   2   7]
 [ 65   0 233  13 107  47  63]
 [ 19   0  16 722  30  26  66]
 [ 73   0  51  13 306   8 143]
 [  6   0  31  14  16 323  26]
 [ 48   0  31  14  98  12 423]]
              precision    recall  f1-score   support

           0       0.53      0.53      0.53       491
           1       0.00      0.00      0.00        55
           2       0.51      0.44      0.47       528
           3       0.91      0.82      0.86       879
           4       0.49      0.52      0.50       594
           5       0.75      0.78      0.76       416
           6       0.54      0.68      0.60       626

    accuracy                           0.63      3589
   macro avg       0.53      0.54      0.53      3589
weighted avg       0.63      0.63      0.63      3589

correct 0.6310950125383115 
f1 0.5320067004355036 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:39,  1.39it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.32it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.84it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.29it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.49it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.74it/s] 12%|█▏        | 7/56.072265625 [00:02<00:12,  3.79it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.95it/s] 16%|█▌        | 9/56.072265625 [00:02<00:11,  3.94it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.03it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.98it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.08it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  4.01it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.10it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.03it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.11it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.03it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.10it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  3.97it/s] 36%|███▌      | 20/56.072265625 [00:05<00:09,  4.01it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  3.91it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  3.96it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  3.87it/s] 43%|████▎     | 24/56.072265625 [00:06<00:08,  3.94it/s] 45%|████▍     | 25/56.072265625 [00:06<00:08,  3.87it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  3.92it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  3.85it/s] 50%|████▉     | 28/56.072265625 [00:07<00:07,  3.93it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  3.94it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.04it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  3.89it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:06,  3.95it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  3.87it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  3.94it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  3.89it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:05,  3.96it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  3.88it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  3.98it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  3.90it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:04,  4.01it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  3.89it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.01it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  3.88it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:03,  3.95it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  3.90it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.01it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  3.99it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.08it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.04it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.11it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  4.01it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.10it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.05it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.13it/s] 98%|█████████▊| 55/56.072265625 [00:14<00:00,  4.09it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.14it/s]57it [00:14,  4.96it/s]                                  57it [00:14,  3.90it/s]epoch 52 iter 0 sum correct 0.861328125 loss 1.8893429040908813
epoch 52 iter 1 sum correct 0.8701171875 loss 1.8886971473693848
epoch 52 iter 2 sum correct 0.8828125 loss 1.8875970840454102
epoch 52 iter 3 sum correct 0.8779296875 loss 1.8880088329315186
epoch 52 iter 4 sum correct 0.87109375 loss 1.8885902166366577
epoch 52 iter 5 sum correct 0.8675130208333334 loss 1.888883352279663
epoch 52 iter 6 sum correct 0.8607700892857143 loss 1.8894236087799072
epoch 52 iter 7 sum correct 0.858642578125 loss 1.8895652294158936
epoch 52 iter 8 sum correct 0.8567708333333334 loss 1.8897167444229126
epoch 52 iter 9 sum correct 0.8552734375 loss 1.8898346424102783
epoch 52 iter 10 sum correct 0.8579545454545454 loss 1.889626145362854
epoch 52 iter 11 sum correct 0.8585611979166666 loss 1.8895972967147827
epoch 52 iter 12 sum correct 0.8604266826923077 loss 1.8894778490066528
epoch 52 iter 13 sum correct 0.8604910714285714 loss 1.8894875049591064
epoch 52 iter 14 sum correct 0.861328125 loss 1.889428973197937
epoch 52 iter 15 sum correct 0.861572265625 loss 1.889397382736206
epoch 52 iter 16 sum correct 0.8622472426470589 loss 1.8893450498580933
epoch 52 iter 17 sum correct 0.8618706597222222 loss 1.8893685340881348
epoch 52 iter 18 sum correct 0.86328125 loss 1.8892508745193481
epoch 52 iter 19 sum correct 0.8634765625 loss 1.8892303705215454
epoch 52 iter 20 sum correct 0.8630952380952381 loss 1.8892627954483032
epoch 52 iter 21 sum correct 0.8631924715909091 loss 1.8892512321472168
epoch 52 iter 22 sum correct 0.862686820652174 loss 1.8892854452133179
epoch 52 iter 23 sum correct 0.8631998697916666 loss 1.8892405033111572
epoch 52 iter 24 sum correct 0.86296875 loss 1.8892593383789062
epoch 52 iter 25 sum correct 0.8630558894230769 loss 1.8892565965652466
epoch 52 iter 26 sum correct 0.8636429398148148 loss 1.889204978942871
epoch 52 iter 27 sum correct 0.86474609375 loss 1.889115333557129
epoch 52 iter 28 sum correct 0.8650996767241379 loss 1.8890831470489502
epoch 52 iter 29 sum correct 0.866015625 loss 1.8890126943588257
epoch 52 iter 30 sum correct 0.8661164314516129 loss 1.8890026807785034
epoch 52 iter 31 sum correct 0.86572265625 loss 1.8890339136123657
epoch 52 iter 32 sum correct 0.8654711174242424 loss 1.8890458345413208
epoch 52 iter 33 sum correct 0.8656364889705882 loss 1.8890293836593628
epoch 52 iter 34 sum correct 0.866015625 loss 1.8889944553375244
epoch 52 iter 35 sum correct 0.8658311631944444 loss 1.889009952545166
epoch 52 iter 36 sum correct 0.8662901182432432 loss 1.888974666595459
epoch 52 iter 37 sum correct 0.866930509868421 loss 1.8889232873916626
epoch 52 iter 38 sum correct 0.8671875 loss 1.8888986110687256
epoch 52 iter 39 sum correct 0.8671875 loss 1.8888992071151733
epoch 52 iter 40 sum correct 0.8673780487804879 loss 1.8888815641403198
epoch 52 iter 41 sum correct 0.8678850446428571 loss 1.8888388872146606
epoch 52 iter 42 sum correct 0.8684593023255814 loss 1.8887873888015747
epoch 52 iter 43 sum correct 0.8682528409090909 loss 1.8888038396835327
epoch 52 iter 44 sum correct 0.8685763888888889 loss 1.8887763023376465
epoch 52 iter 45 sum correct 0.8681640625 loss 1.8888070583343506
epoch 52 iter 46 sum correct 0.8678523936170213 loss 1.8888295888900757
epoch 52 iter 47 sum correct 0.8678385416666666 loss 1.888829231262207
epoch 52 iter 48 sum correct 0.8679448341836735 loss 1.8888157606124878
epoch 52 iter 49 sum correct 0.867890625 loss 1.8888206481933594
epoch 52 iter 50 sum correct 0.8684895833333334 loss 1.8887778520584106
epoch 52 iter 51 sum correct 0.8687650240384616 loss 1.8887521028518677
epoch 52 iter 52 sum correct 0.8694722877358491 loss 1.8886960744857788
epoch 52 iter 53 sum correct 0.8698640046296297 loss 1.8886650800704956
epoch 52 iter 54 sum correct 0.8699928977272727 loss 1.888654351234436
epoch 52 iter 55 sum correct 0.8701171875 loss 1.8886427879333496
epoch 52 iter 56 sum correct 0.8559827302631579 loss 1.888620376586914
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.66it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.33it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.69it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.90it/s]8it [00:01,  5.82it/s]                                 macro  0.5416647540551862
micro  0.6375034828643076
[[259   0  63  23  74   9  39]
 [ 23   0  14   4  11   0   4]
 [ 49   0 242  11 124  36  34]
 [ 27   0  29 721  64  17  37]
 [ 48   0  73  18 411  10  93]
 [ 11   0  41  15  14 329   5]
 [ 38   0  56  43 133  11 326]]
              precision    recall  f1-score   support

           0       0.57      0.55      0.56       467
           1       0.00      0.00      0.00        56
           2       0.47      0.49      0.48       496
           3       0.86      0.81      0.83       895
           4       0.49      0.63      0.55       653
           5       0.80      0.79      0.80       415
           6       0.61      0.54      0.57       607

    accuracy                           0.64      3589
   macro avg       0.54      0.54      0.54      3589
weighted avg       0.64      0.64      0.64      3589

correct 0.6375034828643076 
f1 0.5416647540551862 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.74it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.40it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.88it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.14it/s]8it [00:01,  6.10it/s]                                 macro  0.5440394063448873
micro  0.641961549178044
[[253   0  83  21  83   8  43]
 [ 24   0  16   2   9   2   2]
 [ 48   0 262  18 124  41  35]
 [ 20   0  22 737  63  18  19]
 [ 50   0  69  18 362   2  93]
 [  8   0  44  16  17 322   9]
 [ 43   0  40  38 120  17 368]]
              precision    recall  f1-score   support

           0       0.57      0.52      0.54       491
           1       0.00      0.00      0.00        55
           2       0.49      0.50      0.49       528
           3       0.87      0.84      0.85       879
           4       0.47      0.61      0.53       594
           5       0.79      0.77      0.78       416
           6       0.65      0.59      0.62       626

    accuracy                           0.64      3589
   macro avg       0.55      0.55      0.54      3589
weighted avg       0.64      0.64      0.64      3589

correct 0.641961549178044 
f1 0.5440394063448873 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:39,  1.39it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.31it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.82it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.27it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.42it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.68it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.75it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.92it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.91it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.04it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  4.00it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.10it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  4.06it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.14it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.07it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.14it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.03it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.10it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.04it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.12it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.06it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.14it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.08it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.16it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.10it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  3.93it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  3.94it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.04it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.00it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.09it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.03it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.11it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.07it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.14it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.09it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.16it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.10it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.16it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  4.10it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.15it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.09it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.17it/s] 77%|███████▋  | 43/56.072265625 [00:10<00:03,  4.10it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.17it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.10it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.16it/s] 84%|████████▍ | 47/56.072265625 [00:11<00:02,  4.09it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.09it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  3.98it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.07it/s] 91%|█████████ | 51/56.072265625 [00:12<00:01,  4.02it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.10it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.06it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.13it/s] 98%|█████████▊| 55/56.072265625 [00:13<00:00,  4.08it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.14it/s]57it [00:14,  4.94it/s]                                  57it [00:14,  3.97it/s]epoch 53 iter 0 sum correct 0.880859375 loss 1.8877789974212646
epoch 53 iter 1 sum correct 0.8857421875 loss 1.8875457048416138
epoch 53 iter 2 sum correct 0.890625 loss 1.8871357440948486
epoch 53 iter 3 sum correct 0.88525390625 loss 1.8875277042388916
epoch 53 iter 4 sum correct 0.8859375 loss 1.8874412775039673
epoch 53 iter 5 sum correct 0.8831380208333334 loss 1.8876476287841797
epoch 53 iter 6 sum correct 0.8830915178571429 loss 1.8876492977142334
epoch 53 iter 7 sum correct 0.881591796875 loss 1.8877463340759277
epoch 53 iter 8 sum correct 0.8799913194444444 loss 1.8878729343414307
epoch 53 iter 9 sum correct 0.878125 loss 1.8880119323730469
epoch 53 iter 10 sum correct 0.8773082386363636 loss 1.8880927562713623
epoch 53 iter 11 sum correct 0.8782552083333334 loss 1.8880109786987305
epoch 53 iter 12 sum correct 0.8802584134615384 loss 1.8878480195999146
epoch 53 iter 13 sum correct 0.8815569196428571 loss 1.8877387046813965
epoch 53 iter 14 sum correct 0.8815104166666666 loss 1.8877426385879517
epoch 53 iter 15 sum correct 0.8824462890625 loss 1.8876651525497437
epoch 53 iter 16 sum correct 0.8823529411764706 loss 1.887676477432251
epoch 53 iter 17 sum correct 0.8831380208333334 loss 1.8876181840896606
epoch 53 iter 18 sum correct 0.8824013157894737 loss 1.8876800537109375
epoch 53 iter 19 sum correct 0.88125 loss 1.8877884149551392
epoch 53 iter 20 sum correct 0.8803943452380952 loss 1.887853741645813
epoch 53 iter 21 sum correct 0.880859375 loss 1.887814998626709
epoch 53 iter 22 sum correct 0.8806895380434783 loss 1.8878275156021118
epoch 53 iter 23 sum correct 0.8807779947916666 loss 1.8878148794174194
epoch 53 iter 24 sum correct 0.881640625 loss 1.8877493143081665
epoch 53 iter 25 sum correct 0.8814603365384616 loss 1.88776433467865
epoch 53 iter 26 sum correct 0.8813657407407407 loss 1.8877700567245483
epoch 53 iter 27 sum correct 0.8812779017857143 loss 1.8877836465835571
epoch 53 iter 28 sum correct 0.8824757543103449 loss 1.887691855430603
epoch 53 iter 29 sum correct 0.8826822916666667 loss 1.8876762390136719
epoch 53 iter 30 sum correct 0.8825604838709677 loss 1.8876843452453613
epoch 53 iter 31 sum correct 0.88275146484375 loss 1.887662410736084
epoch 53 iter 32 sum correct 0.8826349431818182 loss 1.8876760005950928
epoch 53 iter 33 sum correct 0.8825252757352942 loss 1.8876816034317017
epoch 53 iter 34 sum correct 0.8831473214285714 loss 1.887633204460144
epoch 53 iter 35 sum correct 0.8837348090277778 loss 1.8875890970230103
epoch 53 iter 36 sum correct 0.8832347972972973 loss 1.887630820274353
epoch 53 iter 37 sum correct 0.8828638980263158 loss 1.8876557350158691
epoch 53 iter 38 sum correct 0.8824619391025641 loss 1.8876827955245972
epoch 53 iter 39 sum correct 0.88203125 loss 1.8877136707305908
epoch 53 iter 40 sum correct 0.8820026676829268 loss 1.8877170085906982
epoch 53 iter 41 sum correct 0.8822544642857143 loss 1.8876934051513672
epoch 53 iter 42 sum correct 0.8822220203488372 loss 1.887695074081421
epoch 53 iter 43 sum correct 0.8824129971590909 loss 1.887679100036621
epoch 53 iter 44 sum correct 0.8831597222222223 loss 1.8876246213912964
epoch 53 iter 45 sum correct 0.8832370923913043 loss 1.8876137733459473
epoch 53 iter 46 sum correct 0.8834773936170213 loss 1.8875950574874878
epoch 53 iter 47 sum correct 0.8834228515625 loss 1.8876004219055176
epoch 53 iter 48 sum correct 0.8837292729591837 loss 1.8875724077224731
epoch 53 iter 49 sum correct 0.8841015625 loss 1.88754141330719
epoch 53 iter 50 sum correct 0.8841145833333334 loss 1.8875397443771362
epoch 53 iter 51 sum correct 0.8841271033653846 loss 1.8875422477722168
epoch 53 iter 52 sum correct 0.8836969339622641 loss 1.887575387954712
epoch 53 iter 53 sum correct 0.8834273726851852 loss 1.8875951766967773
epoch 53 iter 54 sum correct 0.8838423295454545 loss 1.8875601291656494
epoch 53 iter 55 sum correct 0.8841378348214286 loss 1.887536644935608
epoch 53 iter 56 sum correct 0.8696546052631579 loss 1.8876336812973022
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.70it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.33it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.69it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.00it/s]8it [00:01,  5.82it/s]                                 macro  0.5337765694083512
micro  0.6327667874059627
[[269   0  46  30  65  12  45]
 [ 19   0  17   5  13   0   2]
 [ 57   0 226  18 101  41  53]
 [ 34   0  13 752  17  21  58]
 [ 73   0  58  43 325  12 142]
 [ 15   0  29  17   6 336  12]
 [ 56   0  38  54  88   8 363]]
              precision    recall  f1-score   support

           0       0.51      0.58      0.54       467
           1       0.00      0.00      0.00        56
           2       0.53      0.46      0.49       496
           3       0.82      0.84      0.83       895
           4       0.53      0.50      0.51       653
           5       0.78      0.81      0.80       415
           6       0.54      0.60      0.57       607

    accuracy                           0.63      3589
   macro avg       0.53      0.54      0.53      3589
weighted avg       0.62      0.63      0.63      3589

correct 0.6327667874059627 
f1 0.5337765694083512 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.72it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.32it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.59it/s] 86%|████████▌ | 6/7.009765625 [00:01<00:00,  6.34it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.64it/s]8it [00:01,  5.75it/s]                                 macro  0.5407387965846173
micro  0.6450264697687378
[[294   0  57  23  61   9  47]
 [ 24   0  11   4  14   1   1]
 [ 66   0 225  29 104  51  53]
 [ 30   0  11 769  23  18  28]
 [ 73   0  58  40 303  14 106]
 [  8   0  41  20   7 324  16]
 [ 64   0  40  38  73  11 400]]
              precision    recall  f1-score   support

           0       0.53      0.60      0.56       491
           1       0.00      0.00      0.00        55
           2       0.51      0.43      0.46       528
           3       0.83      0.87      0.85       879
           4       0.52      0.51      0.51       594
           5       0.76      0.78      0.77       416
           6       0.61      0.64      0.63       626

    accuracy                           0.65      3589
   macro avg       0.54      0.55      0.54      3589
weighted avg       0.63      0.65      0.64      3589

correct 0.6450264697687378 
f1 0.5407387965846173 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:39,  1.40it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.32it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.81it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.26it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.44it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.70it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.75it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.92it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.89it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.01it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.96it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.06it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  4.01it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.09it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.03it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.08it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.00it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.09it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.03it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.10it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.03it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.11it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.02it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.10it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.03it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.11it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.01it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.10it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.02it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.10it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.02it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.11it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.05it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.12it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.03it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.11it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.03it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.11it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  4.02it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.11it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.01it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.09it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  4.02it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.10it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.02it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.10it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  4.02it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.10it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.04it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.12it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  4.05it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.13it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.06it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.13it/s] 98%|█████████▊| 55/56.072265625 [00:13<00:00,  4.06it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.13it/s]57it [00:14,  5.00it/s]                                  57it [00:14,  3.96it/s]epoch 54 iter 0 sum correct 0.884765625 loss 1.8873721361160278
epoch 54 iter 1 sum correct 0.87890625 loss 1.887800693511963
epoch 54 iter 2 sum correct 0.875 loss 1.8880800008773804
epoch 54 iter 3 sum correct 0.8681640625 loss 1.8886702060699463
epoch 54 iter 4 sum correct 0.8640625 loss 1.8889961242675781
epoch 54 iter 5 sum correct 0.8655598958333334 loss 1.8888583183288574
epoch 54 iter 6 sum correct 0.8630022321428571 loss 1.8890838623046875
epoch 54 iter 7 sum correct 0.863525390625 loss 1.8890559673309326
epoch 54 iter 8 sum correct 0.8648003472222222 loss 1.8889906406402588
epoch 54 iter 9 sum correct 0.8677734375 loss 1.8887676000595093
epoch 54 iter 10 sum correct 0.8705610795454546 loss 1.8885539770126343
epoch 54 iter 11 sum correct 0.8699544270833334 loss 1.8886117935180664
epoch 54 iter 12 sum correct 0.8700420673076923 loss 1.8886383771896362
epoch 54 iter 13 sum correct 0.8709542410714286 loss 1.8885561227798462
epoch 54 iter 14 sum correct 0.8713541666666667 loss 1.8885458707809448
epoch 54 iter 15 sum correct 0.8719482421875 loss 1.888494849205017
epoch 54 iter 16 sum correct 0.8721277573529411 loss 1.8884851932525635
epoch 54 iter 17 sum correct 0.87109375 loss 1.8885633945465088
epoch 54 iter 18 sum correct 0.8722245065789473 loss 1.8884776830673218
epoch 54 iter 19 sum correct 0.872265625 loss 1.888474464416504
epoch 54 iter 20 sum correct 0.873046875 loss 1.888417363166809
epoch 54 iter 21 sum correct 0.8726917613636364 loss 1.8884496688842773
epoch 54 iter 22 sum correct 0.8737262228260869 loss 1.8883742094039917
epoch 54 iter 23 sum correct 0.8741861979166666 loss 1.8883264064788818
epoch 54 iter 24 sum correct 0.87375 loss 1.8883639574050903
epoch 54 iter 25 sum correct 0.8732722355769231 loss 1.8884094953536987
epoch 54 iter 26 sum correct 0.8731192129629629 loss 1.8884178400039673
epoch 54 iter 27 sum correct 0.8744419642857143 loss 1.8883094787597656
epoch 54 iter 28 sum correct 0.874932650862069 loss 1.8882728815078735
epoch 54 iter 29 sum correct 0.8751953125 loss 1.8882558345794678
epoch 54 iter 30 sum correct 0.8748739919354839 loss 1.8882783651351929
epoch 54 iter 31 sum correct 0.87603759765625 loss 1.8881847858428955
epoch 54 iter 32 sum correct 0.8767755681818182 loss 1.8881285190582275
epoch 54 iter 33 sum correct 0.8767233455882353 loss 1.8881322145462036
epoch 54 iter 34 sum correct 0.87734375 loss 1.8880789279937744
epoch 54 iter 35 sum correct 0.8772243923611112 loss 1.8880879878997803
epoch 54 iter 36 sum correct 0.8773754222972973 loss 1.8880735635757446
epoch 54 iter 37 sum correct 0.8778782894736842 loss 1.888036847114563
epoch 54 iter 38 sum correct 0.8781550480769231 loss 1.8880141973495483
epoch 54 iter 39 sum correct 0.877197265625 loss 1.8880894184112549
epoch 54 iter 40 sum correct 0.8776676829268293 loss 1.8880499601364136
epoch 54 iter 41 sum correct 0.8777436755952381 loss 1.8880420923233032
epoch 54 iter 42 sum correct 0.877906976744186 loss 1.8880287408828735
epoch 54 iter 43 sum correct 0.8777965198863636 loss 1.888031005859375
epoch 54 iter 44 sum correct 0.8777777777777778 loss 1.8880293369293213
epoch 54 iter 45 sum correct 0.8780146059782609 loss 1.8880109786987305
epoch 54 iter 46 sum correct 0.8778257978723404 loss 1.8880250453948975
epoch 54 iter 47 sum correct 0.8784586588541666 loss 1.8879746198654175
epoch 54 iter 48 sum correct 0.8784279336734694 loss 1.8879780769348145
epoch 54 iter 49 sum correct 0.87828125 loss 1.887988567352295
epoch 54 iter 50 sum correct 0.8784849877450981 loss 1.8879667520523071
epoch 54 iter 51 sum correct 0.8779672475961539 loss 1.8880057334899902
epoch 54 iter 52 sum correct 0.8783166273584906 loss 1.88798189163208
epoch 54 iter 53 sum correct 0.8787977430555556 loss 1.8879388570785522
epoch 54 iter 54 sum correct 0.8788707386363637 loss 1.8879317045211792
epoch 54 iter 55 sum correct 0.8790806361607143 loss 1.8879151344299316
epoch 54 iter 56 sum correct 0.8646518640350878 loss 1.8880387544631958
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.64it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.27it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.77it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.06it/s]8it [00:01,  5.93it/s]                                 macro  0.5392647306257811
micro  0.6422401783226526
[[267   0  54  30  41  20  55]
 [ 23   0  17   6   5   1   4]
 [ 51   0 236  15  82  48  64]
 [ 21   0  16 768  19  22  49]
 [ 68   0  76  37 308  21 143]
 [  9   0  23  15   4 352  12]
 [ 43   0  43  52  80  15 374]]
              precision    recall  f1-score   support

           0       0.55      0.57      0.56       467
           1       0.00      0.00      0.00        56
           2       0.51      0.48      0.49       496
           3       0.83      0.86      0.84       895
           4       0.57      0.47      0.52       653
           5       0.73      0.85      0.79       415
           6       0.53      0.62      0.57       607

    accuracy                           0.64      3589
   macro avg       0.53      0.55      0.54      3589
weighted avg       0.63      0.64      0.63      3589

correct 0.6422401783226526 
f1 0.5392647306257811 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.71it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.40it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.63it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.83it/s]8it [00:01,  5.90it/s]                                 macro  0.5404120823015461
micro  0.6486486486486487
[[271   0  69  25  51  14  61]
 [ 32   0  11   3   3   2   4]
 [ 62   0 238  35  75  61  57]
 [ 18   0   9 784  16  22  30]
 [ 64   0  64  40 274  14 138]
 [  9   0  26  22  10 338  11]
 [ 39   0  39  39  64  22 423]]
              precision    recall  f1-score   support

           0       0.55      0.55      0.55       491
           1       0.00      0.00      0.00        55
           2       0.52      0.45      0.48       528
           3       0.83      0.89      0.86       879
           4       0.56      0.46      0.50       594
           5       0.71      0.81      0.76       416
           6       0.58      0.68      0.63       626

    accuracy                           0.65      3589
   macro avg       0.54      0.55      0.54      3589
weighted avg       0.63      0.65      0.64      3589

correct 0.6486486486486487 
f1 0.5404120823015461 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:37,  1.48it/s]  4%|▎         | 2/56.072265625 [00:00<00:22,  2.39it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.88it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.29it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.46it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.70it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.76it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.92it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.91it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  3.99it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.92it/s] 21%|██▏       | 12/56.072265625 [00:03<00:11,  3.98it/s] 23%|██▎       | 13/56.072265625 [00:03<00:11,  3.90it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.01it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  3.99it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.08it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.01it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.04it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.01it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.10it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.05it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.04it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.01it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.10it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.05it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.13it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.07it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.14it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.06it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.05it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  3.96it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.06it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.02it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.03it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  3.92it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:05,  3.98it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  3.92it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  3.97it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  3.88it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:04,  3.94it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  3.89it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  3.95it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  3.87it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:03,  3.99it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  3.95it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  3.98it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  3.89it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:02,  4.01it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  3.96it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.06it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  4.04it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:01,  3.94it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  3.94it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.05it/s] 98%|█████████▊| 55/56.072265625 [00:14<00:00,  4.04it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.12it/s]57it [00:14,  4.96it/s]                                  57it [00:14,  3.91it/s]epoch 55 iter 0 sum correct 0.888671875 loss 1.8870497941970825
epoch 55 iter 1 sum correct 0.8984375 loss 1.886289119720459
epoch 55 iter 2 sum correct 0.8997395833333334 loss 1.886171817779541
epoch 55 iter 3 sum correct 0.89599609375 loss 1.8865227699279785
epoch 55 iter 4 sum correct 0.90078125 loss 1.8861302137374878
epoch 55 iter 5 sum correct 0.9000651041666666 loss 1.886178970336914
epoch 55 iter 6 sum correct 0.8978794642857143 loss 1.8863682746887207
epoch 55 iter 7 sum correct 0.899169921875 loss 1.8862684965133667
epoch 55 iter 8 sum correct 0.8982204861111112 loss 1.8863691091537476
epoch 55 iter 9 sum correct 0.8962890625 loss 1.886526346206665
epoch 55 iter 10 sum correct 0.8954190340909091 loss 1.8866015672683716
epoch 55 iter 11 sum correct 0.8935546875 loss 1.8867614269256592
epoch 55 iter 12 sum correct 0.8927283653846154 loss 1.8868333101272583
epoch 55 iter 13 sum correct 0.8916015625 loss 1.8869054317474365
epoch 55 iter 14 sum correct 0.89140625 loss 1.8869264125823975
epoch 55 iter 15 sum correct 0.8907470703125 loss 1.8869810104370117
epoch 55 iter 16 sum correct 0.8902803308823529 loss 1.8870165348052979
epoch 55 iter 17 sum correct 0.8891059027777778 loss 1.8871173858642578
epoch 55 iter 18 sum correct 0.8884662828947368 loss 1.8871744871139526
epoch 55 iter 19 sum correct 0.887890625 loss 1.8872143030166626
epoch 55 iter 20 sum correct 0.8865327380952381 loss 1.8873162269592285
epoch 55 iter 21 sum correct 0.8866299715909091 loss 1.8873049020767212
epoch 55 iter 22 sum correct 0.8875679347826086 loss 1.8872318267822266
epoch 55 iter 23 sum correct 0.8881022135416666 loss 1.8871941566467285
epoch 55 iter 24 sum correct 0.8884375 loss 1.8871740102767944
epoch 55 iter 25 sum correct 0.8885216346153846 loss 1.8871617317199707
epoch 55 iter 26 sum correct 0.8888888888888888 loss 1.8871285915374756
epoch 55 iter 27 sum correct 0.8875558035714286 loss 1.887232780456543
epoch 55 iter 28 sum correct 0.8878636853448276 loss 1.8872168064117432
epoch 55 iter 29 sum correct 0.8873697916666666 loss 1.887255072593689
epoch 55 iter 30 sum correct 0.8870967741935484 loss 1.8872770071029663
epoch 55 iter 31 sum correct 0.88714599609375 loss 1.8872733116149902
epoch 55 iter 32 sum correct 0.8870738636363636 loss 1.8872824907302856
epoch 55 iter 33 sum correct 0.8870059742647058 loss 1.8872838020324707
epoch 55 iter 34 sum correct 0.8864397321428571 loss 1.8873236179351807
epoch 55 iter 35 sum correct 0.8863932291666666 loss 1.8873339891433716
epoch 55 iter 36 sum correct 0.8862964527027027 loss 1.8873448371887207
epoch 55 iter 37 sum correct 0.8865131578947368 loss 1.8873279094696045
epoch 55 iter 38 sum correct 0.8858173076923077 loss 1.8873809576034546
epoch 55 iter 39 sum correct 0.8861328125 loss 1.8873547315597534
epoch 55 iter 40 sum correct 0.8862423780487805 loss 1.8873447179794312
epoch 55 iter 41 sum correct 0.8860677083333334 loss 1.887360692024231
epoch 55 iter 42 sum correct 0.886219113372093 loss 1.8873436450958252
epoch 55 iter 43 sum correct 0.8865855823863636 loss 1.887313723564148
epoch 55 iter 44 sum correct 0.8865017361111112 loss 1.8873205184936523
epoch 55 iter 45 sum correct 0.8866338315217391 loss 1.8873085975646973
epoch 55 iter 46 sum correct 0.8867603058510638 loss 1.8872946500778198
epoch 55 iter 47 sum correct 0.8868408203125 loss 1.8872857093811035
epoch 55 iter 48 sum correct 0.887077487244898 loss 1.887268304824829
epoch 55 iter 49 sum correct 0.886796875 loss 1.8872900009155273
epoch 55 iter 50 sum correct 0.8871017156862745 loss 1.8872621059417725
epoch 55 iter 51 sum correct 0.8874699519230769 loss 1.8872333765029907
epoch 55 iter 52 sum correct 0.8876768867924528 loss 1.8872151374816895
epoch 55 iter 53 sum correct 0.8881655092592593 loss 1.8871773481369019
epoch 55 iter 54 sum correct 0.8882457386363637 loss 1.8871667385101318
epoch 55 iter 55 sum correct 0.8880789620535714 loss 1.8871808052062988
epoch 55 iter 56 sum correct 0.8735608552631579 loss 1.8872485160827637
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.72it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.39it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.83it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.07it/s]8it [00:01,  6.04it/s]                                 macro  0.542060200116268
micro  0.6402897743103929
[[286   0  42  19  53  16  51]
 [ 28   0  13   1   9   1   4]
 [ 56   0 242  10  97  43  48]
 [ 22   0  21 745  28  12  67]
 [ 90   0  88  23 332  18 102]
 [ 10   0  26  18  11 340  10]
 [ 59   0  35  39 114   7 353]]
              precision    recall  f1-score   support

           0       0.52      0.61      0.56       467
           1       0.00      0.00      0.00        56
           2       0.52      0.49      0.50       496
           3       0.87      0.83      0.85       895
           4       0.52      0.51      0.51       653
           5       0.78      0.82      0.80       415
           6       0.56      0.58      0.57       607

    accuracy                           0.64      3589
   macro avg       0.54      0.55      0.54      3589
weighted avg       0.63      0.64      0.64      3589

correct 0.6402897743103929 
f1 0.542060200116268 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.68it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.34it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.70it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.94it/s]8it [00:01,  5.96it/s]                                 macro  0.5421138339697238
micro  0.6447478406241293
[[282   0  65  20  64  15  45]
 [ 29   0  11   3   7   2   3]
 [ 54   0 262  20  90  50  52]
 [ 30   0  15 762  21  19  32]
 [ 77   0  87  29 290  10 101]
 [ 11   0  29  23  10 325  18]
 [ 68   0  27  24 100  14 393]]
              precision    recall  f1-score   support

           0       0.51      0.57      0.54       491
           1       0.00      0.00      0.00        55
           2       0.53      0.50      0.51       528
           3       0.86      0.87      0.87       879
           4       0.50      0.49      0.49       594
           5       0.75      0.78      0.76       416
           6       0.61      0.63      0.62       626

    accuracy                           0.64      3589
   macro avg       0.54      0.55      0.54      3589
weighted avg       0.64      0.64      0.64      3589

correct 0.6447478406241293 
f1 0.5421138339697238 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:39,  1.40it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.33it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.83it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.28it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.46it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.72it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.77it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.93it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.91it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.03it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.97it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.08it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  4.01it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.10it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.03it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.11it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.04it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.12it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.03it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.11it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.03it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.12it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.03it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.12it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.04it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.12it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.04it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.12it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.04it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.12it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.04it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.11it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.01it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.10it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.00it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.09it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  3.98it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.07it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  3.98it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.08it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  3.99it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.07it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  3.99it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.08it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  3.99it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.08it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  4.00it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.09it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.00it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.09it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  4.03it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.12it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.02it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.11it/s] 98%|█████████▊| 55/56.072265625 [00:14<00:00,  4.03it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.11it/s]57it [00:14,  4.96it/s]                                  57it [00:14,  3.95it/s]epoch 56 iter 0 sum correct 0.873046875 loss 1.8880326747894287
epoch 56 iter 1 sum correct 0.880859375 loss 1.8875430822372437
epoch 56 iter 2 sum correct 0.8912760416666666 loss 1.8867931365966797
epoch 56 iter 3 sum correct 0.89306640625 loss 1.8866939544677734
epoch 56 iter 4 sum correct 0.895703125 loss 1.8864895105361938
epoch 56 iter 5 sum correct 0.8932291666666666 loss 1.8866984844207764
epoch 56 iter 6 sum correct 0.8922991071428571 loss 1.8867716789245605
epoch 56 iter 7 sum correct 0.89501953125 loss 1.8865529298782349
epoch 56 iter 8 sum correct 0.8967013888888888 loss 1.8864290714263916
epoch 56 iter 9 sum correct 0.8953125 loss 1.8865333795547485
epoch 56 iter 10 sum correct 0.8957741477272727 loss 1.8864972591400146
epoch 56 iter 11 sum correct 0.8953450520833334 loss 1.8865225315093994
epoch 56 iter 12 sum correct 0.8958834134615384 loss 1.8864821195602417
epoch 56 iter 13 sum correct 0.8957868303571429 loss 1.886505126953125
epoch 56 iter 14 sum correct 0.8944010416666667 loss 1.8866068124771118
epoch 56 iter 15 sum correct 0.8953857421875 loss 1.8865256309509277
epoch 56 iter 16 sum correct 0.8954503676470589 loss 1.8865302801132202
epoch 56 iter 17 sum correct 0.8963758680555556 loss 1.886464238166809
epoch 56 iter 18 sum correct 0.8951480263157895 loss 1.886566162109375
epoch 56 iter 19 sum correct 0.8955078125 loss 1.8865340948104858
epoch 56 iter 20 sum correct 0.8962053571428571 loss 1.8864766359329224
epoch 56 iter 21 sum correct 0.8967507102272727 loss 1.886432409286499
epoch 56 iter 22 sum correct 0.8969089673913043 loss 1.8864167928695679
epoch 56 iter 23 sum correct 0.8972981770833334 loss 1.8863821029663086
epoch 56 iter 24 sum correct 0.89734375 loss 1.8863681554794312
epoch 56 iter 25 sum correct 0.8979116586538461 loss 1.8863248825073242
epoch 56 iter 26 sum correct 0.8973524305555556 loss 1.8863697052001953
epoch 56 iter 27 sum correct 0.8973911830357143 loss 1.8863712549209595
epoch 56 iter 28 sum correct 0.8983028017241379 loss 1.8862987756729126
epoch 56 iter 29 sum correct 0.8981770833333333 loss 1.8863048553466797
epoch 56 iter 30 sum correct 0.8981224798387096 loss 1.8863024711608887
epoch 56 iter 31 sum correct 0.8975830078125 loss 1.886344075202942
epoch 56 iter 32 sum correct 0.8977272727272727 loss 1.8863370418548584
epoch 56 iter 33 sum correct 0.8975183823529411 loss 1.8863534927368164
epoch 56 iter 34 sum correct 0.8975446428571429 loss 1.8863459825515747
epoch 56 iter 35 sum correct 0.8974066840277778 loss 1.886355996131897
epoch 56 iter 36 sum correct 0.8973289695945946 loss 1.8863624334335327
epoch 56 iter 37 sum correct 0.8970497532894737 loss 1.8863804340362549
epoch 56 iter 38 sum correct 0.8968850160256411 loss 1.8863937854766846
epoch 56 iter 39 sum correct 0.896435546875 loss 1.8864332437515259
epoch 56 iter 40 sum correct 0.8963891006097561 loss 1.8864349126815796
epoch 56 iter 41 sum correct 0.8966238839285714 loss 1.8864150047302246
epoch 56 iter 42 sum correct 0.8966206395348837 loss 1.886419653892517
epoch 56 iter 43 sum correct 0.8967507102272727 loss 1.8864092826843262
epoch 56 iter 44 sum correct 0.8966579861111111 loss 1.886417031288147
epoch 56 iter 45 sum correct 0.8968240489130435 loss 1.8864033222198486
epoch 56 iter 46 sum correct 0.8968583776595744 loss 1.8863998651504517
epoch 56 iter 47 sum correct 0.8968098958333334 loss 1.8864021301269531
epoch 56 iter 48 sum correct 0.8967633928571429 loss 1.8864059448242188
epoch 56 iter 49 sum correct 0.896875 loss 1.8863977193832397
epoch 56 iter 50 sum correct 0.8971354166666666 loss 1.8863791227340698
epoch 56 iter 51 sum correct 0.8969350961538461 loss 1.886399269104004
epoch 56 iter 52 sum correct 0.8965212264150944 loss 1.8864320516586304
epoch 56 iter 53 sum correct 0.8965567129629629 loss 1.8864282369613647
epoch 56 iter 54 sum correct 0.8959872159090909 loss 1.8864744901657104
epoch 56 iter 55 sum correct 0.8961356026785714 loss 1.8864617347717285
epoch 56 iter 56 sum correct 0.8816132127192983 loss 1.8864225149154663
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.67it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.32it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.76it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.78it/s]8it [00:01,  5.80it/s]                                 macro  0.5338479568221325
micro  0.6274728336584007
[[287   0  59  13  47  11  50]
 [ 20   0  20   3   8   0   5]
 [ 57   0 264  10  82  42  41]
 [ 39   0  34 709  23  17  73]
 [ 96   0 107  15 306  13 116]
 [ 15   0  38  15   7 331   9]
 [ 57   0  63  41  82   9 355]]
              precision    recall  f1-score   support

           0       0.50      0.61      0.55       467
           1       0.00      0.00      0.00        56
           2       0.45      0.53      0.49       496
           3       0.88      0.79      0.83       895
           4       0.55      0.47      0.51       653
           5       0.78      0.80      0.79       415
           6       0.55      0.58      0.57       607

    accuracy                           0.63      3589
   macro avg       0.53      0.54      0.53      3589
weighted avg       0.63      0.63      0.63      3589

correct 0.6274728336584007 
f1 0.5338479568221325 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.77it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.42it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.76it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.81it/s]8it [00:01,  5.86it/s]                                 macro  0.5444738509224775
micro  0.6411256617442185
[[286   0  79  11  54  11  50]
 [ 24   0  18   2   8   1   2]
 [ 62   0 285  11  85  41  44]
 [ 43   0  28 708  31  21  48]
 [ 72   0  92  17 293   7 113]
 [ 10   0  49  11   9 321  16]
 [ 59   0  52  18  76  13 408]]
              precision    recall  f1-score   support

           0       0.51      0.58      0.55       491
           1       0.00      0.00      0.00        55
           2       0.47      0.54      0.50       528
           3       0.91      0.81      0.85       879
           4       0.53      0.49      0.51       594
           5       0.77      0.77      0.77       416
           6       0.60      0.65      0.62       626

    accuracy                           0.64      3589
   macro avg       0.54      0.55      0.54      3589
weighted avg       0.64      0.64      0.64      3589

correct 0.6411256617442185 
f1 0.5444738509224775 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:38,  1.41it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.34it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.82it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.27it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.46it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.71it/s] 12%|█▏        | 7/56.072265625 [00:02<00:12,  3.78it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.94it/s] 16%|█▌        | 9/56.072265625 [00:02<00:11,  3.93it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.05it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  4.00it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.10it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  4.02it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.11it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.04it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.12it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.04it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.12it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.03it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.11it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.03it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.12it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.03it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.11it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.02it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.10it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  3.99it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.08it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.00it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.09it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.00it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.08it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  3.98it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.07it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  3.99it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.09it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.01it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.09it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  4.00it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.08it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  3.99it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.08it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  3.98it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.08it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  3.99it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.08it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  4.00it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.09it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.01it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.10it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  4.04it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.13it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.03it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.11it/s] 98%|█████████▊| 55/56.072265625 [00:14<00:00,  4.02it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.11it/s]57it [00:14,  4.96it/s]                                  57it [00:14,  3.95it/s]epoch 57 iter 0 sum correct 0.90234375 loss 1.8859888315200806
epoch 57 iter 1 sum correct 0.90625 loss 1.8856993913650513
epoch 57 iter 2 sum correct 0.90234375 loss 1.8859790563583374
epoch 57 iter 3 sum correct 0.8984375 loss 1.8862723112106323
epoch 57 iter 4 sum correct 0.9 loss 1.886155366897583
epoch 57 iter 5 sum correct 0.8984375 loss 1.8862800598144531
epoch 57 iter 6 sum correct 0.8959263392857143 loss 1.886461615562439
epoch 57 iter 7 sum correct 0.89453125 loss 1.8865759372711182
epoch 57 iter 8 sum correct 0.8932291666666666 loss 1.8867005109786987
epoch 57 iter 9 sum correct 0.88984375 loss 1.8869999647140503
epoch 57 iter 10 sum correct 0.8870738636363636 loss 1.8872053623199463
epoch 57 iter 11 sum correct 0.8860677083333334 loss 1.887284517288208
epoch 57 iter 12 sum correct 0.8858173076923077 loss 1.887292504310608
epoch 57 iter 13 sum correct 0.8837890625 loss 1.8874523639678955
epoch 57 iter 14 sum correct 0.8828125 loss 1.887542486190796
epoch 57 iter 15 sum correct 0.883056640625 loss 1.8875229358673096
epoch 57 iter 16 sum correct 0.8830422794117647 loss 1.8875361680984497
epoch 57 iter 17 sum correct 0.8837890625 loss 1.8874861001968384
epoch 57 iter 18 sum correct 0.8827097039473685 loss 1.8875670433044434
epoch 57 iter 19 sum correct 0.88369140625 loss 1.887481689453125
epoch 57 iter 20 sum correct 0.8827194940476191 loss 1.887557864189148
epoch 57 iter 21 sum correct 0.8828125 loss 1.8875430822372437
epoch 57 iter 22 sum correct 0.8823879076086957 loss 1.887583613395691
epoch 57 iter 23 sum correct 0.8821614583333334 loss 1.8876078128814697
epoch 57 iter 24 sum correct 0.882890625 loss 1.887559175491333
epoch 57 iter 25 sum correct 0.8827373798076923 loss 1.8875714540481567
epoch 57 iter 26 sum correct 0.8836082175925926 loss 1.8875036239624023
epoch 57 iter 27 sum correct 0.8828125 loss 1.8875665664672852
epoch 57 iter 28 sum correct 0.882745150862069 loss 1.8875722885131836
epoch 57 iter 29 sum correct 0.8827473958333333 loss 1.8875675201416016
epoch 57 iter 30 sum correct 0.8824974798387096 loss 1.8875852823257446
epoch 57 iter 31 sum correct 0.8826904296875 loss 1.887575626373291
epoch 57 iter 32 sum correct 0.8828716856060606 loss 1.8875590562820435
epoch 57 iter 33 sum correct 0.8833869485294118 loss 1.8875164985656738
epoch 57 iter 34 sum correct 0.8834821428571429 loss 1.887518286705017
epoch 57 iter 35 sum correct 0.8835720486111112 loss 1.887514352798462
epoch 57 iter 36 sum correct 0.8838682432432432 loss 1.8874945640563965
epoch 57 iter 37 sum correct 0.8842002467105263 loss 1.887468695640564
epoch 57 iter 38 sum correct 0.883964342948718 loss 1.8874871730804443
epoch 57 iter 39 sum correct 0.8841796875 loss 1.8874777555465698
epoch 57 iter 40 sum correct 0.8845274390243902 loss 1.8874472379684448
epoch 57 iter 41 sum correct 0.8846726190476191 loss 1.887435793876648
epoch 57 iter 42 sum correct 0.8852198401162791 loss 1.8873991966247559
epoch 57 iter 43 sum correct 0.8856090198863636 loss 1.8873698711395264
epoch 57 iter 44 sum correct 0.8859809027777777 loss 1.8873401880264282
epoch 57 iter 45 sum correct 0.8864215353260869 loss 1.8873056173324585
epoch 57 iter 46 sum correct 0.8861785239361702 loss 1.8873165845870972
epoch 57 iter 47 sum correct 0.8865966796875 loss 1.887282371520996
epoch 57 iter 48 sum correct 0.8868781887755102 loss 1.887261986732483
epoch 57 iter 49 sum correct 0.88671875 loss 1.8872742652893066
epoch 57 iter 50 sum correct 0.8866804534313726 loss 1.887275218963623
epoch 57 iter 51 sum correct 0.8864182692307693 loss 1.8872978687286377
epoch 57 iter 52 sum correct 0.8868661556603774 loss 1.8872617483139038
epoch 57 iter 53 sum correct 0.8870442708333334 loss 1.887245774269104
epoch 57 iter 54 sum correct 0.8865767045454546 loss 1.8872793912887573
epoch 57 iter 55 sum correct 0.885986328125 loss 1.8873300552368164
epoch 57 iter 56 sum correct 0.8715734649122807 loss 1.8873000144958496
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.72it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.41it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.94it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.14it/s]8it [00:01,  6.04it/s]                                 macro  0.5262848736498805
micro  0.6227361382000557
[[286   0  53  25  74   2  27]
 [ 22   0  12   3  15   1   3]
 [ 64   0 241  13 119  19  40]
 [ 45   0  34 753  31   9  23]
 [ 89   0  66  38 390   4  66]
 [ 19   0  72  18  12 284  10]
 [ 84   0  38  63 132   9 281]]
              precision    recall  f1-score   support

           0       0.47      0.61      0.53       467
           1       0.00      0.00      0.00        56
           2       0.47      0.49      0.48       496
           3       0.82      0.84      0.83       895
           4       0.50      0.60      0.55       653
           5       0.87      0.68      0.76       415
           6       0.62      0.46      0.53       607

    accuracy                           0.62      3589
   macro avg       0.54      0.53      0.53      3589
weighted avg       0.63      0.62      0.62      3589

correct 0.6227361382000557 
f1 0.5262848736498805 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.71it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.39it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.96it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.11it/s]8it [00:01,  6.08it/s]                                 macro  0.5180225744167523
micro  0.6143772638617999
[[287   0  61  19  87   3  34]
 [ 24   0   8   2  18   1   2]
 [ 67   0 253  25 119  26  38]
 [ 49   0  17 754  36  11  12]
 [ 67   0  67  48 340   1  71]
 [ 15   0  90  21  13 264  13]
 [108   0  40  51 110  10 307]]
              precision    recall  f1-score   support

           0       0.47      0.58      0.52       491
           1       0.00      0.00      0.00        55
           2       0.47      0.48      0.48       528
           3       0.82      0.86      0.84       879
           4       0.47      0.57      0.52       594
           5       0.84      0.63      0.72       416
           6       0.64      0.49      0.56       626

    accuracy                           0.61      3589
   macro avg       0.53      0.52      0.52      3589
weighted avg       0.62      0.61      0.61      3589

correct 0.6143772638617999 
f1 0.5180225744167523 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:38,  1.42it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.32it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.84it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.26it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.45it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.70it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.76it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.91it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.90it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.01it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.99it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.08it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  4.03it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.10it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.04it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.11it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.03it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.09it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.01it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.08it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.01it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  3.84it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  3.85it/s] 43%|████▎     | 24/56.072265625 [00:06<00:08,  3.98it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  3.97it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.04it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  3.99it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.07it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.03it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.10it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.02it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.09it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.03it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.08it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.01it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.07it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.01it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.08it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  4.02it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.08it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.02it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.08it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  4.01it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.07it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.01it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.07it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  4.01it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.07it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.00it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.07it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  4.01it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.08it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.03it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.10it/s] 98%|█████████▊| 55/56.072265625 [00:14<00:00,  4.05it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.10it/s]57it [00:14,  4.88it/s]                                  57it [00:14,  3.93it/s]epoch 58 iter 0 sum correct 0.904296875 loss 1.8857605457305908
epoch 58 iter 1 sum correct 0.89453125 loss 1.8865749835968018
epoch 58 iter 2 sum correct 0.8860677083333334 loss 1.8872838020324707
epoch 58 iter 3 sum correct 0.8837890625 loss 1.8874446153640747
epoch 58 iter 4 sum correct 0.8796875 loss 1.8877872228622437
epoch 58 iter 5 sum correct 0.8772786458333334 loss 1.8879623413085938
epoch 58 iter 6 sum correct 0.8744419642857143 loss 1.8881629705429077
epoch 58 iter 7 sum correct 0.87353515625 loss 1.8882637023925781
epoch 58 iter 8 sum correct 0.8715277777777778 loss 1.8883813619613647
epoch 58 iter 9 sum correct 0.869921875 loss 1.8885339498519897
epoch 58 iter 10 sum correct 0.8668323863636364 loss 1.8887851238250732
epoch 58 iter 11 sum correct 0.8614908854166666 loss 1.8892163038253784
epoch 58 iter 12 sum correct 0.8589242788461539 loss 1.8894133567810059
epoch 58 iter 13 sum correct 0.8571428571428571 loss 1.8895677328109741
epoch 58 iter 14 sum correct 0.8591145833333333 loss 1.8894143104553223
epoch 58 iter 15 sum correct 0.857666015625 loss 1.889523983001709
epoch 58 iter 16 sum correct 0.8567325367647058 loss 1.8895970582962036
epoch 58 iter 17 sum correct 0.8573133680555556 loss 1.8895560503005981
epoch 58 iter 18 sum correct 0.8577302631578947 loss 1.8895318508148193
epoch 58 iter 19 sum correct 0.85810546875 loss 1.8895031213760376
epoch 58 iter 20 sum correct 0.8583519345238095 loss 1.8894866704940796
epoch 58 iter 21 sum correct 0.8573330965909091 loss 1.8895701169967651
epoch 58 iter 22 sum correct 0.8564877717391305 loss 1.8896440267562866
epoch 58 iter 23 sum correct 0.8558756510416666 loss 1.8896807432174683
epoch 58 iter 24 sum correct 0.856328125 loss 1.889638900756836
epoch 58 iter 25 sum correct 0.8569711538461539 loss 1.8895851373672485
epoch 58 iter 26 sum correct 0.8563368055555556 loss 1.889620065689087
epoch 58 iter 27 sum correct 0.8567940848214286 loss 1.889589548110962
epoch 58 iter 28 sum correct 0.857489224137931 loss 1.8895301818847656
epoch 58 iter 29 sum correct 0.8578125 loss 1.88950777053833
epoch 58 iter 30 sum correct 0.8585559475806451 loss 1.8894551992416382
epoch 58 iter 31 sum correct 0.85833740234375 loss 1.8894803524017334
epoch 58 iter 32 sum correct 0.8590198863636364 loss 1.8894224166870117
epoch 58 iter 33 sum correct 0.8593175551470589 loss 1.8893920183181763
epoch 58 iter 34 sum correct 0.858984375 loss 1.8894189596176147
epoch 58 iter 35 sum correct 0.8595377604166666 loss 1.8893773555755615
epoch 58 iter 36 sum correct 0.8600084459459459 loss 1.8893429040908813
epoch 58 iter 37 sum correct 0.8599403782894737 loss 1.8893458843231201
epoch 58 iter 38 sum correct 0.8601262019230769 loss 1.8893311023712158
epoch 58 iter 39 sum correct 0.86083984375 loss 1.8892742395401
epoch 58 iter 40 sum correct 0.8608517530487805 loss 1.8892724514007568
epoch 58 iter 41 sum correct 0.8608630952380952 loss 1.8892704248428345
epoch 58 iter 42 sum correct 0.8615098110465116 loss 1.8892186880111694
epoch 58 iter 43 sum correct 0.8618607954545454 loss 1.8891983032226562
epoch 58 iter 44 sum correct 0.8624131944444444 loss 1.8891522884368896
epoch 58 iter 45 sum correct 0.8624320652173914 loss 1.8891489505767822
epoch 58 iter 46 sum correct 0.862533244680851 loss 1.8891425132751465
epoch 58 iter 47 sum correct 0.8623860677083334 loss 1.8891547918319702
epoch 58 iter 48 sum correct 0.8626434948979592 loss 1.8891322612762451
epoch 58 iter 49 sum correct 0.8626171875 loss 1.8891363143920898
epoch 58 iter 50 sum correct 0.8624004289215687 loss 1.8891500234603882
epoch 58 iter 51 sum correct 0.8621544471153846 loss 1.8891654014587402
epoch 58 iter 52 sum correct 0.8625810731132075 loss 1.8891303539276123
epoch 58 iter 53 sum correct 0.8629557291666666 loss 1.8890992403030396
epoch 58 iter 54 sum correct 0.8629616477272727 loss 1.8891026973724365
epoch 58 iter 55 sum correct 0.8632114955357143 loss 1.8890787363052368
epoch 58 iter 56 sum correct 0.8490953947368421 loss 1.8891457319259644
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.67it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.28it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.67it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.68it/s]8it [00:01,  5.73it/s]                                 macro  0.5250929680791088
micro  0.6252438005015325
[[201   0  59  30  71   8  98]
 [ 11   0  14   7  11   1  12]
 [ 31   0 240  25  88  26  86]
 [ 11   0  14 772  24   8  66]
 [ 45   0  84  43 316   2 163]
 [ 12   0  51  28  11 293  20]
 [ 16   0  32  54  80   3 422]]
              precision    recall  f1-score   support

           0       0.61      0.43      0.51       467
           1       0.00      0.00      0.00        56
           2       0.49      0.48      0.48       496
           3       0.81      0.86      0.83       895
           4       0.53      0.48      0.50       653
           5       0.86      0.71      0.78       415
           6       0.49      0.70      0.57       607

    accuracy                           0.63      3589
   macro avg       0.54      0.52      0.53      3589
weighted avg       0.63      0.63      0.62      3589

correct 0.6252438005015325 
f1 0.5250929680791088 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.76it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.48it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.85it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.04it/s]8it [00:01,  6.01it/s]                                 macro  0.5309734270323806
micro  0.6372248537196991
[[219   0  83  34  64   4  87]
 [  8   0  15   8  13   2   9]
 [ 35   0 249  40  93  33  78]
 [ 10   0  10 794  16  10  39]
 [ 35   0  70  37 291   2 159]
 [  6   0  50  40  16 274  30]
 [ 16   0  33  34  77   6 460]]
              precision    recall  f1-score   support

           0       0.67      0.45      0.53       491
           1       0.00      0.00      0.00        55
           2       0.49      0.47      0.48       528
           3       0.80      0.90      0.85       879
           4       0.51      0.49      0.50       594
           5       0.83      0.66      0.73       416
           6       0.53      0.73      0.62       626

    accuracy                           0.64      3589
   macro avg       0.55      0.53      0.53      3589
weighted avg       0.63      0.64      0.63      3589

correct 0.6372248537196991 
f1 0.5309734270323806 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:38,  1.44it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.34it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.84it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.29it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.46it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.69it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.75it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.90it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.89it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.00it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.96it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.06it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  4.00it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.09it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.03it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.11it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.05it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.13it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.08it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.15it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.09it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.15it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.08it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.15it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.06it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.11it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.04it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.11it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.04it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.12it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.06it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.14it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.06it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.13it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.06it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.13it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.07it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.14it/s] 70%|██████▉   | 39/56.072265625 [00:09<00:04,  4.07it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.14it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.07it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.14it/s] 77%|███████▋  | 43/56.072265625 [00:10<00:03,  4.08it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.15it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.08it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.14it/s] 84%|████████▍ | 47/56.072265625 [00:11<00:02,  4.07it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.14it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.08it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.15it/s] 91%|█████████ | 51/56.072265625 [00:12<00:01,  4.07it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.15it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.10it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.16it/s] 98%|█████████▊| 55/56.072265625 [00:13<00:00,  4.11it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.17it/s]57it [00:14,  5.03it/s]                                  57it [00:14,  3.98it/s]epoch 59 iter 0 sum correct 0.8984375 loss 1.886268973350525
epoch 59 iter 1 sum correct 0.896484375 loss 1.886630654335022
epoch 59 iter 2 sum correct 0.8912760416666666 loss 1.8868989944458008
epoch 59 iter 3 sum correct 0.8837890625 loss 1.8874872922897339
epoch 59 iter 4 sum correct 0.8765625 loss 1.8880786895751953
epoch 59 iter 5 sum correct 0.8743489583333334 loss 1.888312578201294
epoch 59 iter 6 sum correct 0.8716517857142857 loss 1.8884741067886353
epoch 59 iter 7 sum correct 0.874755859375 loss 1.8882144689559937
epoch 59 iter 8 sum correct 0.8752170138888888 loss 1.8881645202636719
epoch 59 iter 9 sum correct 0.8775390625 loss 1.8879804611206055
epoch 59 iter 10 sum correct 0.8781960227272727 loss 1.8879263401031494
epoch 59 iter 11 sum correct 0.87939453125 loss 1.8878183364868164
epoch 59 iter 12 sum correct 0.8816105769230769 loss 1.887656807899475
epoch 59 iter 13 sum correct 0.8825334821428571 loss 1.8875848054885864
epoch 59 iter 14 sum correct 0.8834635416666666 loss 1.8874942064285278
epoch 59 iter 15 sum correct 0.8839111328125 loss 1.8874562978744507
epoch 59 iter 16 sum correct 0.8835018382352942 loss 1.8874942064285278
epoch 59 iter 17 sum correct 0.8827039930555556 loss 1.8875489234924316
epoch 59 iter 18 sum correct 0.8829152960526315 loss 1.8875336647033691
epoch 59 iter 19 sum correct 0.88369140625 loss 1.8874748945236206
epoch 59 iter 20 sum correct 0.8844866071428571 loss 1.8873969316482544
epoch 59 iter 21 sum correct 0.8848544034090909 loss 1.8873742818832397
epoch 59 iter 22 sum correct 0.884765625 loss 1.8873718976974487
epoch 59 iter 23 sum correct 0.885009765625 loss 1.8873436450958252
epoch 59 iter 24 sum correct 0.88484375 loss 1.8873708248138428
epoch 59 iter 25 sum correct 0.8843149038461539 loss 1.88741135597229
epoch 59 iter 26 sum correct 0.8842592592592593 loss 1.8874173164367676
epoch 59 iter 27 sum correct 0.8846261160714286 loss 1.887392520904541
epoch 59 iter 28 sum correct 0.8844962284482759 loss 1.8874053955078125
epoch 59 iter 29 sum correct 0.8850260416666667 loss 1.887361764907837
epoch 59 iter 30 sum correct 0.8850176411290323 loss 1.8873677253723145
epoch 59 iter 31 sum correct 0.88482666015625 loss 1.8873847723007202
epoch 59 iter 32 sum correct 0.8836410984848485 loss 1.8874801397323608
epoch 59 iter 33 sum correct 0.8839613970588235 loss 1.8874547481536865
epoch 59 iter 34 sum correct 0.8839285714285714 loss 1.8874492645263672
epoch 59 iter 35 sum correct 0.8838433159722222 loss 1.8874588012695312
epoch 59 iter 36 sum correct 0.8845544763513513 loss 1.8873982429504395
epoch 59 iter 37 sum correct 0.8852796052631579 loss 1.8873405456542969
epoch 59 iter 38 sum correct 0.8845653044871795 loss 1.8873915672302246
epoch 59 iter 39 sum correct 0.884326171875 loss 1.8874067068099976
epoch 59 iter 40 sum correct 0.883765243902439 loss 1.8874493837356567
epoch 59 iter 41 sum correct 0.8836030505952381 loss 1.8874576091766357
epoch 59 iter 42 sum correct 0.8836300872093024 loss 1.8874554634094238
epoch 59 iter 43 sum correct 0.8837002840909091 loss 1.8874472379684448
epoch 59 iter 44 sum correct 0.8840711805555556 loss 1.8874166011810303
epoch 59 iter 45 sum correct 0.8834493885869565 loss 1.887470006942749
epoch 59 iter 46 sum correct 0.8832280585106383 loss 1.8874850273132324
epoch 59 iter 47 sum correct 0.8832600911458334 loss 1.887477159500122
epoch 59 iter 48 sum correct 0.8836495535714286 loss 1.8874467611312866
epoch 59 iter 49 sum correct 0.88421875 loss 1.887399673461914
epoch 59 iter 50 sum correct 0.8841911764705882 loss 1.887406826019287
epoch 59 iter 51 sum correct 0.8848407451923077 loss 1.8873587846755981
epoch 59 iter 52 sum correct 0.8850972877358491 loss 1.8873388767242432
epoch 59 iter 53 sum correct 0.8846571180555556 loss 1.8873710632324219
epoch 59 iter 54 sum correct 0.8851917613636363 loss 1.887331247329712
epoch 59 iter 55 sum correct 0.8853585379464286 loss 1.8873178958892822
epoch 59 iter 56 sum correct 0.8706825657894737 loss 1.8876032829284668
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.67it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.28it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.76it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.99it/s]8it [00:01,  5.96it/s]                                 macro  0.539687647822941
micro  0.6394538868765672
[[272   0  41  27  69  10  48]
 [ 28   0   6   4  11   1   6]
 [ 69   0 216  10 101  48  52]
 [ 25   0  17 752  33  16  52]
 [ 70   0  69  34 354  12 114]
 [ 12   0  19  17  14 344   9]
 [ 41   0  23  46 129  11 357]]
              precision    recall  f1-score   support

           0       0.53      0.58      0.55       467
           1       0.00      0.00      0.00        56
           2       0.55      0.44      0.49       496
           3       0.84      0.84      0.84       895
           4       0.50      0.54      0.52       653
           5       0.78      0.83      0.80       415
           6       0.56      0.59      0.57       607

    accuracy                           0.64      3589
   macro avg       0.54      0.55      0.54      3589
weighted avg       0.63      0.64      0.63      3589

correct 0.6394538868765672 
f1 0.539687647822941 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.71it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.34it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.62it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.82it/s]8it [00:01,  5.85it/s]                                 macro  0.551381450640173
micro  0.6556143772638618
[[301   0  38  18  71   9  54]
 [ 29   0   8   6   7   2   3]
 [ 75   0 226  24 100  49  54]
 [ 19   0  17 764  28  22  29]
 [ 69   0  58  28 323  10 106]
 [ 13   0  24  22   9 332  16]
 [ 52   0  35  29  96   7 407]]
              precision    recall  f1-score   support

           0       0.54      0.61      0.57       491
           1       0.00      0.00      0.00        55
           2       0.56      0.43      0.48       528
           3       0.86      0.87      0.86       879
           4       0.51      0.54      0.53       594
           5       0.77      0.80      0.78       416
           6       0.61      0.65      0.63       626

    accuracy                           0.66      3589
   macro avg       0.55      0.56      0.55      3589
weighted avg       0.65      0.66      0.65      3589

correct 0.6556143772638618 
f1 0.551381450640173 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:38,  1.42it/s]  4%|▎         | 2/56.072265625 [00:00<00:22,  2.35it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.85it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.30it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.49it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.74it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.75it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.91it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.89it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.02it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.98it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.07it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  4.03it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.11it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.05it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.13it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.06it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.13it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.03it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.11it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.05it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.12it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.07it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.14it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.08it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.15it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.07it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.14it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.07it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.14it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.08it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.14it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.07it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.14it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.08it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.15it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.06it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.13it/s] 70%|██████▉   | 39/56.072265625 [00:09<00:04,  4.04it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.12it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.06it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.13it/s] 77%|███████▋  | 43/56.072265625 [00:10<00:03,  4.06it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.12it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.05it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.11it/s] 84%|████████▍ | 47/56.072265625 [00:11<00:02,  4.05it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:02,  3.88it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  3.90it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.03it/s] 91%|█████████ | 51/56.072265625 [00:12<00:01,  4.02it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.10it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.05it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.11it/s] 98%|█████████▊| 55/56.072265625 [00:13<00:00,  4.05it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.13it/s]57it [00:14,  5.00it/s]                                  57it [00:14,  3.97it/s]epoch 60 iter 0 sum correct 0.904296875 loss 1.8857314586639404
epoch 60 iter 1 sum correct 0.8994140625 loss 1.8860933780670166
epoch 60 iter 2 sum correct 0.90234375 loss 1.8858622312545776
epoch 60 iter 3 sum correct 0.9033203125 loss 1.8858542442321777
epoch 60 iter 4 sum correct 0.901953125 loss 1.8859366178512573
epoch 60 iter 5 sum correct 0.9016927083333334 loss 1.8859903812408447
epoch 60 iter 6 sum correct 0.8967633928571429 loss 1.8863853216171265
epoch 60 iter 7 sum correct 0.894287109375 loss 1.8866081237792969
epoch 60 iter 8 sum correct 0.8936631944444444 loss 1.8866583108901978
epoch 60 iter 9 sum correct 0.89375 loss 1.8866623640060425
epoch 60 iter 10 sum correct 0.8924005681818182 loss 1.8867666721343994
epoch 60 iter 11 sum correct 0.8929036458333334 loss 1.8867011070251465
epoch 60 iter 12 sum correct 0.8924278846153846 loss 1.8867322206497192
epoch 60 iter 13 sum correct 0.8916015625 loss 1.8867993354797363
epoch 60 iter 14 sum correct 0.892578125 loss 1.886713981628418
epoch 60 iter 15 sum correct 0.893310546875 loss 1.8866597414016724
epoch 60 iter 16 sum correct 0.8914292279411765 loss 1.8867988586425781
epoch 60 iter 17 sum correct 0.8922526041666666 loss 1.886728286743164
epoch 60 iter 18 sum correct 0.8912417763157895 loss 1.8868085145950317
epoch 60 iter 19 sum correct 0.890625 loss 1.8868683576583862
epoch 60 iter 20 sum correct 0.8900669642857143 loss 1.8869153261184692
epoch 60 iter 21 sum correct 0.8902698863636364 loss 1.8868898153305054
epoch 60 iter 22 sum correct 0.8905400815217391 loss 1.8868647813796997
epoch 60 iter 23 sum correct 0.8900553385416666 loss 1.8869060277938843
epoch 60 iter 24 sum correct 0.89125 loss 1.8868159055709839
epoch 60 iter 25 sum correct 0.8908503605769231 loss 1.886852741241455
epoch 60 iter 26 sum correct 0.8906973379629629 loss 1.886856198310852
epoch 60 iter 27 sum correct 0.8908342633928571 loss 1.8868621587753296
epoch 60 iter 28 sum correct 0.8909617456896551 loss 1.8868494033813477
epoch 60 iter 29 sum correct 0.8910807291666667 loss 1.8868385553359985
epoch 60 iter 30 sum correct 0.8919480846774194 loss 1.88677179813385
epoch 60 iter 31 sum correct 0.89202880859375 loss 1.8867645263671875
epoch 60 iter 32 sum correct 0.8918678977272727 loss 1.8867813348770142
epoch 60 iter 33 sum correct 0.8913143382352942 loss 1.8868244886398315
epoch 60 iter 34 sum correct 0.8903459821428571 loss 1.8868966102600098
epoch 60 iter 35 sum correct 0.890625 loss 1.8868764638900757
epoch 60 iter 36 sum correct 0.8908889358108109 loss 1.8868491649627686
epoch 60 iter 37 sum correct 0.8910875822368421 loss 1.8868376016616821
epoch 60 iter 38 sum correct 0.8909755608974359 loss 1.8868457078933716
epoch 60 iter 39 sum correct 0.890478515625 loss 1.8868783712387085
epoch 60 iter 40 sum correct 0.8908155487804879 loss 1.8868556022644043
epoch 60 iter 41 sum correct 0.8908110119047619 loss 1.886852502822876
epoch 60 iter 42 sum correct 0.8909429505813954 loss 1.8868451118469238
epoch 60 iter 43 sum correct 0.8905362215909091 loss 1.8868789672851562
epoch 60 iter 44 sum correct 0.8907986111111111 loss 1.8868604898452759
epoch 60 iter 45 sum correct 0.8912618885869565 loss 1.8868237733840942
epoch 60 iter 46 sum correct 0.8919132313829787 loss 1.8867738246917725
epoch 60 iter 47 sum correct 0.8916829427083334 loss 1.8867943286895752
epoch 60 iter 48 sum correct 0.8916613520408163 loss 1.8867943286895752
epoch 60 iter 49 sum correct 0.8916015625 loss 1.8868000507354736
epoch 60 iter 50 sum correct 0.8914292279411765 loss 1.8868101835250854
epoch 60 iter 51 sum correct 0.8916391225961539 loss 1.886796474456787
epoch 60 iter 52 sum correct 0.8917305424528302 loss 1.886786699295044
epoch 60 iter 53 sum correct 0.8914207175925926 loss 1.886814832687378
epoch 60 iter 54 sum correct 0.8911576704545454 loss 1.8868392705917358
epoch 60 iter 55 sum correct 0.8911830357142857 loss 1.8868365287780762
epoch 60 iter 56 sum correct 0.8765419407894737 loss 1.8869696855545044
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.62it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.26it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.65it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.96it/s]8it [00:01,  5.85it/s]                                 macro  0.5407468655367419
micro  0.6377821120089161
[[238   0  72  39  70   7  41]
 [ 17   0  17   6  10   0   6]
 [ 44   0 267  21  94  25  45]
 [ 12   0  29 764  24  11  55]
 [ 51   0 128  22 351   8  93]
 [  7   0  35  22  13 326  12]
 [ 39   0  59  48 111   7 343]]
              precision    recall  f1-score   support

           0       0.58      0.51      0.54       467
           1       0.00      0.00      0.00        56
           2       0.44      0.54      0.48       496
           3       0.83      0.85      0.84       895
           4       0.52      0.54      0.53       653
           5       0.85      0.79      0.82       415
           6       0.58      0.57      0.57       607

    accuracy                           0.64      3589
   macro avg       0.54      0.54      0.54      3589
weighted avg       0.63      0.64      0.63      3589

correct 0.6377821120089161 
f1 0.5407468655367419 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.75it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.44it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.85it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.02it/s]8it [00:01,  6.01it/s]                                 macro  0.5410745952448612
micro  0.6416829200334355
[[248   0  95  21  79   5  43]
 [ 17   0  20   5  11   2   0]
 [ 45   0 276  23  95  42  47]
 [ 10   0  23 777  26  14  29]
 [ 44   0 121  29 303   5  92]
 [  9   0  36  26  15 315  15]
 [ 49   0  56  27 102   8 384]]
              precision    recall  f1-score   support

           0       0.59      0.51      0.54       491
           1       0.00      0.00      0.00        55
           2       0.44      0.52      0.48       528
           3       0.86      0.88      0.87       879
           4       0.48      0.51      0.49       594
           5       0.81      0.76      0.78       416
           6       0.63      0.61      0.62       626

    accuracy                           0.64      3589
   macro avg       0.54      0.54      0.54      3589
weighted avg       0.64      0.64      0.64      3589

correct 0.6416829200334355 
f1 0.5410745952448612 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:38,  1.42it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.33it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.83it/s]  7%|▋         | 4/56.072265625 [00:01<00:16,  3.25it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.41it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.67it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.75it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.90it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.88it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.01it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  4.00it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.09it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  4.05it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.12it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.06it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.13it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.07it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.12it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.05it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.10it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.02it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.11it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.05it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.11it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.05it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.13it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.05it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.10it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  3.99it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.08it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.04it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.09it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.04it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.11it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.05it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.11it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.03it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.11it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  4.05it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.11it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.04it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.12it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  4.06it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.11it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.02it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.10it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  4.05it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.11it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.04it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.12it/s] 91%|█████████ | 51/56.072265625 [00:12<00:01,  4.08it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.13it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.06it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.14it/s] 98%|█████████▊| 55/56.072265625 [00:13<00:00,  4.08it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.13it/s]57it [00:14,  4.97it/s]                                  57it [00:14,  3.97it/s]epoch 61 iter 0 sum correct 0.904296875 loss 1.8858287334442139
epoch 61 iter 1 sum correct 0.8994140625 loss 1.886136770248413
epoch 61 iter 2 sum correct 0.8880208333333334 loss 1.8870275020599365
epoch 61 iter 3 sum correct 0.8837890625 loss 1.8873934745788574
epoch 61 iter 4 sum correct 0.883984375 loss 1.8873863220214844
epoch 61 iter 5 sum correct 0.8824869791666666 loss 1.887487769126892
epoch 61 iter 6 sum correct 0.8814174107142857 loss 1.8875948190689087
epoch 61 iter 7 sum correct 0.88427734375 loss 1.8873624801635742
epoch 61 iter 8 sum correct 0.8858506944444444 loss 1.887269377708435
epoch 61 iter 9 sum correct 0.8853515625 loss 1.8873217105865479
epoch 61 iter 10 sum correct 0.8852982954545454 loss 1.8873289823532104
epoch 61 iter 11 sum correct 0.8841145833333334 loss 1.8874173164367676
epoch 61 iter 12 sum correct 0.8822115384615384 loss 1.8875892162322998
epoch 61 iter 13 sum correct 0.8809988839285714 loss 1.8876819610595703
epoch 61 iter 14 sum correct 0.8811197916666667 loss 1.8876780271530151
epoch 61 iter 15 sum correct 0.8800048828125 loss 1.88775634765625
epoch 61 iter 16 sum correct 0.8807444852941176 loss 1.8877049684524536
epoch 61 iter 17 sum correct 0.8824869791666666 loss 1.8875643014907837
epoch 61 iter 18 sum correct 0.8814761513157895 loss 1.8876497745513916
epoch 61 iter 19 sum correct 0.88017578125 loss 1.8877590894699097
epoch 61 iter 20 sum correct 0.8791852678571429 loss 1.8878353834152222
epoch 61 iter 21 sum correct 0.8799715909090909 loss 1.8877696990966797
epoch 61 iter 22 sum correct 0.8798403532608695 loss 1.8877832889556885
epoch 61 iter 23 sum correct 0.8795572916666666 loss 1.8878049850463867
epoch 61 iter 24 sum correct 0.87953125 loss 1.8878008127212524
epoch 61 iter 25 sum correct 0.8800330528846154 loss 1.887756109237671
epoch 61 iter 26 sum correct 0.8802083333333334 loss 1.887741208076477
epoch 61 iter 27 sum correct 0.8816266741071429 loss 1.8876267671585083
epoch 61 iter 28 sum correct 0.8819369612068966 loss 1.8875998258590698
epoch 61 iter 29 sum correct 0.8819661458333333 loss 1.8875941038131714
epoch 61 iter 30 sum correct 0.8823714717741935 loss 1.88756263256073
epoch 61 iter 31 sum correct 0.88262939453125 loss 1.8875428438186646
epoch 61 iter 32 sum correct 0.8823982007575758 loss 1.8875610828399658
epoch 61 iter 33 sum correct 0.8825827205882353 loss 1.8875454664230347
epoch 61 iter 34 sum correct 0.8827008928571428 loss 1.8875409364700317
epoch 61 iter 35 sum correct 0.8831380208333334 loss 1.8875051736831665
epoch 61 iter 36 sum correct 0.8830236486486487 loss 1.8875130414962769
epoch 61 iter 37 sum correct 0.8829666940789473 loss 1.8875178098678589
epoch 61 iter 38 sum correct 0.8829126602564102 loss 1.8875267505645752
epoch 61 iter 39 sum correct 0.883349609375 loss 1.8874921798706055
epoch 61 iter 40 sum correct 0.8830506859756098 loss 1.8875130414962769
epoch 61 iter 41 sum correct 0.8828590029761905 loss 1.8875267505645752
epoch 61 iter 42 sum correct 0.8829033430232558 loss 1.8875195980072021
epoch 61 iter 43 sum correct 0.8828568892045454 loss 1.8875197172164917
epoch 61 iter 44 sum correct 0.8832899305555556 loss 1.8874837160110474
epoch 61 iter 45 sum correct 0.8834918478260869 loss 1.8874642848968506
epoch 61 iter 46 sum correct 0.8834773936170213 loss 1.8874611854553223
epoch 61 iter 47 sum correct 0.8836263020833334 loss 1.8874472379684448
epoch 61 iter 48 sum correct 0.8841278698979592 loss 1.8874056339263916
epoch 61 iter 49 sum correct 0.884453125 loss 1.8873733282089233
epoch 61 iter 50 sum correct 0.8844592524509803 loss 1.8873722553253174
epoch 61 iter 51 sum correct 0.8849158653846154 loss 1.887336254119873
epoch 61 iter 52 sum correct 0.8852815448113207 loss 1.8873069286346436
epoch 61 iter 53 sum correct 0.8855251736111112 loss 1.8872931003570557
epoch 61 iter 54 sum correct 0.8854758522727273 loss 1.8872971534729004
epoch 61 iter 55 sum correct 0.8858119419642857 loss 1.8872731924057007
epoch 61 iter 56 sum correct 0.8714021381578947 loss 1.8872573375701904
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.59it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.17it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.65it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.87it/s]8it [00:01,  5.81it/s]                                 macro  0.5254665642804431
micro  0.6205071050431875
[[256   0  74  19  46  15  57]
 [ 18   0  16   4   7   0  11]
 [ 48   0 269  13  58  40  68]
 [ 27   0  38 723  26  14  67]
 [ 59   0 146  28 255  16 149]
 [  7   0  36  17   7 337  11]
 [ 39   0  62  53  51  15 387]]
              precision    recall  f1-score   support

           0       0.56      0.55      0.56       467
           1       0.00      0.00      0.00        56
           2       0.42      0.54      0.47       496
           3       0.84      0.81      0.83       895
           4       0.57      0.39      0.46       653
           5       0.77      0.81      0.79       415
           6       0.52      0.64      0.57       607

    accuracy                           0.62      3589
   macro avg       0.53      0.53      0.53      3589
weighted avg       0.62      0.62      0.62      3589

correct 0.6205071050431875 
f1 0.5254665642804431 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.71it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.36it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.73it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.93it/s]8it [00:01,  5.92it/s]                                 macro  0.5350050495203866
micro  0.6386179994427417
[[258   0  96  14  38   8  77]
 [ 20   0  20   5   2   2   6]
 [ 50   0 279  16  67  51  65]
 [ 19   0  29 755  23  21  32]
 [ 53   0 127  28 219   8 159]
 [  6   0  41  19   8 328  14]
 [ 25   0  57  32  48  11 453]]
              precision    recall  f1-score   support

           0       0.60      0.53      0.56       491
           1       0.00      0.00      0.00        55
           2       0.43      0.53      0.47       528
           3       0.87      0.86      0.86       879
           4       0.54      0.37      0.44       594
           5       0.76      0.79      0.78       416
           6       0.56      0.72      0.63       626

    accuracy                           0.64      3589
   macro avg       0.54      0.54      0.54      3589
weighted avg       0.63      0.64      0.63      3589

correct 0.6386179994427417 
f1 0.5350050495203866 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:39,  1.39it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.30it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.83it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.26it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.47it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.72it/s] 12%|█▏        | 7/56.072265625 [00:02<00:12,  3.78it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.93it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.91it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.03it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  4.00it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.07it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  4.03it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.11it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.05it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.11it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.05it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.13it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.07it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.12it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.05it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.13it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.07it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.13it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.06it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.14it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.08it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.13it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.07it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.14it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.08it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.10it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.02it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.10it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.05it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.11it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.04it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.12it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  4.06it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.12it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.06it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.13it/s] 77%|███████▋  | 43/56.072265625 [00:10<00:03,  4.07it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.12it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.05it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.13it/s] 84%|████████▍ | 47/56.072265625 [00:11<00:02,  4.06it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.11it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.06it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.14it/s] 91%|█████████ | 51/56.072265625 [00:12<00:01,  4.10it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.15it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.09it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.16it/s] 98%|█████████▊| 55/56.072265625 [00:13<00:00,  4.11it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.16it/s]57it [00:14,  5.02it/s]                                  57it [00:14,  3.98it/s]epoch 62 iter 0 sum correct 0.8984375 loss 1.8862277269363403
epoch 62 iter 1 sum correct 0.8857421875 loss 1.887169599533081
epoch 62 iter 2 sum correct 0.8873697916666666 loss 1.887047529220581
epoch 62 iter 3 sum correct 0.880859375 loss 1.8875813484191895
epoch 62 iter 4 sum correct 0.871875 loss 1.8883079290390015
epoch 62 iter 5 sum correct 0.8701171875 loss 1.8884425163269043
epoch 62 iter 6 sum correct 0.8683035714285714 loss 1.8885750770568848
epoch 62 iter 7 sum correct 0.868896484375 loss 1.8885568380355835
epoch 62 iter 8 sum correct 0.8669704861111112 loss 1.88872492313385
epoch 62 iter 9 sum correct 0.865234375 loss 1.8888561725616455
epoch 62 iter 10 sum correct 0.8664772727272727 loss 1.8887609243392944
epoch 62 iter 11 sum correct 0.8675130208333334 loss 1.8887122869491577
epoch 62 iter 12 sum correct 0.8677884615384616 loss 1.888709306716919
epoch 62 iter 13 sum correct 0.8655133928571429 loss 1.888870358467102
epoch 62 iter 14 sum correct 0.865625 loss 1.8888742923736572
epoch 62 iter 15 sum correct 0.8648681640625 loss 1.888936161994934
epoch 62 iter 16 sum correct 0.8642003676470589 loss 1.8889976739883423
epoch 62 iter 17 sum correct 0.8649088541666666 loss 1.8889533281326294
epoch 62 iter 18 sum correct 0.8651315789473685 loss 1.88892662525177
epoch 62 iter 19 sum correct 0.86416015625 loss 1.8890050649642944
epoch 62 iter 20 sum correct 0.8657924107142857 loss 1.8888742923736572
epoch 62 iter 21 sum correct 0.8663884943181818 loss 1.8888200521469116
epoch 62 iter 22 sum correct 0.8671875 loss 1.8887513875961304
epoch 62 iter 23 sum correct 0.867431640625 loss 1.8887280225753784
epoch 62 iter 24 sum correct 0.868671875 loss 1.8886271715164185
epoch 62 iter 25 sum correct 0.8679387019230769 loss 1.8886831998825073
epoch 62 iter 26 sum correct 0.8692853009259259 loss 1.8885754346847534
epoch 62 iter 27 sum correct 0.8693498883928571 loss 1.8885656595230103
epoch 62 iter 28 sum correct 0.8699488146551724 loss 1.8885233402252197
epoch 62 iter 29 sum correct 0.8705729166666667 loss 1.888478398323059
epoch 62 iter 30 sum correct 0.8709047379032258 loss 1.888449788093567
epoch 62 iter 31 sum correct 0.87060546875 loss 1.8884717226028442
epoch 62 iter 32 sum correct 0.8702651515151515 loss 1.888488531112671
epoch 62 iter 33 sum correct 0.8708639705882353 loss 1.8884429931640625
epoch 62 iter 34 sum correct 0.8714285714285714 loss 1.88839852809906
epoch 62 iter 35 sum correct 0.8712565104166666 loss 1.8884080648422241
epoch 62 iter 36 sum correct 0.8712521114864865 loss 1.888403296470642
epoch 62 iter 37 sum correct 0.8706311677631579 loss 1.8884464502334595
epoch 62 iter 38 sum correct 0.8708433493589743 loss 1.8884286880493164
epoch 62 iter 39 sum correct 0.8716796875 loss 1.8883638381958008
epoch 62 iter 40 sum correct 0.8718083079268293 loss 1.8883548974990845
epoch 62 iter 41 sum correct 0.8719308035714286 loss 1.888345718383789
epoch 62 iter 42 sum correct 0.8721384447674418 loss 1.8883299827575684
epoch 62 iter 43 sum correct 0.8726917613636364 loss 1.8882849216461182
epoch 62 iter 44 sum correct 0.8733506944444445 loss 1.8882333040237427
epoch 62 iter 45 sum correct 0.8735563858695652 loss 1.8882144689559937
epoch 62 iter 46 sum correct 0.874251994680851 loss 1.8881558179855347
epoch 62 iter 47 sum correct 0.8745930989583334 loss 1.8881266117095947
epoch 62 iter 48 sum correct 0.8753188775510204 loss 1.8880717754364014
epoch 62 iter 49 sum correct 0.875390625 loss 1.888060450553894
epoch 62 iter 50 sum correct 0.875765931372549 loss 1.8880313634872437
epoch 62 iter 51 sum correct 0.8756009615384616 loss 1.888047456741333
epoch 62 iter 52 sum correct 0.8754422169811321 loss 1.888059139251709
epoch 62 iter 53 sum correct 0.8757957175925926 loss 1.8880326747894287
epoch 62 iter 54 sum correct 0.8760653409090909 loss 1.8880099058151245
epoch 62 iter 55 sum correct 0.8761858258928571 loss 1.8879992961883545
epoch 62 iter 56 sum correct 0.8617735745614035 loss 1.8881452083587646
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.71it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.32it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.42it/s] 86%|████████▌ | 6/7.009765625 [00:01<00:00,  6.15it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.72it/s]8it [00:01,  5.72it/s]                                 macro  0.53263127368401
micro  0.6271942045137922
[[273   0  69  24  61   8  32]
 [ 22   0  18   4   8   0   4]
 [ 57   0 245  11 100  39  44]
 [ 34   0  27 730  21  17  66]
 [ 62   0 108  30 345   8 100]
 [ 14   0  33  17   8 327  16]
 [ 53   0  50  48 119   6 331]]
              precision    recall  f1-score   support

           0       0.53      0.58      0.56       467
           1       0.00      0.00      0.00        56
           2       0.45      0.49      0.47       496
           3       0.84      0.82      0.83       895
           4       0.52      0.53      0.52       653
           5       0.81      0.79      0.80       415
           6       0.56      0.55      0.55       607

    accuracy                           0.63      3589
   macro avg       0.53      0.54      0.53      3589
weighted avg       0.62      0.63      0.63      3589

correct 0.6271942045137922 
f1 0.53263127368401 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.79it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.47it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.94it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.10it/s]8it [00:01,  6.11it/s]                                 macro  0.5364607564132011
micro  0.6338813039843968
[[276   0  89  18  66   8  34]
 [ 28   0  16   4   5   2   0]
 [ 61   0 269  22  86  42  48]
 [ 35   0  27 740  22  18  37]
 [ 58   0 100  29 308   5  94]
 [ 11   0  46  21  11 310  17]
 [ 60   0  44  34 109   7 372]]
              precision    recall  f1-score   support

           0       0.52      0.56      0.54       491
           1       0.00      0.00      0.00        55
           2       0.46      0.51      0.48       528
           3       0.85      0.84      0.85       879
           4       0.51      0.52      0.51       594
           5       0.79      0.75      0.77       416
           6       0.62      0.59      0.61       626

    accuracy                           0.63      3589
   macro avg       0.54      0.54      0.54      3589
weighted avg       0.63      0.63      0.63      3589

correct 0.6338813039843968 
f1 0.5364607564132011 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:39,  1.40it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.32it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.82it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.27it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.46it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.72it/s] 12%|█▏        | 7/56.072265625 [00:02<00:12,  3.78it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.94it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.91it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.04it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.88it/s] 21%|██▏       | 12/56.072265625 [00:03<00:12,  3.66it/s] 23%|██▎       | 13/56.072265625 [00:03<00:11,  3.69it/s] 25%|██▍       | 14/56.072265625 [00:04<00:11,  3.80it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  3.80it/s] 29%|██▊       | 16/56.072265625 [00:04<00:10,  3.92it/s] 30%|███       | 17/56.072265625 [00:04<00:10,  3.87it/s] 32%|███▏      | 18/56.072265625 [00:05<00:09,  3.99it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  3.93it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.04it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  3.99it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.07it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.01it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.09it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.01it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.09it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.01it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.09it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.01it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.09it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  3.95it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.02it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  3.98it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.07it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.00it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.08it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.00it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.08it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  4.00it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.09it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.03it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.10it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  4.02it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.09it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.03it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.10it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  4.01it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.09it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.03it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.11it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  3.96it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:01,  4.06it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.03it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.11it/s] 98%|█████████▊| 55/56.072265625 [00:14<00:00,  4.04it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.11it/s]57it [00:14,  4.95it/s]                                  57it [00:14,  3.91it/s]epoch 63 iter 0 sum correct 0.91015625 loss 1.885297179222107
epoch 63 iter 1 sum correct 0.884765625 loss 1.8873047828674316
epoch 63 iter 2 sum correct 0.8697916666666666 loss 1.8885493278503418
epoch 63 iter 3 sum correct 0.85791015625 loss 1.88958740234375
epoch 63 iter 4 sum correct 0.84609375 loss 1.8905175924301147
epoch 63 iter 5 sum correct 0.8336588541666666 loss 1.891542911529541
epoch 63 iter 6 sum correct 0.8270089285714286 loss 1.8921003341674805
epoch 63 iter 7 sum correct 0.822998046875 loss 1.8924404382705688
epoch 63 iter 8 sum correct 0.8159722222222222 loss 1.8929868936538696
epoch 63 iter 9 sum correct 0.81484375 loss 1.893105149269104
epoch 63 iter 10 sum correct 0.8128551136363636 loss 1.8932563066482544
epoch 63 iter 11 sum correct 0.81201171875 loss 1.8933171033859253
epoch 63 iter 12 sum correct 0.8099459134615384 loss 1.8934653997421265
epoch 63 iter 13 sum correct 0.8115234375 loss 1.8933366537094116
epoch 63 iter 14 sum correct 0.8125 loss 1.8932693004608154
epoch 63 iter 15 sum correct 0.8128662109375 loss 1.8932390213012695
epoch 63 iter 16 sum correct 0.8120404411764706 loss 1.8932980298995972
epoch 63 iter 17 sum correct 0.8121744791666666 loss 1.89328932762146
epoch 63 iter 18 sum correct 0.811780427631579 loss 1.893319845199585
epoch 63 iter 19 sum correct 0.81162109375 loss 1.89334237575531
epoch 63 iter 20 sum correct 0.8126860119047619 loss 1.893237590789795
epoch 63 iter 21 sum correct 0.8119673295454546 loss 1.8932878971099854
epoch 63 iter 22 sum correct 0.8127547554347826 loss 1.8932281732559204
epoch 63 iter 23 sum correct 0.8116861979166666 loss 1.8933041095733643
epoch 63 iter 24 sum correct 0.81234375 loss 1.8932437896728516
epoch 63 iter 25 sum correct 0.8128756009615384 loss 1.8931902647018433
epoch 63 iter 26 sum correct 0.8143084490740741 loss 1.89307701587677
epoch 63 iter 27 sum correct 0.8151506696428571 loss 1.8930182456970215
epoch 63 iter 28 sum correct 0.8152613146551724 loss 1.8930028676986694
epoch 63 iter 29 sum correct 0.8166666666666667 loss 1.8928889036178589
epoch 63 iter 30 sum correct 0.817351310483871 loss 1.8928301334381104
epoch 63 iter 31 sum correct 0.81756591796875 loss 1.8928117752075195
epoch 63 iter 32 sum correct 0.8184777462121212 loss 1.892740249633789
epoch 63 iter 33 sum correct 0.8185317095588235 loss 1.8927338123321533
epoch 63 iter 34 sum correct 0.8198660714285714 loss 1.8926167488098145
epoch 63 iter 35 sum correct 0.8206380208333334 loss 1.8925446271896362
epoch 63 iter 36 sum correct 0.821948902027027 loss 1.8924410343170166
epoch 63 iter 37 sum correct 0.8226254111842105 loss 1.8923908472061157
epoch 63 iter 38 sum correct 0.8229667467948718 loss 1.8923635482788086
epoch 63 iter 39 sum correct 0.823828125 loss 1.8922935724258423
epoch 63 iter 40 sum correct 0.8249809451219512 loss 1.8922041654586792
epoch 63 iter 41 sum correct 0.8262183779761905 loss 1.8921029567718506
epoch 63 iter 42 sum correct 0.8268986191860465 loss 1.892049789428711
epoch 63 iter 43 sum correct 0.8276811079545454 loss 1.8919862508773804
epoch 63 iter 44 sum correct 0.8282552083333333 loss 1.8919380903244019
epoch 63 iter 45 sum correct 0.8288043478260869 loss 1.8918943405151367
epoch 63 iter 46 sum correct 0.8297456781914894 loss 1.8918110132217407
epoch 63 iter 47 sum correct 0.8301188151041666 loss 1.8917759656906128
epoch 63 iter 48 sum correct 0.8307955994897959 loss 1.8917208909988403
epoch 63 iter 49 sum correct 0.831171875 loss 1.8916828632354736
epoch 63 iter 50 sum correct 0.8319929534313726 loss 1.891617774963379
epoch 63 iter 51 sum correct 0.8326697716346154 loss 1.8915560245513916
epoch 63 iter 52 sum correct 0.8338001179245284 loss 1.891462802886963
epoch 63 iter 53 sum correct 0.8344184027777778 loss 1.8914092779159546
epoch 63 iter 54 sum correct 0.8346946022727273 loss 1.8913840055465698
epoch 63 iter 55 sum correct 0.8348214285714286 loss 1.891370415687561
epoch 63 iter 56 sum correct 0.8211348684210527 loss 1.891478419303894
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.61it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.15it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.40it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.61it/s]8it [00:01,  5.64it/s]                                 macro  0.5231822296152397
micro  0.6269155753691836
[[270   0  37  27  53  22  58]
 [ 20   0  12   6   9   2   7]
 [ 61   0 179  13  86  73  84]
 [ 28   0  21 737  29  24  56]
 [ 70   0  61  29 325  27 141]
 [  9   0  12  17   8 356  13]
 [ 39   0  22  40 105  18 383]]
              precision    recall  f1-score   support

           0       0.54      0.58      0.56       467
           1       0.00      0.00      0.00        56
           2       0.52      0.36      0.43       496
           3       0.85      0.82      0.84       895
           4       0.53      0.50      0.51       653
           5       0.68      0.86      0.76       415
           6       0.52      0.63      0.57       607

    accuracy                           0.63      3589
   macro avg       0.52      0.54      0.52      3589
weighted avg       0.62      0.63      0.62      3589

correct 0.6269155753691836 
f1 0.5231822296152397 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.73it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.32it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.89it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.83it/s]8it [00:01,  5.90it/s]                                 macro  0.5260232241002645
micro  0.6324881582613542
[[262   0  53  23  66  26  61]
 [ 21   0  14   6   8   2   4]
 [ 64   0 198  23  91  88  64]
 [ 17   0  24 754  27  18  39]
 [ 69   0  51  25 301  14 134]
 [  7   0  12  26  11 349  11]
 [ 43   0  27  35  94  21 406]]
              precision    recall  f1-score   support

           0       0.54      0.53      0.54       491
           1       0.00      0.00      0.00        55
           2       0.52      0.38      0.44       528
           3       0.85      0.86      0.85       879
           4       0.50      0.51      0.51       594
           5       0.67      0.84      0.75       416
           6       0.56      0.65      0.60       626

    accuracy                           0.63      3589
   macro avg       0.52      0.54      0.53      3589
weighted avg       0.62      0.63      0.62      3589

correct 0.6324881582613542 
f1 0.5260232241002645 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:38,  1.43it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.34it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.85it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.28it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.47it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.72it/s] 12%|█▏        | 7/56.072265625 [00:02<00:12,  3.79it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.94it/s] 16%|█▌        | 9/56.072265625 [00:02<00:11,  3.94it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.05it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  4.02it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.09it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  4.03it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.12it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.07it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.08it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  3.99it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.02it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  3.95it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.05it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  3.98it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.07it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.04it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.11it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.04it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.12it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.06it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.14it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.08it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.15it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.06it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.14it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.05it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.06it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.03it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.10it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.05it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.12it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  4.06it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.14it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.08it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.12it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  4.03it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.11it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.06it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.13it/s] 84%|████████▍ | 47/56.072265625 [00:11<00:02,  4.07it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.14it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.07it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.15it/s] 91%|█████████ | 51/56.072265625 [00:12<00:01,  4.10it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.17it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.11it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.18it/s] 98%|█████████▊| 55/56.072265625 [00:13<00:00,  4.12it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.18it/s]57it [00:14,  3.98it/s]                                  epoch 64 iter 0 sum correct 0.87890625 loss 1.8879386186599731
epoch 64 iter 1 sum correct 0.875 loss 1.8881678581237793
epoch 64 iter 2 sum correct 0.880859375 loss 1.8876341581344604
epoch 64 iter 3 sum correct 0.88330078125 loss 1.8875161409378052
epoch 64 iter 4 sum correct 0.88046875 loss 1.8877861499786377
epoch 64 iter 5 sum correct 0.8785807291666666 loss 1.8879032135009766
epoch 64 iter 6 sum correct 0.880859375 loss 1.8877121210098267
epoch 64 iter 7 sum correct 0.879150390625 loss 1.8878426551818848
epoch 64 iter 8 sum correct 0.8802083333333334 loss 1.88773775100708
epoch 64 iter 9 sum correct 0.8806640625 loss 1.8877063989639282
epoch 64 iter 10 sum correct 0.8813920454545454 loss 1.887622356414795
epoch 64 iter 11 sum correct 0.88134765625 loss 1.887626051902771
epoch 64 iter 12 sum correct 0.8819110576923077 loss 1.8875768184661865
epoch 64 iter 13 sum correct 0.8825334821428571 loss 1.8875231742858887
epoch 64 iter 14 sum correct 0.8833333333333333 loss 1.8874505758285522
epoch 64 iter 15 sum correct 0.8839111328125 loss 1.887391448020935
epoch 64 iter 16 sum correct 0.8832720588235294 loss 1.8874307870864868
epoch 64 iter 17 sum correct 0.8845486111111112 loss 1.8873285055160522
epoch 64 iter 18 sum correct 0.882092927631579 loss 1.887523889541626
epoch 64 iter 19 sum correct 0.88154296875 loss 1.8875675201416016
epoch 64 iter 20 sum correct 0.8818824404761905 loss 1.887542963027954
epoch 64 iter 21 sum correct 0.8820134943181818 loss 1.8875280618667603
epoch 64 iter 22 sum correct 0.8829823369565217 loss 1.8874468803405762
epoch 64 iter 23 sum correct 0.8837076822916666 loss 1.8873906135559082
epoch 64 iter 24 sum correct 0.8834375 loss 1.8874189853668213
epoch 64 iter 25 sum correct 0.8830378605769231 loss 1.8874436616897583
epoch 64 iter 26 sum correct 0.8830295138888888 loss 1.887441635131836
epoch 64 iter 27 sum correct 0.88330078125 loss 1.887420415878296
epoch 64 iter 28 sum correct 0.8821390086206896 loss 1.8875131607055664
epoch 64 iter 29 sum correct 0.8819010416666667 loss 1.8875315189361572
epoch 64 iter 30 sum correct 0.881804435483871 loss 1.8875356912612915
epoch 64 iter 31 sum correct 0.88189697265625 loss 1.8875348567962646
epoch 64 iter 32 sum correct 0.8815696022727273 loss 1.8875521421432495
epoch 64 iter 33 sum correct 0.8816061580882353 loss 1.887542963027954
epoch 64 iter 34 sum correct 0.8814174107142857 loss 1.8875560760498047
epoch 64 iter 35 sum correct 0.8809678819444444 loss 1.8875876665115356
epoch 64 iter 36 sum correct 0.881862331081081 loss 1.8875153064727783
epoch 64 iter 37 sum correct 0.8818873355263158 loss 1.8875113725662231
epoch 64 iter 38 sum correct 0.8820612980769231 loss 1.887500286102295
epoch 64 iter 39 sum correct 0.881591796875 loss 1.8875341415405273
epoch 64 iter 40 sum correct 0.8820979420731707 loss 1.8874953985214233
epoch 64 iter 41 sum correct 0.8818359375 loss 1.887516736984253
epoch 64 iter 42 sum correct 0.8815406976744186 loss 1.8875386714935303
epoch 64 iter 43 sum correct 0.8812588778409091 loss 1.8875609636306763
epoch 64 iter 44 sum correct 0.8809027777777778 loss 1.8875880241394043
epoch 64 iter 45 sum correct 0.8807744565217391 loss 1.887596845626831
epoch 64 iter 46 sum correct 0.8810255984042553 loss 1.887574315071106
epoch 64 iter 47 sum correct 0.880859375 loss 1.8875821828842163
epoch 64 iter 48 sum correct 0.8808195153061225 loss 1.8875826597213745
epoch 64 iter 49 sum correct 0.881328125 loss 1.8875391483306885
epoch 64 iter 50 sum correct 0.8814721200980392 loss 1.8875277042388916
epoch 64 iter 51 sum correct 0.8818359375 loss 1.8875000476837158
epoch 64 iter 52 sum correct 0.8819649174528302 loss 1.8874894380569458
epoch 64 iter 53 sum correct 0.8819444444444444 loss 1.8874905109405518
epoch 64 iter 54 sum correct 0.8820667613636364 loss 1.8874783515930176
epoch 64 iter 55 sum correct 0.8822195870535714 loss 1.8874667882919312
epoch 64 iter 56 sum correct 0.8675986842105263 loss 1.887742042541504
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.76it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.46it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.83it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.05it/s]8it [00:01,  6.01it/s]                                 macro  0.5287611444090752
micro  0.6283087210922262
[[310   0  41  28  46  12  30]
 [ 27   0  10   6  10   0   3]
 [ 85   0 222  15  93  48  33]
 [ 35   0  32 751  25  20  32]
 [106   0  90  33 324  19  81]
 [ 17   0  28  18   6 340   6]
 [ 91   0  54  53  89  12 308]]
              precision    recall  f1-score   support

           0       0.46      0.66      0.54       467
           1       0.00      0.00      0.00        56
           2       0.47      0.45      0.46       496
           3       0.83      0.84      0.83       895
           4       0.55      0.50      0.52       653
           5       0.75      0.82      0.79       415
           6       0.62      0.51      0.56       607

    accuracy                           0.63      3589
   macro avg       0.53      0.54      0.53      3589
weighted avg       0.62      0.63      0.62      3589

correct 0.6283087210922262 
f1 0.5287611444090752 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.79it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.35it/s] 57%|█████▋    | 4/7.009765625 [00:00<00:00,  5.41it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  6.04it/s] 86%|████████▌ | 6/7.009765625 [00:01<00:00,  6.92it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.17it/s]8it [00:01,  5.87it/s]                                 macro  0.5334139315451877
micro  0.6363889662858735
[[327   0  51  21  52  11  29]
 [ 22   0   9   7  13   2   2]
 [ 99   0 232  23  87  51  36]
 [ 31   0  18 778  18  16  18]
 [102   0  74  39 290  12  77]
 [ 18   0  31  17  12 331   7]
 [ 93   0  46  48  94  19 326]]
              precision    recall  f1-score   support

           0       0.47      0.67      0.55       491
           1       0.00      0.00      0.00        55
           2       0.50      0.44      0.47       528
           3       0.83      0.89      0.86       879
           4       0.51      0.49      0.50       594
           5       0.75      0.80      0.77       416
           6       0.66      0.52      0.58       626

    accuracy                           0.64      3589
   macro avg       0.53      0.54      0.53      3589
weighted avg       0.63      0.64      0.63      3589

correct 0.6363889662858735 
f1 0.5334139315451877 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:38,  1.43it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.31it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.81it/s]  7%|▋         | 4/56.072265625 [00:01<00:16,  3.25it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.45it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.71it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.75it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.92it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.89it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  3.99it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.95it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.05it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  4.00it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.08it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.01it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.10it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.02it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.10it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.01it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.09it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.03it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.11it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.06it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.13it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.06it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  3.83it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  3.87it/s] 50%|████▉     | 28/56.072265625 [00:07<00:07,  3.98it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  3.96it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.06it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.01it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.10it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  3.97it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.02it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  3.95it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.05it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.02it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.09it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  4.05it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.12it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.06it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.13it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  4.08it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.15it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.06it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.13it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  3.96it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.05it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.01it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.10it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  4.06it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.13it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.08it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.15it/s] 98%|█████████▊| 55/56.072265625 [00:14<00:00,  4.06it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.12it/s]57it [00:14,  4.90it/s]                                  57it [00:14,  3.94it/s]epoch 65 iter 0 sum correct 0.89453125 loss 1.8864399194717407
epoch 65 iter 1 sum correct 0.8955078125 loss 1.886372685432434
epoch 65 iter 2 sum correct 0.89453125 loss 1.8865337371826172
epoch 65 iter 3 sum correct 0.89111328125 loss 1.8867977857589722
epoch 65 iter 4 sum correct 0.886328125 loss 1.8872263431549072
epoch 65 iter 5 sum correct 0.8831380208333334 loss 1.8874953985214233
epoch 65 iter 6 sum correct 0.880859375 loss 1.8876606225967407
epoch 65 iter 7 sum correct 0.87939453125 loss 1.8877736330032349
epoch 65 iter 8 sum correct 0.8791232638888888 loss 1.8878018856048584
epoch 65 iter 9 sum correct 0.8763671875 loss 1.8880475759506226
epoch 65 iter 10 sum correct 0.8734019886363636 loss 1.8882755041122437
epoch 65 iter 11 sum correct 0.8719075520833334 loss 1.888392448425293
epoch 65 iter 12 sum correct 0.8706430288461539 loss 1.888492465019226
epoch 65 iter 13 sum correct 0.8705357142857143 loss 1.888502597808838
epoch 65 iter 14 sum correct 0.8692708333333333 loss 1.8885877132415771
epoch 65 iter 15 sum correct 0.8692626953125 loss 1.8885945081710815
epoch 65 iter 16 sum correct 0.8701746323529411 loss 1.8885223865509033
epoch 65 iter 17 sum correct 0.8713107638888888 loss 1.8884245157241821
epoch 65 iter 18 sum correct 0.8707853618421053 loss 1.8884693384170532
epoch 65 iter 19 sum correct 0.86904296875 loss 1.8886116743087769
epoch 65 iter 20 sum correct 0.8687686011904762 loss 1.8886281251907349
epoch 65 iter 21 sum correct 0.8682528409090909 loss 1.8886595964431763
epoch 65 iter 22 sum correct 0.8683763586956522 loss 1.8886528015136719
epoch 65 iter 23 sum correct 0.869140625 loss 1.8885862827301025
epoch 65 iter 24 sum correct 0.87 loss 1.888521432876587
epoch 65 iter 25 sum correct 0.8690655048076923 loss 1.8885912895202637
epoch 65 iter 26 sum correct 0.8697916666666666 loss 1.8885282278060913
epoch 65 iter 27 sum correct 0.8684430803571429 loss 1.8886263370513916
epoch 65 iter 28 sum correct 0.8689385775862069 loss 1.8885771036148071
epoch 65 iter 29 sum correct 0.8695963541666667 loss 1.8885252475738525
epoch 65 iter 30 sum correct 0.8694556451612904 loss 1.8885369300842285
epoch 65 iter 31 sum correct 0.86956787109375 loss 1.888530969619751
epoch 65 iter 32 sum correct 0.8695549242424242 loss 1.8885364532470703
epoch 65 iter 33 sum correct 0.8698874080882353 loss 1.8885020017623901
epoch 65 iter 34 sum correct 0.8698102678571429 loss 1.8885048627853394
epoch 65 iter 35 sum correct 0.86962890625 loss 1.8885258436203003
epoch 65 iter 36 sum correct 0.8698268581081081 loss 1.8885149955749512
epoch 65 iter 37 sum correct 0.8697060032894737 loss 1.8885362148284912
epoch 65 iter 38 sum correct 0.8698918269230769 loss 1.8885223865509033
epoch 65 iter 39 sum correct 0.870263671875 loss 1.888490915298462
epoch 65 iter 40 sum correct 0.8703791920731707 loss 1.8884795904159546
epoch 65 iter 41 sum correct 0.8713727678571429 loss 1.8883986473083496
epoch 65 iter 42 sum correct 0.8717750726744186 loss 1.8883651494979858
epoch 65 iter 43 sum correct 0.87158203125 loss 1.8883781433105469
epoch 65 iter 44 sum correct 0.8717881944444444 loss 1.88835608959198
epoch 65 iter 45 sum correct 0.8719429347826086 loss 1.888338327407837
epoch 65 iter 46 sum correct 0.872298869680851 loss 1.8883076906204224
epoch 65 iter 47 sum correct 0.8722330729166666 loss 1.8883050680160522
epoch 65 iter 48 sum correct 0.8720503826530612 loss 1.8883191347122192
epoch 65 iter 49 sum correct 0.8721484375 loss 1.8883123397827148
epoch 65 iter 50 sum correct 0.8720128676470589 loss 1.888319969177246
epoch 65 iter 51 sum correct 0.8718449519230769 loss 1.8883305788040161
epoch 65 iter 52 sum correct 0.8723466981132075 loss 1.8882896900177002
epoch 65 iter 53 sum correct 0.8726128472222222 loss 1.8882719278335571
epoch 65 iter 54 sum correct 0.8721590909090909 loss 1.8883076906204224
epoch 65 iter 55 sum correct 0.8724190848214286 loss 1.8882859945297241
epoch 65 iter 56 sum correct 0.8580386513157895 loss 1.8884849548339844
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.66it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.30it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.44it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.74it/s]8it [00:01,  5.77it/s]                                 macro  0.4970832574154086
micro  0.5945945945945946
[[324   0  28  26  28  10  51]
 [ 38   0   3   5   2   1   7]
 [128   0 207  11  44  48  58]
 [ 53   0  21 717  18  19  67]
 [145   0 110  26 196  15 161]
 [ 48   0  21  14   5 317  10]
 [ 81   0  47  34  50  22 373]]
              precision    recall  f1-score   support

           0       0.40      0.69      0.50       467
           1       0.00      0.00      0.00        56
           2       0.47      0.42      0.44       496
           3       0.86      0.80      0.83       895
           4       0.57      0.30      0.39       653
           5       0.73      0.76      0.75       415
           6       0.51      0.61      0.56       607

    accuracy                           0.59      3589
   macro avg       0.51      0.51      0.50      3589
weighted avg       0.61      0.59      0.59      3589

correct 0.5945945945945946 
f1 0.4970832574154086 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.86it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.49it/s] 71%|███████▏  | 5/7.009765625 [00:00<00:00,  6.07it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.09it/s]8it [00:01,  6.07it/s]                                 macro  0.5076120054255999
micro  0.6107550849818891
[[332   0  43  10  29   5  72]
 [ 35   0  11   5   0   2   2]
 [133   0 212  17  48  55  63]
 [ 44   0  13 736  20  18  48]
 [124   0  86  24 188  11 161]
 [ 39   0  33  19   5 307  13]
 [ 81   0  45  28  36  19 417]]
              precision    recall  f1-score   support

           0       0.42      0.68      0.52       491
           1       0.00      0.00      0.00        55
           2       0.48      0.40      0.44       528
           3       0.88      0.84      0.86       879
           4       0.58      0.32      0.41       594
           5       0.74      0.74      0.74       416
           6       0.54      0.67      0.59       626

    accuracy                           0.61      3589
   macro avg       0.52      0.52      0.51      3589
weighted avg       0.62      0.61      0.60      3589

correct 0.6107550849818891 
f1 0.5076120054255999 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:38,  1.41it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.29it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.81it/s]  7%|▋         | 4/56.072265625 [00:01<00:16,  3.21it/s]  9%|▉         | 5/56.072265625 [00:01<00:15,  3.39it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.64it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.69it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.87it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.87it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.00it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.98it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.05it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  4.00it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.09it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.05it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.12it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.04it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.11it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.04it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.11it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.04it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.11it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.05it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.12it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.06it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.13it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.04it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.12it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.06it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.13it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.07it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.08it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  3.98it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.02it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  3.94it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.04it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.01it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.06it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  4.00it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.09it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.02it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.11it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  4.06it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.14it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.04it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.11it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  4.06it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.14it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.05it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.13it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  4.08it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.15it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.08it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.16it/s] 98%|█████████▊| 55/56.072265625 [00:13<00:00,  4.09it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.16it/s]57it [00:14,  3.96it/s]                                  epoch 66 iter 0 sum correct 0.890625 loss 1.8868576288223267
epoch 66 iter 1 sum correct 0.890625 loss 1.88677978515625
epoch 66 iter 2 sum correct 0.876953125 loss 1.887850284576416
epoch 66 iter 3 sum correct 0.87060546875 loss 1.8883416652679443
epoch 66 iter 4 sum correct 0.863671875 loss 1.8888877630233765
epoch 66 iter 5 sum correct 0.8548177083333334 loss 1.8896297216415405
epoch 66 iter 6 sum correct 0.8510044642857143 loss 1.8899321556091309
epoch 66 iter 7 sum correct 0.842529296875 loss 1.8905982971191406
epoch 66 iter 8 sum correct 0.837890625 loss 1.8909833431243896
epoch 66 iter 9 sum correct 0.8349609375 loss 1.8912460803985596
epoch 66 iter 10 sum correct 0.8322088068181818 loss 1.8914843797683716
epoch 66 iter 11 sum correct 0.8268229166666666 loss 1.8919075727462769
epoch 66 iter 12 sum correct 0.8233173076923077 loss 1.8921654224395752
epoch 66 iter 13 sum correct 0.8212890625 loss 1.8923444747924805
epoch 66 iter 14 sum correct 0.8205729166666667 loss 1.892396330833435
epoch 66 iter 15 sum correct 0.8204345703125 loss 1.8924109935760498
epoch 66 iter 16 sum correct 0.8205422794117647 loss 1.892419695854187
epoch 66 iter 17 sum correct 0.8182508680555556 loss 1.8925999402999878
epoch 66 iter 18 sum correct 0.8172286184210527 loss 1.8926819562911987
epoch 66 iter 19 sum correct 0.81640625 loss 1.8927472829818726
epoch 66 iter 20 sum correct 0.8165922619047619 loss 1.8927299976348877
epoch 66 iter 21 sum correct 0.8162286931818182 loss 1.8927420377731323
epoch 66 iter 22 sum correct 0.817000679347826 loss 1.8926784992218018
epoch 66 iter 23 sum correct 0.8173828125 loss 1.892651081085205
epoch 66 iter 24 sum correct 0.817734375 loss 1.892627239227295
epoch 66 iter 25 sum correct 0.8184344951923077 loss 1.8925739526748657
epoch 66 iter 26 sum correct 0.8186487268518519 loss 1.89255952835083
epoch 66 iter 27 sum correct 0.81884765625 loss 1.8925378322601318
epoch 66 iter 28 sum correct 0.8176858836206896 loss 1.8926280736923218
epoch 66 iter 29 sum correct 0.8176432291666667 loss 1.892638921737671
epoch 66 iter 30 sum correct 0.8187373991935484 loss 1.89254629611969
epoch 66 iter 31 sum correct 0.81915283203125 loss 1.8925127983093262
epoch 66 iter 32 sum correct 0.8188328598484849 loss 1.8925479650497437
epoch 66 iter 33 sum correct 0.8192210477941176 loss 1.8925166130065918
epoch 66 iter 34 sum correct 0.8198660714285714 loss 1.8924658298492432
epoch 66 iter 35 sum correct 0.8205837673611112 loss 1.8924051523208618
epoch 66 iter 36 sum correct 0.8210515202702703 loss 1.8923659324645996
epoch 66 iter 37 sum correct 0.8213918585526315 loss 1.892331838607788
epoch 66 iter 38 sum correct 0.8221153846153846 loss 1.8922747373580933
epoch 66 iter 39 sum correct 0.822412109375 loss 1.8922433853149414
epoch 66 iter 40 sum correct 0.8241711128048781 loss 1.892106294631958
epoch 66 iter 41 sum correct 0.8245442708333334 loss 1.892069935798645
epoch 66 iter 42 sum correct 0.8255359738372093 loss 1.8919938802719116
epoch 66 iter 43 sum correct 0.8267933238636364 loss 1.8918955326080322
epoch 66 iter 44 sum correct 0.8272569444444444 loss 1.8918585777282715
epoch 66 iter 45 sum correct 0.8273182744565217 loss 1.8918596506118774
epoch 66 iter 46 sum correct 0.8281665558510638 loss 1.8917897939682007
epoch 66 iter 47 sum correct 0.8285319010416666 loss 1.8917587995529175
epoch 66 iter 48 sum correct 0.8287627551020408 loss 1.891739010810852
epoch 66 iter 49 sum correct 0.829453125 loss 1.8916879892349243
epoch 66 iter 50 sum correct 0.8300398284313726 loss 1.8916380405426025
epoch 66 iter 51 sum correct 0.8302659254807693 loss 1.8916184902191162
epoch 66 iter 52 sum correct 0.8307414504716981 loss 1.891584873199463
epoch 66 iter 53 sum correct 0.8306206597222222 loss 1.8915971517562866
epoch 66 iter 54 sum correct 0.8306463068181819 loss 1.8915977478027344
epoch 66 iter 55 sum correct 0.830810546875 loss 1.8915876150131226
epoch 66 iter 56 sum correct 0.8171943530701754 loss 1.8916823863983154
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.61it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.08it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.20it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.59it/s]8it [00:01,  5.56it/s]                                 macro  0.5185524208264094
micro  0.619671217609362
[[294   0  36  36  61   6  34]
 [ 30   0   8   6   9   0   3]
 [ 81   0 204  34  89  49  39]
 [ 30   0   9 792  19  13  32]
 [ 83   0 107  49 320   9  85]
 [ 23   0  22  24  10 326  10]
 [ 64   0  57  85 110   3 288]]
              precision    recall  f1-score   support

           0       0.49      0.63      0.55       467
           1       0.00      0.00      0.00        56
           2       0.46      0.41      0.43       496
           3       0.77      0.88      0.82       895
           4       0.52      0.49      0.50       653
           5       0.80      0.79      0.79       415
           6       0.59      0.47      0.52       607

    accuracy                           0.62      3589
   macro avg       0.52      0.53      0.52      3589
weighted avg       0.61      0.62      0.61      3589

correct 0.619671217609362 
f1 0.5185524208264094 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.70it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.24it/s] 57%|█████▋    | 4/7.009765625 [00:00<00:00,  5.27it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  6.07it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.55it/s]8it [00:01,  5.99it/s]                                 macro  0.5198796700618284
micro  0.6227361382000557
[[301   0  44  33  69   5  39]
 [ 24   0  11   8   9   2   1]
 [101   0 227  39  80  51  30]
 [ 22   0  14 803  16  11  13]
 [ 79   0  93  62 266   7  87]
 [ 25   0  27  33   8 311  12]
 [ 63   0  49  72 105  10 327]]
              precision    recall  f1-score   support

           0       0.49      0.61      0.54       491
           1       0.00      0.00      0.00        55
           2       0.49      0.43      0.46       528
           3       0.76      0.91      0.83       879
           4       0.48      0.45      0.46       594
           5       0.78      0.75      0.77       416
           6       0.64      0.52      0.58       626

    accuracy                           0.62      3589
   macro avg       0.52      0.52      0.52      3589
weighted avg       0.61      0.62      0.61      3589

correct 0.6227361382000557 
f1 0.5198796700618284 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:38,  1.45it/s]  4%|▎         | 2/56.072265625 [00:00<00:22,  2.37it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.88it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.32it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.49it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.71it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.77it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.93it/s] 16%|█▌        | 9/56.072265625 [00:02<00:11,  3.93it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.04it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  4.01it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.09it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  4.02it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.11it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.06it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.13it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.04it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.11it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.04it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.11it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.06it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.13it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.05it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.13it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.05it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.13it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.05it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.12it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.05it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.12it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.06it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.14it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.07it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.14it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.07it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.14it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.08it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.14it/s] 70%|██████▉   | 39/56.072265625 [00:09<00:04,  4.07it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.14it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.04it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.12it/s] 77%|███████▋  | 43/56.072265625 [00:10<00:03,  4.04it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.10it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.06it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.13it/s] 84%|████████▍ | 47/56.072265625 [00:11<00:02,  4.07it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.14it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.06it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.14it/s] 91%|█████████ | 51/56.072265625 [00:12<00:01,  4.09it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:01,  3.95it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  3.96it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.06it/s] 98%|█████████▊| 55/56.072265625 [00:13<00:00,  4.04it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.11it/s]57it [00:14,  4.99it/s]                                  57it [00:14,  3.98it/s]epoch 67 iter 0 sum correct 0.90625 loss 1.885733723640442
epoch 67 iter 1 sum correct 0.8837890625 loss 1.8874289989471436
epoch 67 iter 2 sum correct 0.8815104166666666 loss 1.8877218961715698
epoch 67 iter 3 sum correct 0.873046875 loss 1.8883929252624512
epoch 67 iter 4 sum correct 0.86875 loss 1.8887426853179932
epoch 67 iter 5 sum correct 0.8681640625 loss 1.8887708187103271
epoch 67 iter 6 sum correct 0.869140625 loss 1.8886445760726929
epoch 67 iter 7 sum correct 0.868896484375 loss 1.888644814491272
epoch 67 iter 8 sum correct 0.8697916666666666 loss 1.8885732889175415
epoch 67 iter 9 sum correct 0.8669921875 loss 1.8887989521026611
epoch 67 iter 10 sum correct 0.8677201704545454 loss 1.888727068901062
epoch 67 iter 11 sum correct 0.8671875 loss 1.8887605667114258
epoch 67 iter 12 sum correct 0.8682391826923077 loss 1.888682246208191
epoch 67 iter 13 sum correct 0.8680245535714286 loss 1.8886786699295044
epoch 67 iter 14 sum correct 0.8696614583333333 loss 1.888533115386963
epoch 67 iter 15 sum correct 0.8697509765625 loss 1.8885074853897095
epoch 67 iter 16 sum correct 0.8708639705882353 loss 1.888413429260254
epoch 67 iter 17 sum correct 0.8705512152777778 loss 1.8884410858154297
epoch 67 iter 18 sum correct 0.8707853618421053 loss 1.888428807258606
epoch 67 iter 19 sum correct 0.8708984375 loss 1.888429045677185
epoch 67 iter 20 sum correct 0.8701636904761905 loss 1.8884824514389038
epoch 67 iter 21 sum correct 0.8711825284090909 loss 1.888392448425293
epoch 67 iter 22 sum correct 0.8720278532608695 loss 1.8883219957351685
epoch 67 iter 23 sum correct 0.8715006510416666 loss 1.8883699178695679
epoch 67 iter 24 sum correct 0.873203125 loss 1.8882296085357666
epoch 67 iter 25 sum correct 0.8731971153846154 loss 1.8882297277450562
epoch 67 iter 26 sum correct 0.8735532407407407 loss 1.8881957530975342
epoch 67 iter 27 sum correct 0.8741629464285714 loss 1.8881416320800781
epoch 67 iter 28 sum correct 0.8746632543103449 loss 1.8881072998046875
epoch 67 iter 29 sum correct 0.8736979166666666 loss 1.8881820440292358
epoch 67 iter 30 sum correct 0.8736769153225806 loss 1.888177752494812
epoch 67 iter 31 sum correct 0.87310791015625 loss 1.8882173299789429
epoch 67 iter 32 sum correct 0.8729876893939394 loss 1.8882206678390503
epoch 67 iter 33 sum correct 0.8725873161764706 loss 1.8882497549057007
epoch 67 iter 34 sum correct 0.8735491071428572 loss 1.8881717920303345
epoch 67 iter 35 sum correct 0.8743489583333334 loss 1.8881088495254517
epoch 67 iter 36 sum correct 0.8738386824324325 loss 1.888150691986084
epoch 67 iter 37 sum correct 0.874743009868421 loss 1.8880749940872192
epoch 67 iter 38 sum correct 0.8747996794871795 loss 1.88807213306427
epoch 67 iter 39 sum correct 0.8751953125 loss 1.8880451917648315
epoch 67 iter 40 sum correct 0.8749523628048781 loss 1.888066053390503
epoch 67 iter 41 sum correct 0.8751395089285714 loss 1.8880510330200195
epoch 67 iter 42 sum correct 0.8752271075581395 loss 1.88804292678833
epoch 67 iter 43 sum correct 0.8752663352272727 loss 1.888043761253357
epoch 67 iter 44 sum correct 0.8756076388888889 loss 1.8880168199539185
epoch 67 iter 45 sum correct 0.8756793478260869 loss 1.888012409210205
epoch 67 iter 46 sum correct 0.8761635638297872 loss 1.8879696130752563
epoch 67 iter 47 sum correct 0.875732421875 loss 1.888010025024414
epoch 67 iter 48 sum correct 0.8762356505102041 loss 1.887968897819519
epoch 67 iter 49 sum correct 0.8762109375 loss 1.8879737854003906
epoch 67 iter 50 sum correct 0.8763020833333334 loss 1.8879646062850952
epoch 67 iter 51 sum correct 0.8763897235576923 loss 1.8879600763320923
epoch 67 iter 52 sum correct 0.8764003537735849 loss 1.887953758239746
epoch 67 iter 53 sum correct 0.8767361111111112 loss 1.8879276514053345
epoch 67 iter 54 sum correct 0.8767045454545455 loss 1.887929916381836
epoch 67 iter 55 sum correct 0.8768484933035714 loss 1.8879162073135376
epoch 67 iter 56 sum correct 0.8624931469298246 loss 1.8880075216293335
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.63it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.12it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.64it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.99it/s]8it [00:01,  5.78it/s]                                 macro  0.5267215346625347
micro  0.6330454165505712
[[272   0  31  32  64  23  45]
 [ 30   0   5   4   9   2   6]
 [ 70   0 172  20 109  72  53]
 [ 19   0   9 759  30  22  56]
 [ 60   0  73  37 345  20 118]
 [  8   0  13  19  11 355   9]
 [ 49   0  22  51 101  15 369]]
              precision    recall  f1-score   support

           0       0.54      0.58      0.56       467
           1       0.00      0.00      0.00        56
           2       0.53      0.35      0.42       496
           3       0.82      0.85      0.84       895
           4       0.52      0.53      0.52       653
           5       0.70      0.86      0.77       415
           6       0.56      0.61      0.58       607

    accuracy                           0.63      3589
   macro avg       0.52      0.54      0.53      3589
weighted avg       0.62      0.63      0.62      3589

correct 0.6330454165505712 
f1 0.5267215346625347 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.81it/s] 29%|██▊       | 2/7.009765625 [00:00<00:01,  3.48it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.68it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  6.39it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.61it/s]8it [00:01,  6.11it/s]                                 macro  0.5370830399367924
micro  0.6466982446363889
[[296   0  23  19  72  21  60]
 [ 27   0   9   6   7   3   3]
 [ 88   0 190  30  93  79  48]
 [ 18   0   3 771  35  27  25]
 [ 68   0  50  43 298  14 121]
 [  6   0  15  23   9 348  15]
 [ 35   0  35  42  75  21 418]]
              precision    recall  f1-score   support

           0       0.55      0.60      0.58       491
           1       0.00      0.00      0.00        55
           2       0.58      0.36      0.45       528
           3       0.83      0.88      0.85       879
           4       0.51      0.50      0.50       594
           5       0.68      0.84      0.75       416
           6       0.61      0.67      0.64       626

    accuracy                           0.65      3589
   macro avg       0.54      0.55      0.54      3589
weighted avg       0.63      0.65      0.63      3589

correct 0.6466982446363889 
f1 0.5370830399367924 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:39,  1.40it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.32it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.81it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.26it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.46it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.70it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.74it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.91it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.89it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  3.99it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.94it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.05it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  3.99it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.08it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  3.99it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.09it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.02it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.10it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.04it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.11it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.02it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.10it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.00it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.10it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.02it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.10it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.04it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.12it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.04it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.12it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.05it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.12it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.03it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.11it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.05it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.12it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.03it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.11it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  4.05it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.12it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.04it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.12it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  4.03it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.11it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.05it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.12it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  4.04it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.11it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.04it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.13it/s] 91%|█████████ | 51/56.072265625 [00:12<00:01,  4.07it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.14it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.07it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.14it/s] 98%|█████████▊| 55/56.072265625 [00:13<00:00,  4.07it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.13it/s]57it [00:14,  4.98it/s]                                  57it [00:14,  3.96it/s]epoch 68 iter 0 sum correct 0.88671875 loss 1.887230396270752
epoch 68 iter 1 sum correct 0.8896484375 loss 1.8868415355682373
epoch 68 iter 2 sum correct 0.8919270833333334 loss 1.886627197265625
epoch 68 iter 3 sum correct 0.8916015625 loss 1.8867053985595703
epoch 68 iter 4 sum correct 0.888671875 loss 1.8869510889053345
epoch 68 iter 5 sum correct 0.8922526041666666 loss 1.8866361379623413
epoch 68 iter 6 sum correct 0.8903459821428571 loss 1.8867809772491455
epoch 68 iter 7 sum correct 0.890869140625 loss 1.8867398500442505
epoch 68 iter 8 sum correct 0.8921440972222222 loss 1.886670708656311
epoch 68 iter 9 sum correct 0.890234375 loss 1.886826515197754
epoch 68 iter 10 sum correct 0.8895596590909091 loss 1.8868917226791382
epoch 68 iter 11 sum correct 0.8899739583333334 loss 1.8868564367294312
epoch 68 iter 12 sum correct 0.8892728365384616 loss 1.8869142532348633
epoch 68 iter 13 sum correct 0.8893694196428571 loss 1.886911153793335
epoch 68 iter 14 sum correct 0.8889322916666667 loss 1.8869431018829346
epoch 68 iter 15 sum correct 0.8887939453125 loss 1.8869502544403076
epoch 68 iter 16 sum correct 0.8890165441176471 loss 1.886932134628296
epoch 68 iter 17 sum correct 0.8883463541666666 loss 1.8869739770889282
epoch 68 iter 18 sum correct 0.8882606907894737 loss 1.8869822025299072
epoch 68 iter 19 sum correct 0.8888671875 loss 1.88693368434906
epoch 68 iter 20 sum correct 0.8889508928571429 loss 1.886932373046875
epoch 68 iter 21 sum correct 0.8883167613636364 loss 1.8869868516921997
epoch 68 iter 22 sum correct 0.8887567934782609 loss 1.886952519416809
epoch 68 iter 23 sum correct 0.887939453125 loss 1.887010931968689
epoch 68 iter 24 sum correct 0.888671875 loss 1.8869521617889404
epoch 68 iter 25 sum correct 0.8893479567307693 loss 1.8868968486785889
epoch 68 iter 26 sum correct 0.8888888888888888 loss 1.8869266510009766
epoch 68 iter 27 sum correct 0.89013671875 loss 1.886830449104309
epoch 68 iter 28 sum correct 0.8898841594827587 loss 1.8868544101715088
epoch 68 iter 29 sum correct 0.8904947916666667 loss 1.8868106603622437
epoch 68 iter 30 sum correct 0.8902469758064516 loss 1.886823296546936
epoch 68 iter 31 sum correct 0.890380859375 loss 1.8868143558502197
epoch 68 iter 32 sum correct 0.8909801136363636 loss 1.8867690563201904
epoch 68 iter 33 sum correct 0.8909122242647058 loss 1.8867710828781128
epoch 68 iter 34 sum correct 0.8911272321428572 loss 1.8867560625076294
epoch 68 iter 35 sum correct 0.8911675347222222 loss 1.886749029159546
epoch 68 iter 36 sum correct 0.8917335304054054 loss 1.8867073059082031
epoch 68 iter 37 sum correct 0.892321134868421 loss 1.8866561651229858
epoch 68 iter 38 sum correct 0.8921774839743589 loss 1.8866641521453857
epoch 68 iter 39 sum correct 0.891943359375 loss 1.8866831064224243
epoch 68 iter 40 sum correct 0.891577743902439 loss 1.8867101669311523
epoch 68 iter 41 sum correct 0.8915085565476191 loss 1.8867107629776
epoch 68 iter 42 sum correct 0.8911700581395349 loss 1.8867348432540894
epoch 68 iter 43 sum correct 0.8908913352272727 loss 1.8867571353912354
epoch 68 iter 44 sum correct 0.8911892361111111 loss 1.8867343664169312
epoch 68 iter 45 sum correct 0.8912618885869565 loss 1.8867254257202148
epoch 68 iter 46 sum correct 0.8913314494680851 loss 1.886717677116394
epoch 68 iter 47 sum correct 0.8916422526041666 loss 1.8866949081420898
epoch 68 iter 48 sum correct 0.8909040178571429 loss 1.886754035949707
epoch 68 iter 49 sum correct 0.891015625 loss 1.8867433071136475
epoch 68 iter 50 sum correct 0.890625 loss 1.8867732286453247
epoch 68 iter 51 sum correct 0.8909254807692307 loss 1.8867499828338623
epoch 68 iter 52 sum correct 0.8908092570754716 loss 1.8867576122283936
epoch 68 iter 53 sum correct 0.8908781828703703 loss 1.8867536783218384
epoch 68 iter 54 sum correct 0.8907315340909091 loss 1.8867666721343994
epoch 68 iter 55 sum correct 0.8909388950892857 loss 1.8867473602294922
epoch 68 iter 56 sum correct 0.8763363486842105 loss 1.886857509613037
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.65it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.27it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.37it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.57it/s]8it [00:01,  5.62it/s]                                 macro  0.5362480727361661
micro  0.6388966285873502
[[281   0  44  30  46  18  48]
 [ 33   0   6   3   8   0   6]
 [ 68   0 223  14  62  64  65]
 [ 27   0  13 756  25  18  56]
 [ 79   0  80  27 293  18 156]
 [  4   0  16  23   9 351  12]
 [ 56   0  25  55  71  11 389]]
              precision    recall  f1-score   support

           0       0.51      0.60      0.55       467
           1       0.00      0.00      0.00        56
           2       0.55      0.45      0.49       496
           3       0.83      0.84      0.84       895
           4       0.57      0.45      0.50       653
           5       0.73      0.85      0.78       415
           6       0.53      0.64      0.58       607

    accuracy                           0.64      3589
   macro avg       0.53      0.55      0.54      3589
weighted avg       0.63      0.64      0.63      3589

correct 0.6388966285873502 
f1 0.5362480727361661 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.76it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.43it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.55it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.68it/s]8it [00:01,  5.79it/s]                                 macro  0.5275494365147027
micro  0.6349958205628309
[[290   0  52  24  49  19  57]
 [ 31   0   9   4   6   2   3]
 [ 80   0 222  23  70  75  58]
 [ 26   0  12 763  25  25  28]
 [ 91   0  69  35 240  12 147]
 [ 10   0  21  29   7 335  14]
 [ 49   0  27  38  65  18 429]]
              precision    recall  f1-score   support

           0       0.50      0.59      0.54       491
           1       0.00      0.00      0.00        55
           2       0.54      0.42      0.47       528
           3       0.83      0.87      0.85       879
           4       0.52      0.40      0.45       594
           5       0.69      0.81      0.74       416
           6       0.58      0.69      0.63       626

    accuracy                           0.63      3589
   macro avg       0.52      0.54      0.53      3589
weighted avg       0.62      0.63      0.62      3589

correct 0.6349958205628309 
f1 0.5275494365147027 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:39,  1.39it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.28it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.80it/s]  7%|▋         | 4/56.072265625 [00:01<00:16,  3.24it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.44it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.70it/s] 12%|█▏        | 7/56.072265625 [00:02<00:12,  3.78it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.94it/s] 16%|█▌        | 9/56.072265625 [00:02<00:11,  3.93it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.04it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  4.01it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.10it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  4.07it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.14it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.07it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.08it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  3.96it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.05it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.01it/s] 36%|███▌      | 20/56.072265625 [00:05<00:09,  4.01it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  3.93it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.03it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.00it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.09it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  3.95it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.05it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.01it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.02it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  3.98it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.07it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.03it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.04it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  3.90it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  3.96it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  3.90it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:05,  3.96it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  3.89it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  3.98it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  3.93it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:04,  3.97it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  3.90it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  3.96it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  3.89it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:03,  3.95it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  3.89it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  3.94it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  3.88it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:02,  3.94it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  3.86it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  3.96it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  3.97it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:01,  4.06it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.02it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.11it/s] 98%|█████████▊| 55/56.072265625 [00:14<00:00,  4.07it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.14it/s]57it [00:14,  4.94it/s]                                  57it [00:14,  3.90it/s]epoch 69 iter 0 sum correct 0.91015625 loss 1.8854113817214966
epoch 69 iter 1 sum correct 0.892578125 loss 1.886728048324585
epoch 69 iter 2 sum correct 0.89453125 loss 1.8865594863891602
epoch 69 iter 3 sum correct 0.892578125 loss 1.8867107629776
epoch 69 iter 4 sum correct 0.891796875 loss 1.8867946863174438
epoch 69 iter 5 sum correct 0.8909505208333334 loss 1.8868422508239746
epoch 69 iter 6 sum correct 0.8922991071428571 loss 1.8867199420928955
epoch 69 iter 7 sum correct 0.89306640625 loss 1.8866679668426514
epoch 69 iter 8 sum correct 0.8934461805555556 loss 1.8866195678710938
epoch 69 iter 9 sum correct 0.8939453125 loss 1.8865588903427124
epoch 69 iter 10 sum correct 0.8955965909090909 loss 1.8864203691482544
epoch 69 iter 11 sum correct 0.8966471354166666 loss 1.8863314390182495
epoch 69 iter 12 sum correct 0.8963341346153846 loss 1.8863365650177002
epoch 69 iter 13 sum correct 0.8938337053571429 loss 1.8865408897399902
epoch 69 iter 14 sum correct 0.8950520833333333 loss 1.8864508867263794
epoch 69 iter 15 sum correct 0.8956298828125 loss 1.8864011764526367
epoch 69 iter 16 sum correct 0.8952205882352942 loss 1.8864341974258423
epoch 69 iter 17 sum correct 0.8939887152777778 loss 1.8865255117416382
epoch 69 iter 18 sum correct 0.8943256578947368 loss 1.886507511138916
epoch 69 iter 19 sum correct 0.8943359375 loss 1.886492371559143
epoch 69 iter 20 sum correct 0.8941592261904762 loss 1.8864902257919312
epoch 69 iter 21 sum correct 0.8955078125 loss 1.8863741159439087
epoch 69 iter 22 sum correct 0.8962296195652174 loss 1.8863180875778198
epoch 69 iter 23 sum correct 0.8960774739583334 loss 1.8863298892974854
epoch 69 iter 24 sum correct 0.89640625 loss 1.886309027671814
epoch 69 iter 25 sum correct 0.8965594951923077 loss 1.8862969875335693
epoch 69 iter 26 sum correct 0.8977864583333334 loss 1.8862029314041138
epoch 69 iter 27 sum correct 0.8982282366071429 loss 1.8861658573150635
epoch 69 iter 28 sum correct 0.8978313577586207 loss 1.8861925601959229
epoch 69 iter 29 sum correct 0.8978515625 loss 1.8861902952194214
epoch 69 iter 30 sum correct 0.897492439516129 loss 1.8862181901931763
epoch 69 iter 31 sum correct 0.89739990234375 loss 1.886228084564209
epoch 69 iter 32 sum correct 0.8974313446969697 loss 1.8862237930297852
epoch 69 iter 33 sum correct 0.8976332720588235 loss 1.8861976861953735
epoch 69 iter 34 sum correct 0.8977120535714286 loss 1.8861899375915527
epoch 69 iter 35 sum correct 0.8972981770833334 loss 1.8862253427505493
epoch 69 iter 36 sum correct 0.896801097972973 loss 1.886266827583313
epoch 69 iter 37 sum correct 0.8966385690789473 loss 1.8862792253494263
epoch 69 iter 38 sum correct 0.8965845352564102 loss 1.8862847089767456
epoch 69 iter 39 sum correct 0.8966796875 loss 1.8862794637680054
epoch 69 iter 40 sum correct 0.8971512957317073 loss 1.8862419128417969
epoch 69 iter 41 sum correct 0.8975074404761905 loss 1.8862144947052002
epoch 69 iter 42 sum correct 0.8978015988372093 loss 1.8861923217773438
epoch 69 iter 43 sum correct 0.8978604403409091 loss 1.8861873149871826
epoch 69 iter 44 sum correct 0.8976996527777777 loss 1.8861982822418213
epoch 69 iter 45 sum correct 0.8972486413043478 loss 1.886231780052185
epoch 69 iter 46 sum correct 0.896983045212766 loss 1.8862550258636475
epoch 69 iter 47 sum correct 0.8968912760416666 loss 1.8862613439559937
epoch 69 iter 48 sum correct 0.896843112244898 loss 1.8862663507461548
epoch 69 iter 49 sum correct 0.8969921875 loss 1.8862563371658325
epoch 69 iter 50 sum correct 0.8965992647058824 loss 1.8862836360931396
epoch 69 iter 51 sum correct 0.8965594951923077 loss 1.8862873315811157
epoch 69 iter 52 sum correct 0.8960790094339622 loss 1.8863236904144287
epoch 69 iter 53 sum correct 0.8961950231481481 loss 1.8863122463226318
epoch 69 iter 54 sum correct 0.8963068181818182 loss 1.8863022327423096
epoch 69 iter 55 sum correct 0.8965890066964286 loss 1.886278510093689
epoch 69 iter 56 sum correct 0.8819558662280702 loss 1.8863098621368408
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.60it/s] 29%|██▊       | 2/7.009765625 [00:00<00:01,  3.15it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.28it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  6.08it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.29it/s]8it [00:01,  5.72it/s]                                 macro  0.5347920690854152
micro  0.6291446085260518
[[292   0  47  16  56  15  41]
 [ 34   0   7   5   7   0   3]
 [ 77   0 251  12  82  35  39]
 [ 46   0  24 718  33  18  56]
 [ 89   0 113  17 327   7 100]
 [ 18   0  30  16  10 332   9]
 [ 74   0  45  43  97  10 338]]
              precision    recall  f1-score   support

           0       0.46      0.63      0.53       467
           1       0.00      0.00      0.00        56
           2       0.49      0.51      0.50       496
           3       0.87      0.80      0.83       895
           4       0.53      0.50      0.52       653
           5       0.80      0.80      0.80       415
           6       0.58      0.56      0.57       607

    accuracy                           0.63      3589
   macro avg       0.53      0.54      0.53      3589
weighted avg       0.63      0.63      0.63      3589

correct 0.6291446085260518 
f1 0.5347920690854152 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.70it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.39it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.70it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.99it/s]8it [00:01,  5.93it/s]                                 macro  0.5456694105185725
micro  0.6436333240456952
[[326   0  55  11  56  11  32]
 [ 30   0  10   5   6   2   2]
 [100   0 255  12  82  38  41]
 [ 40   0  26 738  34  18  23]
 [ 86   0  80  23 303   4  98]
 [ 18   0  42  15   9 323   9]
 [ 73   0  51  25 104   8 365]]
              precision    recall  f1-score   support

           0       0.48      0.66      0.56       491
           1       0.00      0.00      0.00        55
           2       0.49      0.48      0.49       528
           3       0.89      0.84      0.86       879
           4       0.51      0.51      0.51       594
           5       0.80      0.78      0.79       416
           6       0.64      0.58      0.61       626

    accuracy                           0.64      3589
   macro avg       0.55      0.55      0.55      3589
weighted avg       0.65      0.64      0.64      3589

correct 0.6436333240456952 
f1 0.5456694105185725 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:38,  1.43it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.34it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.84it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.28it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.46it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.71it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.76it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.91it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.88it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.00it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.97it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.07it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  4.01it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.06it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.03it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.11it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  3.98it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.05it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  3.97it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.06it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.03it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  3.87it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  3.90it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.02it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  3.99it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.08it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.04it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.12it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  3.94it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  3.98it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  3.95it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.05it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.02it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.10it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.05it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.13it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  3.95it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  3.99it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  3.89it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:04,  3.95it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  3.89it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  3.95it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  3.88it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:03,  3.95it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  3.90it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  3.96it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  3.90it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:02,  3.96it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  3.89it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  3.95it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  3.88it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:01,  4.00it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.00it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.09it/s] 98%|█████████▊| 55/56.072265625 [00:14<00:00,  4.07it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.14it/s]57it [00:14,  3.91it/s]                                  epoch 70 iter 0 sum correct 0.908203125 loss 1.885278344154358
epoch 70 iter 1 sum correct 0.91015625 loss 1.8852002620697021
epoch 70 iter 2 sum correct 0.9088541666666666 loss 1.8853237628936768
epoch 70 iter 3 sum correct 0.90380859375 loss 1.8856770992279053
epoch 70 iter 4 sum correct 0.90234375 loss 1.8857920169830322
epoch 70 iter 5 sum correct 0.9036458333333334 loss 1.8856714963912964
epoch 70 iter 6 sum correct 0.9031808035714286 loss 1.8857263326644897
epoch 70 iter 7 sum correct 0.901611328125 loss 1.8858619928359985
epoch 70 iter 8 sum correct 0.9019097222222222 loss 1.8858391046524048
epoch 70 iter 9 sum correct 0.9013671875 loss 1.8858698606491089
epoch 70 iter 10 sum correct 0.9012784090909091 loss 1.8858954906463623
epoch 70 iter 11 sum correct 0.8986002604166666 loss 1.8860944509506226
epoch 70 iter 12 sum correct 0.8975360576923077 loss 1.8861842155456543
epoch 70 iter 13 sum correct 0.8980189732142857 loss 1.88614821434021
epoch 70 iter 14 sum correct 0.8986979166666667 loss 1.8860893249511719
epoch 70 iter 15 sum correct 0.898681640625 loss 1.8860960006713867
epoch 70 iter 16 sum correct 0.8983226102941176 loss 1.8861342668533325
epoch 70 iter 17 sum correct 0.8976779513888888 loss 1.8861745595932007
epoch 70 iter 18 sum correct 0.8975123355263158 loss 1.8861923217773438
epoch 70 iter 19 sum correct 0.89638671875 loss 1.886286735534668
epoch 70 iter 20 sum correct 0.8965773809523809 loss 1.886268973350525
epoch 70 iter 21 sum correct 0.8967507102272727 loss 1.8862619400024414
epoch 70 iter 22 sum correct 0.8959748641304348 loss 1.8863219022750854
epoch 70 iter 23 sum correct 0.895263671875 loss 1.8863742351531982
epoch 70 iter 24 sum correct 0.89421875 loss 1.8864610195159912
epoch 70 iter 25 sum correct 0.89453125 loss 1.8864328861236572
epoch 70 iter 26 sum correct 0.8940972222222222 loss 1.8864634037017822
epoch 70 iter 27 sum correct 0.8935546875 loss 1.8865066766738892
epoch 70 iter 28 sum correct 0.8927128232758621 loss 1.8865773677825928
epoch 70 iter 29 sum correct 0.8927734375 loss 1.886574625968933
epoch 70 iter 30 sum correct 0.8931451612903226 loss 1.8865444660186768
epoch 70 iter 31 sum correct 0.8929443359375 loss 1.8865666389465332
epoch 70 iter 32 sum correct 0.892874053030303 loss 1.8865703344345093
epoch 70 iter 33 sum correct 0.8921185661764706 loss 1.8866370916366577
epoch 70 iter 34 sum correct 0.892578125 loss 1.8865997791290283
epoch 70 iter 35 sum correct 0.8926323784722222 loss 1.8865960836410522
epoch 70 iter 36 sum correct 0.892261402027027 loss 1.8866204023361206
epoch 70 iter 37 sum correct 0.8918071546052632 loss 1.8866541385650635
epoch 70 iter 38 sum correct 0.8923778044871795 loss 1.8866076469421387
epoch 70 iter 39 sum correct 0.892138671875 loss 1.8866297006607056
epoch 70 iter 40 sum correct 0.8927686737804879 loss 1.8865785598754883
epoch 70 iter 41 sum correct 0.8931361607142857 loss 1.8865525722503662
epoch 70 iter 42 sum correct 0.8933502906976745 loss 1.886536717414856
epoch 70 iter 43 sum correct 0.8937322443181818 loss 1.886505365371704
epoch 70 iter 44 sum correct 0.8936631944444444 loss 1.8865089416503906
epoch 70 iter 45 sum correct 0.8936820652173914 loss 1.886507272720337
epoch 70 iter 46 sum correct 0.8943650265957447 loss 1.8864513635635376
epoch 70 iter 47 sum correct 0.894775390625 loss 1.8864178657531738
epoch 70 iter 48 sum correct 0.8948102678571429 loss 1.8864127397537231
epoch 70 iter 49 sum correct 0.894609375 loss 1.8864284753799438
epoch 70 iter 50 sum correct 0.8951439950980392 loss 1.8863856792449951
epoch 70 iter 51 sum correct 0.8947566105769231 loss 1.8864164352416992
epoch 70 iter 52 sum correct 0.8949734669811321 loss 1.8864009380340576
epoch 70 iter 53 sum correct 0.8953993055555556 loss 1.8863664865493774
epoch 70 iter 54 sum correct 0.8952769886363636 loss 1.886372447013855
epoch 70 iter 55 sum correct 0.8953683035714286 loss 1.886365294456482
epoch 70 iter 56 sum correct 0.880859375 loss 1.8862943649291992
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.60it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.18it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.39it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.56it/s]8it [00:01,  5.60it/s]                                 macro  0.5435246826875672
micro  0.6402897743103929
[[261   0  51  18  71  10  56]
 [ 31   0   4   5  10   0   6]
 [ 61   0 225  17 106  28  59]
 [ 33   0  19 722  19  20  82]
 [ 59   0  65  19 373  10 127]
 [ 13   0  20  17  16 329  20]
 [ 38   0  31  33 114   3 388]]
              precision    recall  f1-score   support

           0       0.53      0.56      0.54       467
           1       0.00      0.00      0.00        56
           2       0.54      0.45      0.49       496
           3       0.87      0.81      0.84       895
           4       0.53      0.57      0.55       653
           5       0.82      0.79      0.81       415
           6       0.53      0.64      0.58       607

    accuracy                           0.64      3589
   macro avg       0.54      0.55      0.54      3589
weighted avg       0.64      0.64      0.64      3589

correct 0.6402897743103929 
f1 0.5435246826875672 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.73it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.43it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.80it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.80it/s]8it [00:01,  5.86it/s]                                 macro  0.5536022056005742
micro  0.6533853441069936
[[286   0  42  20  81   7  55]
 [ 23   0   9   5  13   1   4]
 [ 70   0 246  23 103  31  55]
 [ 24   0  18 742  38  16  41]
 [ 55   0  59  19 333   6 122]
 [ 11   0  44  15  12 315  19]
 [ 31   0  29  25 109   9 423]]
              precision    recall  f1-score   support

           0       0.57      0.58      0.58       491
           1       0.00      0.00      0.00        55
           2       0.55      0.47      0.50       528
           3       0.87      0.84      0.86       879
           4       0.48      0.56      0.52       594
           5       0.82      0.76      0.79       416
           6       0.59      0.68      0.63       626

    accuracy                           0.65      3589
   macro avg       0.56      0.56      0.55      3589
weighted avg       0.65      0.65      0.65      3589

correct 0.6533853441069936 
f1 0.5536022056005742 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:37,  1.46it/s]  4%|▎         | 2/56.072265625 [00:00<00:22,  2.36it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.87it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.30it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.49it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.71it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.73it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.84it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.79it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  3.93it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.85it/s] 21%|██▏       | 12/56.072265625 [00:03<00:11,  3.93it/s] 23%|██▎       | 13/56.072265625 [00:03<00:11,  3.85it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  3.92it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  3.86it/s] 29%|██▊       | 16/56.072265625 [00:04<00:10,  3.93it/s] 30%|███       | 17/56.072265625 [00:04<00:10,  3.88it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  3.99it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  3.98it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.07it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  3.94it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  3.98it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  3.91it/s] 43%|████▎     | 24/56.072265625 [00:06<00:08,  3.97it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  3.92it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  3.97it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  3.87it/s] 50%|████▉     | 28/56.072265625 [00:07<00:07,  3.93it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  3.91it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.01it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  3.88it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:06,  3.95it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  3.87it/s] 61%|██████    | 34/56.072265625 [00:09<00:05,  3.94it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  3.88it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:05,  3.95it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  3.88it/s] 68%|██████▊   | 38/56.072265625 [00:10<00:04,  3.94it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  3.89it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:04,  3.95it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  3.89it/s] 75%|███████▍  | 42/56.072265625 [00:11<00:03,  3.95it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  3.89it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:03,  3.95it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  3.89it/s] 82%|████████▏ | 46/56.072265625 [00:12<00:02,  3.95it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  3.89it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:02,  3.95it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  3.89it/s] 89%|████████▉ | 50/56.072265625 [00:13<00:01,  3.98it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  3.98it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.07it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.03it/s] 96%|█████████▋| 54/56.072265625 [00:14<00:00,  4.11it/s] 98%|█████████▊| 55/56.072265625 [00:14<00:00,  4.07it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.14it/s]57it [00:14,  4.94it/s]                                  57it [00:14,  3.86it/s]epoch 71 iter 0 sum correct 0.90625 loss 1.88556706905365
epoch 71 iter 1 sum correct 0.900390625 loss 1.8859562873840332
epoch 71 iter 2 sum correct 0.9055989583333334 loss 1.8855366706848145
epoch 71 iter 3 sum correct 0.9033203125 loss 1.885735273361206
epoch 71 iter 4 sum correct 0.903125 loss 1.8857250213623047
epoch 71 iter 5 sum correct 0.9052734375 loss 1.8855547904968262
epoch 71 iter 6 sum correct 0.9017857142857143 loss 1.8858219385147095
epoch 71 iter 7 sum correct 0.90283203125 loss 1.885718822479248
epoch 71 iter 8 sum correct 0.9019097222222222 loss 1.8858002424240112
epoch 71 iter 9 sum correct 0.90390625 loss 1.8856643438339233
epoch 71 iter 10 sum correct 0.9046519886363636 loss 1.8856103420257568
epoch 71 iter 11 sum correct 0.9025065104166666 loss 1.885772466659546
epoch 71 iter 12 sum correct 0.9015925480769231 loss 1.885849952697754
epoch 71 iter 13 sum correct 0.9024832589285714 loss 1.8857742547988892
epoch 71 iter 14 sum correct 0.9032552083333333 loss 1.8857080936431885
epoch 71 iter 15 sum correct 0.902587890625 loss 1.885765552520752
epoch 71 iter 16 sum correct 0.9025735294117647 loss 1.8857711553573608
epoch 71 iter 17 sum correct 0.9029947916666666 loss 1.8857312202453613
epoch 71 iter 18 sum correct 0.9040912828947368 loss 1.8856440782546997
epoch 71 iter 19 sum correct 0.90517578125 loss 1.8855632543563843
epoch 71 iter 20 sum correct 0.9056919642857143 loss 1.8855247497558594
epoch 71 iter 21 sum correct 0.9070490056818182 loss 1.885423183441162
epoch 71 iter 22 sum correct 0.9069293478260869 loss 1.8854297399520874
epoch 71 iter 23 sum correct 0.9069010416666666 loss 1.8854339122772217
epoch 71 iter 24 sum correct 0.9065625 loss 1.88546884059906
epoch 71 iter 25 sum correct 0.9067007211538461 loss 1.8854578733444214
epoch 71 iter 26 sum correct 0.9069010416666666 loss 1.8854397535324097
epoch 71 iter 27 sum correct 0.9063197544642857 loss 1.8854862451553345
epoch 71 iter 28 sum correct 0.906317349137931 loss 1.8854894638061523
epoch 71 iter 29 sum correct 0.9057291666666667 loss 1.8855390548706055
epoch 71 iter 30 sum correct 0.9053679435483871 loss 1.8855617046356201
epoch 71 iter 31 sum correct 0.9046630859375 loss 1.8856145143508911
epoch 71 iter 32 sum correct 0.9047703598484849 loss 1.8856021165847778
epoch 71 iter 33 sum correct 0.9052734375 loss 1.8855628967285156
epoch 71 iter 34 sum correct 0.90546875 loss 1.8855472803115845
epoch 71 iter 35 sum correct 0.9056532118055556 loss 1.8855302333831787
epoch 71 iter 36 sum correct 0.9063027871621622 loss 1.8854789733886719
epoch 71 iter 37 sum correct 0.9064041940789473 loss 1.885470986366272
epoch 71 iter 38 sum correct 0.9064002403846154 loss 1.8854730129241943
epoch 71 iter 39 sum correct 0.90673828125 loss 1.8854471445083618
epoch 71 iter 40 sum correct 0.9063929115853658 loss 1.885473608970642
epoch 71 iter 41 sum correct 0.9059709821428571 loss 1.8855046033859253
epoch 71 iter 42 sum correct 0.9050236191860465 loss 1.885575771331787
epoch 71 iter 43 sum correct 0.9045188210227273 loss 1.8856157064437866
epoch 71 iter 44 sum correct 0.9045572916666667 loss 1.885613203048706
epoch 71 iter 45 sum correct 0.9041270380434783 loss 1.8856425285339355
epoch 71 iter 46 sum correct 0.9033410904255319 loss 1.8857054710388184
epoch 71 iter 47 sum correct 0.9033610026041666 loss 1.8857016563415527
epoch 71 iter 48 sum correct 0.9035794005102041 loss 1.8856818675994873
epoch 71 iter 49 sum correct 0.903515625 loss 1.8856894969940186
epoch 71 iter 50 sum correct 0.9033394607843137 loss 1.8857018947601318
epoch 71 iter 51 sum correct 0.9033578725961539 loss 1.8856992721557617
epoch 71 iter 52 sum correct 0.9037441037735849 loss 1.8856709003448486
epoch 71 iter 53 sum correct 0.9039351851851852 loss 1.8856581449508667
epoch 71 iter 54 sum correct 0.9044034090909091 loss 1.8856199979782104
epoch 71 iter 55 sum correct 0.9044363839285714 loss 1.8856176137924194
epoch 71 iter 56 sum correct 0.8896998355263158 loss 1.8856266736984253
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.53it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.08it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.57it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.86it/s]8it [00:01,  5.74it/s]                                 macro  0.5478513048201232
micro  0.6486486486486487
[[268   0  38  29  59  11  62]
 [ 27   0   8   5   7   0   9]
 [ 55   0 225  29  93  31  63]
 [ 21   0  18 780  21  11  44]
 [ 62   0  54  38 364   5 130]
 [ 10   0  23  25  17 328  12]
 [ 36   0  20  83  99   6 363]]
              precision    recall  f1-score   support

           0       0.56      0.57      0.57       467
           1       0.00      0.00      0.00        56
           2       0.58      0.45      0.51       496
           3       0.79      0.87      0.83       895
           4       0.55      0.56      0.55       653
           5       0.84      0.79      0.81       415
           6       0.53      0.60      0.56       607

    accuracy                           0.65      3589
   macro avg       0.55      0.55      0.55      3589
weighted avg       0.64      0.65      0.64      3589

correct 0.6486486486486487 
f1 0.5478513048201232 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.68it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.23it/s] 57%|█████▋    | 4/7.009765625 [00:00<00:00,  5.28it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.90it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.99it/s]8it [00:01,  5.80it/s]                                 macro  0.5440125017789342
micro  0.647255502925606
[[285   0  38  32  67   7  62]
 [ 24   0   5   8  13   2   3]
 [ 68   0 232  38  97  33  60]
 [ 16   0  18 793  22  11  19]
 [ 56   0  44  51 312   4 127]
 [ 11   0  41  23  16 310  15]
 [ 37   0  19  83  84  12 391]]
              precision    recall  f1-score   support

           0       0.57      0.58      0.58       491
           1       0.00      0.00      0.00        55
           2       0.58      0.44      0.50       528
           3       0.77      0.90      0.83       879
           4       0.51      0.53      0.52       594
           5       0.82      0.75      0.78       416
           6       0.58      0.62      0.60       626

    accuracy                           0.65      3589
   macro avg       0.55      0.55      0.54      3589
weighted avg       0.63      0.65      0.64      3589

correct 0.647255502925606 
f1 0.5440125017789342 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:38,  1.43it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.34it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.86it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.28it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.46it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.71it/s] 12%|█▏        | 7/56.072265625 [00:02<00:12,  3.79it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.94it/s] 16%|█▌        | 9/56.072265625 [00:02<00:11,  3.93it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.03it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  4.00it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.10it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  4.06it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.05it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  3.91it/s] 29%|██▊       | 16/56.072265625 [00:04<00:10,  3.99it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  3.95it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.05it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  3.99it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.08it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.02it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.10it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.04it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.11it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.06it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.05it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.02it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.03it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  3.92it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  3.98it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  3.89it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:06,  3.95it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  3.87it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  3.93it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  3.88it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:05,  3.94it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  3.88it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  3.95it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  3.88it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:04,  3.95it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  3.89it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  3.95it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  3.88it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:03,  3.93it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  3.86it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  3.93it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  3.86it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:02,  3.62it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  3.65it/s] 89%|████████▉ | 50/56.072265625 [00:13<00:01,  3.83it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  3.88it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:01,  4.01it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.00it/s] 96%|█████████▋| 54/56.072265625 [00:14<00:00,  4.10it/s] 98%|█████████▊| 55/56.072265625 [00:14<00:00,  4.07it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.14it/s]57it [00:14,  5.01it/s]                                  57it [00:14,  3.88it/s]epoch 72 iter 0 sum correct 0.908203125 loss 1.8851776123046875
epoch 72 iter 1 sum correct 0.904296875 loss 1.885500192642212
epoch 72 iter 2 sum correct 0.9069010416666666 loss 1.8853404521942139
epoch 72 iter 3 sum correct 0.91015625 loss 1.8850810527801514
epoch 72 iter 4 sum correct 0.91015625 loss 1.8850822448730469
epoch 72 iter 5 sum correct 0.9075520833333334 loss 1.88527250289917
epoch 72 iter 6 sum correct 0.90625 loss 1.8854060173034668
epoch 72 iter 7 sum correct 0.90283203125 loss 1.8856728076934814
epoch 72 iter 8 sum correct 0.9025607638888888 loss 1.8856993913650513
epoch 72 iter 9 sum correct 0.9025390625 loss 1.8856970071792603
epoch 72 iter 10 sum correct 0.9021661931818182 loss 1.8857523202896118
epoch 72 iter 11 sum correct 0.9021809895833334 loss 1.8857603073120117
epoch 72 iter 12 sum correct 0.9030949519230769 loss 1.8856998682022095
epoch 72 iter 13 sum correct 0.9037388392857143 loss 1.8856468200683594
epoch 72 iter 14 sum correct 0.9036458333333334 loss 1.8856432437896729
epoch 72 iter 15 sum correct 0.9036865234375 loss 1.8856558799743652
epoch 72 iter 16 sum correct 0.9046415441176471 loss 1.8855727910995483
epoch 72 iter 17 sum correct 0.9053819444444444 loss 1.8855152130126953
epoch 72 iter 18 sum correct 0.9053248355263158 loss 1.8855150938034058
epoch 72 iter 19 sum correct 0.906640625 loss 1.8854213953018188
epoch 72 iter 20 sum correct 0.9055059523809523 loss 1.8855067491531372
epoch 72 iter 21 sum correct 0.9055397727272727 loss 1.8855020999908447
epoch 72 iter 22 sum correct 0.9058254076086957 loss 1.8854882717132568
epoch 72 iter 23 sum correct 0.9054361979166666 loss 1.8855197429656982
epoch 72 iter 24 sum correct 0.904921875 loss 1.8855615854263306
epoch 72 iter 25 sum correct 0.9044471153846154 loss 1.8856003284454346
epoch 72 iter 26 sum correct 0.9040798611111112 loss 1.8856340646743774
epoch 72 iter 27 sum correct 0.9040876116071429 loss 1.8856337070465088
epoch 72 iter 28 sum correct 0.9036233836206896 loss 1.8856676816940308
epoch 72 iter 29 sum correct 0.9046223958333334 loss 1.8855955600738525
epoch 72 iter 30 sum correct 0.9046118951612904 loss 1.885597825050354
epoch 72 iter 31 sum correct 0.90478515625 loss 1.8855823278427124
epoch 72 iter 32 sum correct 0.9051254734848485 loss 1.8855551481246948
epoch 72 iter 33 sum correct 0.9048713235294118 loss 1.8855763673782349
epoch 72 iter 34 sum correct 0.9041852678571428 loss 1.8856269121170044
epoch 72 iter 35 sum correct 0.9041883680555556 loss 1.885629415512085
epoch 72 iter 36 sum correct 0.9036634290540541 loss 1.8856697082519531
epoch 72 iter 37 sum correct 0.9037314967105263 loss 1.8856655359268188
epoch 72 iter 38 sum correct 0.9038461538461539 loss 1.885655164718628
epoch 72 iter 39 sum correct 0.90380859375 loss 1.8856576681137085
epoch 72 iter 40 sum correct 0.9035823170731707 loss 1.8856749534606934
epoch 72 iter 41 sum correct 0.9035528273809523 loss 1.8856769800186157
epoch 72 iter 42 sum correct 0.9037063953488372 loss 1.8856619596481323
epoch 72 iter 43 sum correct 0.9037198153409091 loss 1.8856618404388428
epoch 72 iter 44 sum correct 0.9036892361111111 loss 1.8856679201126099
epoch 72 iter 45 sum correct 0.9034476902173914 loss 1.8856861591339111
epoch 72 iter 46 sum correct 0.9036319813829787 loss 1.8856745958328247
epoch 72 iter 47 sum correct 0.9038899739583334 loss 1.8856563568115234
epoch 72 iter 48 sum correct 0.9039779974489796 loss 1.8856496810913086
epoch 72 iter 49 sum correct 0.9041796875 loss 1.8856327533721924
epoch 72 iter 50 sum correct 0.9039522058823529 loss 1.8856532573699951
epoch 72 iter 51 sum correct 0.9039963942307693 loss 1.8856494426727295
epoch 72 iter 52 sum correct 0.9038546580188679 loss 1.8856589794158936
epoch 72 iter 53 sum correct 0.9040798611111112 loss 1.8856419324874878
epoch 72 iter 54 sum correct 0.9038352272727272 loss 1.8856593370437622
epoch 72 iter 55 sum correct 0.9039132254464286 loss 1.8856528997421265
epoch 72 iter 56 sum correct 0.8891858552631579 loss 1.8856689929962158
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.52it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.10it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.48it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.68it/s]8it [00:01,  5.57it/s]                                 macro  0.5513358117584003
micro  0.6483700195040402
[[296   0  35  16  70   7  43]
 [ 25   0   8   5  16   0   2]
 [ 68   0 230   9  96  30  63]
 [ 30   0  20 731  38  15  61]
 [ 75   0  65  17 369   8 119]
 [ 15   0  34  17  10 323  16]
 [ 49   0  27  43 106   4 378]]
              precision    recall  f1-score   support

           0       0.53      0.63      0.58       467
           1       0.00      0.00      0.00        56
           2       0.55      0.46      0.50       496
           3       0.87      0.82      0.84       895
           4       0.52      0.57      0.54       653
           5       0.83      0.78      0.81       415
           6       0.55      0.62      0.59       607

    accuracy                           0.65      3589
   macro avg       0.55      0.55      0.55      3589
weighted avg       0.65      0.65      0.65      3589

correct 0.6483700195040402 
f1 0.5513358117584003 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.75it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.44it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.77it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.96it/s]8it [00:01,  5.93it/s]                                 macro  0.5602155867394626
micro  0.6617442184452493
[[305   0  32  16  81   7  50]
 [ 24   0   7   6  14   2   2]
 [ 82   0 245  13  96  36  56]
 [ 32   0  18 745  37  14  33]
 [ 56   0  45  18 354   4 117]
 [ 12   0  48  15  16 311  14]
 [ 53   0  25  23 100  10 415]]
              precision    recall  f1-score   support

           0       0.54      0.62      0.58       491
           1       0.00      0.00      0.00        55
           2       0.58      0.46      0.52       528
           3       0.89      0.85      0.87       879
           4       0.51      0.60      0.55       594
           5       0.81      0.75      0.78       416
           6       0.60      0.66      0.63       626

    accuracy                           0.66      3589
   macro avg       0.56      0.56      0.56      3589
weighted avg       0.66      0.66      0.66      3589

correct 0.6617442184452493 
f1 0.5602155867394626 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:39,  1.39it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.31it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.81it/s]  7%|▋         | 4/56.072265625 [00:01<00:16,  3.25it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.43it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.69it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.75it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.90it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.86it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  3.99it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.95it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.04it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  4.00it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.09it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.03it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.11it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.03it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.10it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.02it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.10it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.06it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.13it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.05it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.12it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.06it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.13it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.05it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.12it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.01it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.09it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.03it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.11it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.04it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.12it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.04it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.12it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.04it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.06it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  3.99it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.08it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.02it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.10it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  4.04it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.08it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.01it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.09it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  4.01it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.09it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.02it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.12it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  4.06it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.13it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.07it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.15it/s] 98%|█████████▊| 55/56.072265625 [00:13<00:00,  4.07it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.15it/s]57it [00:14,  3.96it/s]                                  epoch 73 iter 0 sum correct 0.921875 loss 1.8842889070510864
epoch 73 iter 1 sum correct 0.9267578125 loss 1.8838679790496826
epoch 73 iter 2 sum correct 0.921875 loss 1.8842191696166992
epoch 73 iter 3 sum correct 0.9111328125 loss 1.8850295543670654
epoch 73 iter 4 sum correct 0.911328125 loss 1.8850125074386597
epoch 73 iter 5 sum correct 0.9117838541666666 loss 1.88496994972229
epoch 73 iter 6 sum correct 0.9104352678571429 loss 1.8850898742675781
epoch 73 iter 7 sum correct 0.91064453125 loss 1.8850791454315186
epoch 73 iter 8 sum correct 0.9108072916666666 loss 1.885058045387268
epoch 73 iter 9 sum correct 0.9103515625 loss 1.8851016759872437
epoch 73 iter 10 sum correct 0.9122869318181818 loss 1.8849447965621948
epoch 73 iter 11 sum correct 0.91064453125 loss 1.885075330734253
epoch 73 iter 12 sum correct 0.9116586538461539 loss 1.884992241859436
epoch 73 iter 13 sum correct 0.9104352678571429 loss 1.885095238685608
epoch 73 iter 14 sum correct 0.9106770833333333 loss 1.885079264640808
epoch 73 iter 15 sum correct 0.9110107421875 loss 1.8850555419921875
epoch 73 iter 16 sum correct 0.9113051470588235 loss 1.8850380182266235
epoch 73 iter 17 sum correct 0.912109375 loss 1.8849751949310303
epoch 73 iter 18 sum correct 0.9115953947368421 loss 1.8850188255310059
epoch 73 iter 19 sum correct 0.9109375 loss 1.8850654363632202
epoch 73 iter 20 sum correct 0.9110863095238095 loss 1.885057806968689
epoch 73 iter 21 sum correct 0.9104225852272727 loss 1.8851150274276733
epoch 73 iter 22 sum correct 0.9104959239130435 loss 1.885109782218933
epoch 73 iter 23 sum correct 0.91064453125 loss 1.88508939743042
epoch 73 iter 24 sum correct 0.911328125 loss 1.8850345611572266
epoch 73 iter 25 sum correct 0.9118088942307693 loss 1.8849986791610718
epoch 73 iter 26 sum correct 0.9109519675925926 loss 1.8850679397583008
epoch 73 iter 27 sum correct 0.9105050223214286 loss 1.88510262966156
epoch 73 iter 28 sum correct 0.9099542025862069 loss 1.8851473331451416
epoch 73 iter 29 sum correct 0.9099609375 loss 1.8851439952850342
epoch 73 iter 30 sum correct 0.9092741935483871 loss 1.8852009773254395
epoch 73 iter 31 sum correct 0.90911865234375 loss 1.885206937789917
epoch 73 iter 32 sum correct 0.9087949810606061 loss 1.885233759880066
epoch 73 iter 33 sum correct 0.9089499080882353 loss 1.885222315788269
epoch 73 iter 34 sum correct 0.9088169642857142 loss 1.8852347135543823
epoch 73 iter 35 sum correct 0.90869140625 loss 1.885244369506836
epoch 73 iter 36 sum correct 0.9087837837837838 loss 1.8852357864379883
epoch 73 iter 37 sum correct 0.9089740953947368 loss 1.8852206468582153
epoch 73 iter 38 sum correct 0.9092548076923077 loss 1.8851970434188843
epoch 73 iter 39 sum correct 0.9095703125 loss 1.8851721286773682
epoch 73 iter 40 sum correct 0.909155868902439 loss 1.8852041959762573
epoch 73 iter 41 sum correct 0.9093191964285714 loss 1.885190486907959
epoch 73 iter 42 sum correct 0.9088844476744186 loss 1.8852237462997437
epoch 73 iter 43 sum correct 0.9085582386363636 loss 1.8852519989013672
epoch 73 iter 44 sum correct 0.9083767361111111 loss 1.8852654695510864
epoch 73 iter 45 sum correct 0.9084154211956522 loss 1.8852630853652954
epoch 73 iter 46 sum correct 0.907953789893617 loss 1.8853026628494263
epoch 73 iter 47 sum correct 0.9079182942708334 loss 1.8853058815002441
epoch 73 iter 48 sum correct 0.9078045280612245 loss 1.8853135108947754
epoch 73 iter 49 sum correct 0.9077734375 loss 1.885317087173462
epoch 73 iter 50 sum correct 0.9075903799019608 loss 1.8853322267532349
epoch 73 iter 51 sum correct 0.9074143629807693 loss 1.8853468894958496
epoch 73 iter 52 sum correct 0.9070238797169812 loss 1.885382056236267
epoch 73 iter 53 sum correct 0.9067201967592593 loss 1.8854042291641235
epoch 73 iter 54 sum correct 0.9064985795454545 loss 1.8854238986968994
epoch 73 iter 55 sum correct 0.9065290178571429 loss 1.8854230642318726
epoch 73 iter 56 sum correct 0.8917900219298246 loss 1.8854060173034668
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.55it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.09it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.57it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.73it/s]8it [00:01,  5.71it/s]                                 macro  0.5457492536942407
micro  0.6444692114795207
[[244   0  47  30 100   7  39]
 [ 21   0   7   6  18   0   4]
 [ 47   0 246  17 122  32  32]
 [ 15   0  31 760  42  13  34]
 [ 48   0  69  29 417   6  84]
 [  9   0  45  20  15 318   8]
 [ 32   0  42  47 152   6 328]]
              precision    recall  f1-score   support

           0       0.59      0.52      0.55       467
           1       0.00      0.00      0.00        56
           2       0.51      0.50      0.50       496
           3       0.84      0.85      0.84       895
           4       0.48      0.64      0.55       653
           5       0.83      0.77      0.80       415
           6       0.62      0.54      0.58       607

    accuracy                           0.64      3589
   macro avg       0.55      0.54      0.55      3589
weighted avg       0.64      0.64      0.64      3589

correct 0.6444692114795207 
f1 0.5457492536942407 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.87it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.65it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.54it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.78it/s]8it [00:01,  5.92it/s]                                 macro  0.5508045959224839
micro  0.6511563109501254
[[246   0  56  23 115   7  44]
 [ 15   0   9   6  22   2   1]
 [ 51   0 261  24 122  31  39]
 [  9   0  26 772  44  12  16]
 [ 39   0  51  26 392   4  82]
 [  9   0  54  20  19 309   5]
 [ 26   0  46  36 152   9 357]]
              precision    recall  f1-score   support

           0       0.62      0.50      0.56       491
           1       0.00      0.00      0.00        55
           2       0.52      0.49      0.51       528
           3       0.85      0.88      0.86       879
           4       0.45      0.66      0.54       594
           5       0.83      0.74      0.78       416
           6       0.66      0.57      0.61       626

    accuracy                           0.65      3589
   macro avg       0.56      0.55      0.55      3589
weighted avg       0.66      0.65      0.65      3589

correct 0.6511563109501254 
f1 0.5508045959224839 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:38,  1.43it/s]  4%|▎         | 2/56.072265625 [00:00<00:22,  2.37it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.87it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.29it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.49it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.74it/s] 12%|█▏        | 7/56.072265625 [00:02<00:12,  3.78it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.94it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.90it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.02it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.97it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.05it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  4.00it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.09it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.01it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.09it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.01it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.09it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.02it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.09it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.01it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.10it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.02it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.09it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.03it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.12it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.05it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.11it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.03it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.11it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.05it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.11it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.05it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.12it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.04it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.10it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.02it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.10it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  4.05it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.11it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.03it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.11it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  4.03it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.10it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.02it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.11it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  3.98it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.04it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  3.95it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.05it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  4.01it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.08it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.03it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.11it/s] 98%|█████████▊| 55/56.072265625 [00:13<00:00,  4.05it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.11it/s]57it [00:14,  4.92it/s]                                  57it [00:14,  3.95it/s]epoch 74 iter 0 sum correct 0.912109375 loss 1.8850104808807373
epoch 74 iter 1 sum correct 0.91015625 loss 1.885115623474121
epoch 74 iter 2 sum correct 0.91796875 loss 1.8844873905181885
epoch 74 iter 3 sum correct 0.91162109375 loss 1.8850250244140625
epoch 74 iter 4 sum correct 0.909765625 loss 1.8851540088653564
epoch 74 iter 5 sum correct 0.9124348958333334 loss 1.8849601745605469
epoch 74 iter 6 sum correct 0.9087611607142857 loss 1.8852458000183105
epoch 74 iter 7 sum correct 0.90478515625 loss 1.8855664730072021
epoch 74 iter 8 sum correct 0.9064670138888888 loss 1.8854378461837769
epoch 74 iter 9 sum correct 0.905859375 loss 1.8854782581329346
epoch 74 iter 10 sum correct 0.9064275568181818 loss 1.885427713394165
epoch 74 iter 11 sum correct 0.908203125 loss 1.8852803707122803
epoch 74 iter 12 sum correct 0.9065504807692307 loss 1.8854044675827026
epoch 74 iter 13 sum correct 0.9061104910714286 loss 1.8854429721832275
epoch 74 iter 14 sum correct 0.9065104166666667 loss 1.8854187726974487
epoch 74 iter 15 sum correct 0.9075927734375 loss 1.8853398561477661
epoch 74 iter 16 sum correct 0.908203125 loss 1.8852871656417847
epoch 74 iter 17 sum correct 0.9098307291666666 loss 1.8851661682128906
epoch 74 iter 18 sum correct 0.9093338815789473 loss 1.8852028846740723
epoch 74 iter 19 sum correct 0.908984375 loss 1.8852242231369019
epoch 74 iter 20 sum correct 0.9080171130952381 loss 1.885295033454895
epoch 74 iter 21 sum correct 0.908203125 loss 1.885278582572937
epoch 74 iter 22 sum correct 0.9085427989130435 loss 1.8852512836456299
epoch 74 iter 23 sum correct 0.9088541666666666 loss 1.885219931602478
epoch 74 iter 24 sum correct 0.908671875 loss 1.8852369785308838
epoch 74 iter 25 sum correct 0.9091796875 loss 1.8852002620697021
epoch 74 iter 26 sum correct 0.9088541666666666 loss 1.8852225542068481
epoch 74 iter 27 sum correct 0.9093191964285714 loss 1.8851932287216187
epoch 74 iter 28 sum correct 0.9088092672413793 loss 1.8852293491363525
epoch 74 iter 29 sum correct 0.909375 loss 1.8851827383041382
epoch 74 iter 30 sum correct 0.9100302419354839 loss 1.8851344585418701
epoch 74 iter 31 sum correct 0.910400390625 loss 1.885109782218933
epoch 74 iter 32 sum correct 0.9100970643939394 loss 1.8851326704025269
epoch 74 iter 33 sum correct 0.9100413602941176 loss 1.8851356506347656
epoch 74 iter 34 sum correct 0.9100446428571428 loss 1.885132074356079
epoch 74 iter 35 sum correct 0.9099934895833334 loss 1.8851405382156372
epoch 74 iter 36 sum correct 0.9095755912162162 loss 1.8851696252822876
epoch 74 iter 37 sum correct 0.9092824835526315 loss 1.885192632675171
epoch 74 iter 38 sum correct 0.9096053685897436 loss 1.8851674795150757
epoch 74 iter 39 sum correct 0.90908203125 loss 1.885207176208496
epoch 74 iter 40 sum correct 0.9090129573170732 loss 1.8852131366729736
epoch 74 iter 41 sum correct 0.9088076636904762 loss 1.885230302810669
epoch 74 iter 42 sum correct 0.9087481831395349 loss 1.885236144065857
epoch 74 iter 43 sum correct 0.9078036221590909 loss 1.8853129148483276
epoch 74 iter 44 sum correct 0.9080729166666667 loss 1.8852946758270264
epoch 74 iter 45 sum correct 0.9081182065217391 loss 1.8852901458740234
epoch 74 iter 46 sum correct 0.9081200132978723 loss 1.885289192199707
epoch 74 iter 47 sum correct 0.9078369140625 loss 1.8853087425231934
epoch 74 iter 48 sum correct 0.9079639668367347 loss 1.8852945566177368
epoch 74 iter 49 sum correct 0.907734375 loss 1.885309100151062
epoch 74 iter 50 sum correct 0.9076286764705882 loss 1.8853222131729126
epoch 74 iter 51 sum correct 0.9075646033653846 loss 1.8853312730789185
epoch 74 iter 52 sum correct 0.9076135023584906 loss 1.8853265047073364
epoch 74 iter 53 sum correct 0.9076967592592593 loss 1.885319709777832
epoch 74 iter 54 sum correct 0.9072798295454545 loss 1.8853517770767212
epoch 74 iter 55 sum correct 0.9070870535714286 loss 1.885366439819336
epoch 74 iter 56 sum correct 0.8921326754385965 loss 1.885575771331787
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.55it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.06it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.39it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.67it/s]8it [00:01,  5.65it/s]                                 macro  0.5408956641711904
micro  0.6430760657564781
[[230   0  55  31  98  18  35]
 [ 16   0  14   5  19   0   2]
 [ 34   0 236  27 112  38  49]
 [ 22   0  13 771  31  25  33]
 [ 36   0  59  37 399   9 113]
 [ 15   0  26  21  12 328  13]
 [ 29   0  36  66 126   6 344]]
              precision    recall  f1-score   support

           0       0.60      0.49      0.54       467
           1       0.00      0.00      0.00        56
           2       0.54      0.48      0.50       496
           3       0.80      0.86      0.83       895
           4       0.50      0.61      0.55       653
           5       0.77      0.79      0.78       415
           6       0.58      0.57      0.58       607

    accuracy                           0.64      3589
   macro avg       0.54      0.54      0.54      3589
weighted avg       0.63      0.64      0.64      3589

correct 0.6430760657564781 
f1 0.5408956641711904 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.77it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.39it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.41it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.79it/s]8it [00:01,  5.80it/s]                                 macro  0.5390026435381416
micro  0.6411256617442185
[[234   0  56  26 111  14  50]
 [ 16   0  14   9  14   1   1]
 [ 38   0 247  38 118  39  48]
 [ 16   0  15 775  34  20  19]
 [ 35   0  46  46 351   8 108]
 [  5   0  47  15  13 319  17]
 [ 27   0  37  62 113  12 375]]
              precision    recall  f1-score   support

           0       0.63      0.48      0.54       491
           1       0.00      0.00      0.00        55
           2       0.53      0.47      0.50       528
           3       0.80      0.88      0.84       879
           4       0.47      0.59      0.52       594
           5       0.77      0.77      0.77       416
           6       0.61      0.60      0.60       626

    accuracy                           0.64      3589
   macro avg       0.54      0.54      0.54      3589
weighted avg       0.63      0.64      0.63      3589

correct 0.6411256617442185 
f1 0.5390026435381416 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:38,  1.45it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.35it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.87it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.30it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.49it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.71it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.76it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.93it/s] 16%|█▌        | 9/56.072265625 [00:02<00:11,  3.93it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.03it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.99it/s] 21%|██▏       | 12/56.072265625 [00:03<00:11,  3.81it/s] 23%|██▎       | 13/56.072265625 [00:03<00:11,  3.86it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  3.99it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  3.98it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.06it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  3.96it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.05it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  3.98it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.06it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.00it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.07it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.00it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.09it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.02it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.10it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.03it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.10it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  3.93it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.03it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  3.99it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.08it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.03it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.11it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.07it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.13it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  3.96it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.00it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  3.95it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.04it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  3.92it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.03it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  3.98it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.07it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.03it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.11it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  4.05it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.12it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.01it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.06it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  4.02it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.11it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.08it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.15it/s] 98%|█████████▊| 55/56.072265625 [00:14<00:00,  4.11it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.17it/s]57it [00:14,  5.05it/s]                                  57it [00:14,  3.94it/s]epoch 75 iter 0 sum correct 0.921875 loss 1.8841674327850342
epoch 75 iter 1 sum correct 0.9140625 loss 1.8847689628601074
epoch 75 iter 2 sum correct 0.9134114583333334 loss 1.884918212890625
epoch 75 iter 3 sum correct 0.9072265625 loss 1.885394811630249
epoch 75 iter 4 sum correct 0.90546875 loss 1.8855491876602173
epoch 75 iter 5 sum correct 0.9059244791666666 loss 1.885512351989746
epoch 75 iter 6 sum correct 0.9065290178571429 loss 1.8854395151138306
epoch 75 iter 7 sum correct 0.909912109375 loss 1.8851778507232666
epoch 75 iter 8 sum correct 0.9092881944444444 loss 1.8852211236953735
epoch 75 iter 9 sum correct 0.9083984375 loss 1.8852903842926025
epoch 75 iter 10 sum correct 0.9090909090909091 loss 1.8852453231811523
epoch 75 iter 11 sum correct 0.9099934895833334 loss 1.885175108909607
epoch 75 iter 12 sum correct 0.9100060096153846 loss 1.88517427444458
epoch 75 iter 13 sum correct 0.9112723214285714 loss 1.8850655555725098
epoch 75 iter 14 sum correct 0.9104166666666667 loss 1.8851312398910522
epoch 75 iter 15 sum correct 0.910888671875 loss 1.8850986957550049
epoch 75 iter 16 sum correct 0.9098115808823529 loss 1.8851885795593262
epoch 75 iter 17 sum correct 0.9096137152777778 loss 1.885202169418335
epoch 75 iter 18 sum correct 0.91015625 loss 1.8851511478424072
epoch 75 iter 19 sum correct 0.909765625 loss 1.8851919174194336
epoch 75 iter 20 sum correct 0.9103422619047619 loss 1.8851467370986938
epoch 75 iter 21 sum correct 0.9094460227272727 loss 1.8852179050445557
epoch 75 iter 22 sum correct 0.9093919836956522 loss 1.8852200508117676
epoch 75 iter 23 sum correct 0.908203125 loss 1.8853086233139038
epoch 75 iter 24 sum correct 0.907734375 loss 1.8853439092636108
epoch 75 iter 25 sum correct 0.9073016826923077 loss 1.885368824005127
epoch 75 iter 26 sum correct 0.9081307870370371 loss 1.8853096961975098
epoch 75 iter 27 sum correct 0.9086216517857143 loss 1.8852707147598267
epoch 75 iter 28 sum correct 0.9089439655172413 loss 1.8852427005767822
epoch 75 iter 29 sum correct 0.908203125 loss 1.8853009939193726
epoch 75 iter 30 sum correct 0.9090851814516129 loss 1.885232925415039
epoch 75 iter 31 sum correct 0.90911865234375 loss 1.8852298259735107
epoch 75 iter 32 sum correct 0.9087357954545454 loss 1.8852640390396118
epoch 75 iter 33 sum correct 0.9081456801470589 loss 1.8853093385696411
epoch 75 iter 34 sum correct 0.908203125 loss 1.8853044509887695
epoch 75 iter 35 sum correct 0.9084743923611112 loss 1.8852852582931519
epoch 75 iter 36 sum correct 0.9084670608108109 loss 1.885285496711731
epoch 75 iter 37 sum correct 0.9082545230263158 loss 1.885301113128662
epoch 75 iter 38 sum correct 0.9079527243589743 loss 1.8853262662887573
epoch 75 iter 39 sum correct 0.908349609375 loss 1.8852958679199219
epoch 75 iter 40 sum correct 0.9083460365853658 loss 1.885298252105713
epoch 75 iter 41 sum correct 0.908203125 loss 1.8853142261505127
epoch 75 iter 42 sum correct 0.9082939680232558 loss 1.8853051662445068
epoch 75 iter 43 sum correct 0.9081587357954546 loss 1.8853130340576172
epoch 75 iter 44 sum correct 0.9086371527777778 loss 1.8852715492248535
epoch 75 iter 45 sum correct 0.9083305027173914 loss 1.8852931261062622
epoch 75 iter 46 sum correct 0.9083693484042553 loss 1.8852896690368652
epoch 75 iter 47 sum correct 0.9076334635416666 loss 1.8853497505187988
epoch 75 iter 48 sum correct 0.9076450892857143 loss 1.8853496313095093
epoch 75 iter 49 sum correct 0.9075 loss 1.8853652477264404
epoch 75 iter 50 sum correct 0.9076669730392157 loss 1.8853527307510376
epoch 75 iter 51 sum correct 0.9080528846153846 loss 1.8853236436843872
epoch 75 iter 52 sum correct 0.9076135023584906 loss 1.8853591680526733
epoch 75 iter 53 sum correct 0.9076605902777778 loss 1.8853532075881958
epoch 75 iter 54 sum correct 0.9079190340909091 loss 1.8853331804275513
epoch 75 iter 55 sum correct 0.9079241071428571 loss 1.8853299617767334
epoch 75 iter 56 sum correct 0.8930235745614035 loss 1.885461688041687
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.52it/s] 43%|████▎     | 3/7.009765625 [00:00<00:01,  4.00it/s] 57%|█████▋    | 4/7.009765625 [00:00<00:00,  5.02it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.75it/s] 86%|████████▌ | 6/7.009765625 [00:01<00:00,  6.62it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.18it/s]8it [00:01,  5.62it/s]                                 macro  0.5367855943489275
micro  0.6388966285873502
[[240   0  59  42  54  18  54]
 [ 18   0  14   3  14   0   7]
 [ 53   0 237  28  80  52  46]
 [ 12   0  12 766  30  18  57]
 [ 61   0  89  34 347  11 111]
 [  7   0  24  24  12 340   8]
 [ 42   0  34  60  99   9 363]]
              precision    recall  f1-score   support

           0       0.55      0.51      0.53       467
           1       0.00      0.00      0.00        56
           2       0.51      0.48      0.49       496
           3       0.80      0.86      0.83       895
           4       0.55      0.53      0.54       653
           5       0.76      0.82      0.79       415
           6       0.56      0.60      0.58       607

    accuracy                           0.64      3589
   macro avg       0.53      0.54      0.54      3589
weighted avg       0.62      0.64      0.63      3589

correct 0.6388966285873502 
f1 0.5367855943489275 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.74it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.34it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  6.03it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.29it/s]8it [00:01,  6.13it/s]                                 macro  0.5423020881355578
micro  0.6464196154917804
[[263   0  69  27  70  12  50]
 [ 20   0  11   9  13   2   0]
 [ 56   0 246  31  94  51  50]
 [ 14   0  13 773  31  16  32]
 [ 64   0  72  39 308   5 106]
 [  6   0  31  28  11 321  19]
 [ 46   0  28  45  83  15 409]]
              precision    recall  f1-score   support

           0       0.56      0.54      0.55       491
           1       0.00      0.00      0.00        55
           2       0.52      0.47      0.49       528
           3       0.81      0.88      0.84       879
           4       0.50      0.52      0.51       594
           5       0.76      0.77      0.77       416
           6       0.61      0.65      0.63       626

    accuracy                           0.65      3589
   macro avg       0.54      0.55      0.54      3589
weighted avg       0.63      0.65      0.64      3589

correct 0.6464196154917804 
f1 0.5423020881355578 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:37,  1.49it/s]  4%|▎         | 2/56.072265625 [00:00<00:22,  2.40it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.88it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.29it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.48it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.72it/s] 12%|█▏        | 7/56.072265625 [00:02<00:12,  3.79it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.92it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.91it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.02it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  4.01it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.08it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  4.03it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.12it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.07it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.12it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.04it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.04it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  3.90it/s] 36%|███▌      | 20/56.072265625 [00:05<00:09,  4.00it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  3.95it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.04it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  3.99it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.06it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.01it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.07it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.01it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.04it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  3.93it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  3.97it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  3.88it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:06,  3.99it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  3.96it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.00it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  3.92it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:05,  3.97it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  3.95it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.06it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  3.97it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.06it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.02it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.03it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  3.93it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:03,  3.98it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  3.94it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.04it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  3.99it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:02,  4.01it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  3.99it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.08it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  4.05it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.12it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.06it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.14it/s] 98%|█████████▊| 55/56.072265625 [00:14<00:00,  4.09it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.16it/s]57it [00:14,  5.03it/s]                                  57it [00:14,  3.93it/s]epoch 76 iter 0 sum correct 0.91015625 loss 1.8855212926864624
epoch 76 iter 1 sum correct 0.9052734375 loss 1.8857711553573608
epoch 76 iter 2 sum correct 0.9095052083333334 loss 1.8853776454925537
epoch 76 iter 3 sum correct 0.91259765625 loss 1.8851020336151123
epoch 76 iter 4 sum correct 0.91640625 loss 1.884764313697815
epoch 76 iter 5 sum correct 0.9134114583333334 loss 1.8849849700927734
epoch 76 iter 6 sum correct 0.9107142857142857 loss 1.885168433189392
epoch 76 iter 7 sum correct 0.910888671875 loss 1.8851453065872192
epoch 76 iter 8 sum correct 0.9127604166666666 loss 1.8849912881851196
epoch 76 iter 9 sum correct 0.9123046875 loss 1.8850332498550415
epoch 76 iter 10 sum correct 0.9119318181818182 loss 1.8850665092468262
epoch 76 iter 11 sum correct 0.9114583333333334 loss 1.8850864171981812
epoch 76 iter 12 sum correct 0.9103064903846154 loss 1.8851741552352905
epoch 76 iter 13 sum correct 0.9075055803571429 loss 1.8853845596313477
epoch 76 iter 14 sum correct 0.9075520833333334 loss 1.8853859901428223
epoch 76 iter 15 sum correct 0.9080810546875 loss 1.8853423595428467
epoch 76 iter 16 sum correct 0.9086626838235294 loss 1.885290265083313
epoch 76 iter 17 sum correct 0.9086371527777778 loss 1.8852882385253906
epoch 76 iter 18 sum correct 0.9091282894736842 loss 1.8852518796920776
epoch 76 iter 19 sum correct 0.9087890625 loss 1.8852713108062744
epoch 76 iter 20 sum correct 0.9079241071428571 loss 1.8853392601013184
epoch 76 iter 21 sum correct 0.9071377840909091 loss 1.8854033946990967
epoch 76 iter 22 sum correct 0.9067595108695652 loss 1.885428547859192
epoch 76 iter 23 sum correct 0.9073893229166666 loss 1.8853912353515625
epoch 76 iter 24 sum correct 0.907265625 loss 1.8854011297225952
epoch 76 iter 25 sum correct 0.9076021634615384 loss 1.8853710889816284
epoch 76 iter 26 sum correct 0.9083478009259259 loss 1.8853063583374023
epoch 76 iter 27 sum correct 0.9072963169642857 loss 1.8853890895843506
epoch 76 iter 28 sum correct 0.9076643318965517 loss 1.8853546380996704
epoch 76 iter 29 sum correct 0.907421875 loss 1.8853784799575806
epoch 76 iter 30 sum correct 0.9074470766129032 loss 1.8853806257247925
epoch 76 iter 31 sum correct 0.907958984375 loss 1.8853325843811035
epoch 76 iter 32 sum correct 0.9074928977272727 loss 1.8853667974472046
epoch 76 iter 33 sum correct 0.9079159007352942 loss 1.8853360414505005
epoch 76 iter 34 sum correct 0.9077566964285714 loss 1.8853496313095093
epoch 76 iter 35 sum correct 0.9071180555555556 loss 1.8854012489318848
epoch 76 iter 36 sum correct 0.9074113175675675 loss 1.8853812217712402
epoch 76 iter 37 sum correct 0.9071237664473685 loss 1.885403037071228
epoch 76 iter 38 sum correct 0.9075020032051282 loss 1.8853744268417358
epoch 76 iter 39 sum correct 0.907421875 loss 1.8853800296783447
epoch 76 iter 40 sum correct 0.9074885670731707 loss 1.885378360748291
epoch 76 iter 41 sum correct 0.9071800595238095 loss 1.8854014873504639
epoch 76 iter 42 sum correct 0.9071130087209303 loss 1.8854068517684937
epoch 76 iter 43 sum correct 0.9071821732954546 loss 1.8853988647460938
epoch 76 iter 44 sum correct 0.9075520833333334 loss 1.8853673934936523
epoch 76 iter 45 sum correct 0.9077360733695652 loss 1.885349154472351
epoch 76 iter 46 sum correct 0.9074966755319149 loss 1.8853700160980225
epoch 76 iter 47 sum correct 0.907470703125 loss 1.8853743076324463
epoch 76 iter 48 sum correct 0.9076450892857143 loss 1.8853569030761719
epoch 76 iter 49 sum correct 0.907734375 loss 1.8853472471237183
epoch 76 iter 50 sum correct 0.9073606004901961 loss 1.8853782415390015
epoch 76 iter 51 sum correct 0.9074894831730769 loss 1.8853678703308105
epoch 76 iter 52 sum correct 0.9074660966981132 loss 1.885367512702942
epoch 76 iter 53 sum correct 0.9074074074074074 loss 1.8853733539581299
epoch 76 iter 54 sum correct 0.9077414772727272 loss 1.8853459358215332
epoch 76 iter 55 sum correct 0.9078194754464286 loss 1.8853391408920288
epoch 76 iter 56 sum correct 0.8928522478070176 loss 1.8855674266815186
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.59it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.18it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.62it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.83it/s]8it [00:01,  5.76it/s]                                 macro  0.5202373531536443
micro  0.6232933964892727
[[297   0  29  23  55  19  44]
 [ 40   0   2   5   8   0   1]
 [101   0 179  20  96  58  42]
 [ 39   0   9 751  23  22  51]
 [101   0  75  38 339  19  81]
 [ 18   0  10  19   8 349  11]
 [ 71   0  29  46 118  21 322]]
              precision    recall  f1-score   support

           0       0.45      0.64      0.52       467
           1       0.00      0.00      0.00        56
           2       0.54      0.36      0.43       496
           3       0.83      0.84      0.84       895
           4       0.52      0.52      0.52       653
           5       0.72      0.84      0.77       415
           6       0.58      0.53      0.56       607

    accuracy                           0.62      3589
   macro avg       0.52      0.53      0.52      3589
weighted avg       0.62      0.62      0.61      3589

correct 0.6232933964892727 
f1 0.5202373531536443 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.77it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.48it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.81it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.00it/s]8it [00:01,  6.01it/s]                                 macro  0.5267258031194773
micro  0.6327667874059627
[[316   0  32  20  65  16  42]
 [ 32   0   5   8   7   2   1]
 [111   0 194  27  93  61  42]
 [ 33   0   6 776  24  19  21]
 [ 95   0  61  34 307  14  83]
 [ 14   0  20  28  13 331  10]
 [ 58   0  32  36 122  31 347]]
              precision    recall  f1-score   support

           0       0.48      0.64      0.55       491
           1       0.00      0.00      0.00        55
           2       0.55      0.37      0.44       528
           3       0.84      0.88      0.86       879
           4       0.49      0.52      0.50       594
           5       0.70      0.80      0.74       416
           6       0.64      0.55      0.59       626

    accuracy                           0.63      3589
   macro avg       0.53      0.54      0.53      3589
weighted avg       0.62      0.63      0.62      3589

correct 0.6327667874059627 
f1 0.5267258031194773 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:38,  1.44it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.31it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.82it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.26it/s]  9%|▉         | 5/56.072265625 [00:01<00:15,  3.39it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.61it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.64it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.77it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.76it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  3.86it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.84it/s] 21%|██▏       | 12/56.072265625 [00:03<00:11,  3.92it/s] 23%|██▎       | 13/56.072265625 [00:03<00:11,  3.87it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  3.94it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  3.88it/s] 29%|██▊       | 16/56.072265625 [00:04<00:10,  3.94it/s] 30%|███       | 17/56.072265625 [00:04<00:10,  3.87it/s] 32%|███▏      | 18/56.072265625 [00:05<00:09,  3.98it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  3.94it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.05it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.01it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.10it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.05it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.13it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.07it/s] 46%|████▋     | 26/56.072265625 [00:07<00:07,  3.83it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  3.86it/s] 50%|████▉     | 28/56.072265625 [00:07<00:07,  3.99it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  3.97it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.06it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.03it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.11it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  3.94it/s] 61%|██████    | 34/56.072265625 [00:09<00:05,  3.98it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  3.89it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:05,  3.96it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  3.88it/s] 68%|██████▊   | 38/56.072265625 [00:10<00:04,  3.97it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  3.92it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.03it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.00it/s] 75%|███████▍  | 42/56.072265625 [00:11<00:03,  4.09it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  4.03it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.10it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  3.92it/s] 82%|████████▏ | 46/56.072265625 [00:12<00:02,  3.97it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  3.90it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:02,  3.96it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  3.95it/s] 89%|████████▉ | 50/56.072265625 [00:13<00:01,  4.05it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  4.03it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.11it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.06it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.14it/s] 98%|█████████▊| 55/56.072265625 [00:14<00:00,  4.10it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.16it/s]57it [00:14,  3.89it/s]                                  epoch 77 iter 0 sum correct 0.919921875 loss 1.8842746019363403
epoch 77 iter 1 sum correct 0.9072265625 loss 1.885335922241211
epoch 77 iter 2 sum correct 0.9153645833333334 loss 1.8846924304962158
epoch 77 iter 3 sum correct 0.91357421875 loss 1.8848737478256226
epoch 77 iter 4 sum correct 0.91015625 loss 1.8851574659347534
epoch 77 iter 5 sum correct 0.9091796875 loss 1.8852388858795166
epoch 77 iter 6 sum correct 0.9054129464285714 loss 1.8855291604995728
epoch 77 iter 7 sum correct 0.90576171875 loss 1.8855103254318237
epoch 77 iter 8 sum correct 0.9038628472222222 loss 1.8856931924819946
epoch 77 iter 9 sum correct 0.901171875 loss 1.8858963251113892
epoch 77 iter 10 sum correct 0.8977272727272727 loss 1.8861647844314575
epoch 77 iter 11 sum correct 0.8982747395833334 loss 1.8861249685287476
epoch 77 iter 12 sum correct 0.8966346153846154 loss 1.886257290840149
epoch 77 iter 13 sum correct 0.8967633928571429 loss 1.886250615119934
epoch 77 iter 14 sum correct 0.8953125 loss 1.8863751888275146
epoch 77 iter 15 sum correct 0.8935546875 loss 1.886507511138916
epoch 77 iter 16 sum correct 0.8931525735294118 loss 1.8865338563919067
epoch 77 iter 17 sum correct 0.8927951388888888 loss 1.8865617513656616
epoch 77 iter 18 sum correct 0.8939144736842105 loss 1.8864686489105225
epoch 77 iter 19 sum correct 0.89423828125 loss 1.88644540309906
epoch 77 iter 20 sum correct 0.8947172619047619 loss 1.8864045143127441
epoch 77 iter 21 sum correct 0.8947975852272727 loss 1.886383295059204
epoch 77 iter 22 sum correct 0.8942764945652174 loss 1.8864222764968872
epoch 77 iter 23 sum correct 0.8943684895833334 loss 1.8864144086837769
epoch 77 iter 24 sum correct 0.89359375 loss 1.8864731788635254
epoch 77 iter 25 sum correct 0.8941556490384616 loss 1.8864291906356812
epoch 77 iter 26 sum correct 0.8940972222222222 loss 1.886430025100708
epoch 77 iter 27 sum correct 0.8936941964285714 loss 1.8864688873291016
epoch 77 iter 28 sum correct 0.8935883620689655 loss 1.8864773511886597
epoch 77 iter 29 sum correct 0.8935546875 loss 1.8864811658859253
epoch 77 iter 30 sum correct 0.8934601814516129 loss 1.8864890336990356
epoch 77 iter 31 sum correct 0.89324951171875 loss 1.886498212814331
epoch 77 iter 32 sum correct 0.8934067234848485 loss 1.8864825963974
epoch 77 iter 33 sum correct 0.8938419117647058 loss 1.8864492177963257
epoch 77 iter 34 sum correct 0.8944754464285715 loss 1.8864009380340576
epoch 77 iter 35 sum correct 0.8942599826388888 loss 1.8864127397537231
epoch 77 iter 36 sum correct 0.8945840371621622 loss 1.8863900899887085
epoch 77 iter 37 sum correct 0.894788240131579 loss 1.8863778114318848
epoch 77 iter 38 sum correct 0.8947816506410257 loss 1.8863757848739624
epoch 77 iter 39 sum correct 0.89453125 loss 1.8863979578018188
epoch 77 iter 40 sum correct 0.8939596036585366 loss 1.886440634727478
epoch 77 iter 41 sum correct 0.8938337053571429 loss 1.8864530324935913
epoch 77 iter 42 sum correct 0.8937590843023255 loss 1.8864532709121704
epoch 77 iter 43 sum correct 0.8936878551136364 loss 1.886456847190857
epoch 77 iter 44 sum correct 0.8940538194444444 loss 1.8864271640777588
epoch 77 iter 45 sum correct 0.8945737092391305 loss 1.8863859176635742
epoch 77 iter 46 sum correct 0.8946143617021277 loss 1.886380910873413
epoch 77 iter 47 sum correct 0.8943277994791666 loss 1.886401891708374
epoch 77 iter 48 sum correct 0.8946906887755102 loss 1.886375069618225
epoch 77 iter 49 sum correct 0.89453125 loss 1.886387586593628
epoch 77 iter 50 sum correct 0.8948376225490197 loss 1.8863657712936401
epoch 77 iter 51 sum correct 0.8950570913461539 loss 1.8863486051559448
epoch 77 iter 52 sum correct 0.8948629127358491 loss 1.8863639831542969
epoch 77 iter 53 sum correct 0.8949652777777778 loss 1.8863552808761597
epoch 77 iter 54 sum correct 0.8947798295454545 loss 1.886366605758667
epoch 77 iter 55 sum correct 0.894775390625 loss 1.8863669633865356
epoch 77 iter 56 sum correct 0.8801740679824561 loss 1.8864084482192993
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.62it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.15it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.65it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.78it/s]8it [00:01,  5.76it/s]                                 macro  0.5131069160098635
micro  0.603789356366676
[[295   0  74  21  64   3  10]
 [ 29   0  11   3   9   0   4]
 [ 68   0 272  15  97  22  22]
 [ 45   0  31 719  46  19  35]
 [109   0 121  21 345   8  49]
 [ 32   0  42  18  15 297  11]
 [112   0  63  46 141   6 239]]
              precision    recall  f1-score   support

           0       0.43      0.63      0.51       467
           1       0.00      0.00      0.00        56
           2       0.44      0.55      0.49       496
           3       0.85      0.80      0.83       895
           4       0.48      0.53      0.50       653
           5       0.84      0.72      0.77       415
           6       0.65      0.39      0.49       607

    accuracy                           0.60      3589
   macro avg       0.53      0.52      0.51      3589
weighted avg       0.62      0.60      0.60      3589

correct 0.603789356366676 
f1 0.5131069160098635 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.69it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.39it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.84it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.10it/s]8it [00:01,  6.00it/s]                                 macro  0.5194254402369082
micro  0.6129841181387573
[[300   0  87  19  68   3  14]
 [ 31   0  17   1   5   1   0]
 [ 69   0 286  19  98  29  27]
 [ 36   0  26 743  42  15  17]
 [ 98   0 106  26 312   5  47]
 [ 26   0  59  21  14 285  11]
 [122   0  56  25 139  10 274]]
              precision    recall  f1-score   support

           0       0.44      0.61      0.51       491
           1       0.00      0.00      0.00        55
           2       0.45      0.54      0.49       528
           3       0.87      0.85      0.86       879
           4       0.46      0.53      0.49       594
           5       0.82      0.69      0.75       416
           6       0.70      0.44      0.54       626

    accuracy                           0.61      3589
   macro avg       0.53      0.52      0.52      3589
weighted avg       0.63      0.61      0.61      3589

correct 0.6129841181387573 
f1 0.5194254402369082 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:38,  1.44it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.33it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.82it/s]  7%|▋         | 4/56.072265625 [00:01<00:16,  3.23it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.42it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.67it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.74it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.91it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.85it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  3.93it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.84it/s] 21%|██▏       | 12/56.072265625 [00:03<00:11,  3.92it/s] 23%|██▎       | 13/56.072265625 [00:03<00:11,  3.84it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  3.92it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  3.87it/s] 29%|██▊       | 16/56.072265625 [00:04<00:10,  3.94it/s] 30%|███       | 17/56.072265625 [00:04<00:10,  3.89it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  3.96it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  3.93it/s] 36%|███▌      | 20/56.072265625 [00:05<00:09,  3.97it/s] 37%|███▋      | 21/56.072265625 [00:05<00:09,  3.89it/s] 39%|███▉      | 22/56.072265625 [00:06<00:08,  3.92it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  3.88it/s] 43%|████▎     | 24/56.072265625 [00:06<00:08,  4.00it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  3.91it/s] 46%|████▋     | 26/56.072265625 [00:07<00:07,  3.97it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  3.94it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.05it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  3.97it/s] 54%|█████▎    | 30/56.072265625 [00:08<00:06,  4.06it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.00it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.05it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.01it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.10it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.01it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.10it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.06it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.13it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  4.07it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.14it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.02it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.10it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  4.03it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.07it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.02it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.03it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  3.94it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:02,  3.97it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  3.91it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.02it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  4.01it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.09it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.05it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.11it/s] 98%|█████████▊| 55/56.072265625 [00:14<00:00,  4.06it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.12it/s]57it [00:14,  4.94it/s]                                  57it [00:14,  3.90it/s]epoch 78 iter 0 sum correct 0.9140625 loss 1.8849453926086426
epoch 78 iter 1 sum correct 0.912109375 loss 1.8849924802780151
epoch 78 iter 2 sum correct 0.9088541666666666 loss 1.8851722478866577
epoch 78 iter 3 sum correct 0.9091796875 loss 1.8851436376571655
epoch 78 iter 4 sum correct 0.909375 loss 1.8851584196090698
epoch 78 iter 5 sum correct 0.9104817708333334 loss 1.885071039199829
epoch 78 iter 6 sum correct 0.9073660714285714 loss 1.885312557220459
epoch 78 iter 7 sum correct 0.908447265625 loss 1.8852428197860718
epoch 78 iter 8 sum correct 0.9099392361111112 loss 1.8851277828216553
epoch 78 iter 9 sum correct 0.906640625 loss 1.8853930234909058
epoch 78 iter 10 sum correct 0.904296875 loss 1.885573148727417
epoch 78 iter 11 sum correct 0.9034830729166666 loss 1.8856427669525146
epoch 78 iter 12 sum correct 0.9048978365384616 loss 1.8855277299880981
epoch 78 iter 13 sum correct 0.9047154017857143 loss 1.8855438232421875
epoch 78 iter 14 sum correct 0.905078125 loss 1.8854997158050537
epoch 78 iter 15 sum correct 0.9052734375 loss 1.8854795694351196
epoch 78 iter 16 sum correct 0.9056755514705882 loss 1.8854374885559082
epoch 78 iter 17 sum correct 0.9059244791666666 loss 1.8854299783706665
epoch 78 iter 18 sum correct 0.9054276315789473 loss 1.8854763507843018
epoch 78 iter 19 sum correct 0.90517578125 loss 1.885506510734558
epoch 78 iter 20 sum correct 0.9052269345238095 loss 1.8854941129684448
epoch 78 iter 21 sum correct 0.9050071022727273 loss 1.8855135440826416
epoch 78 iter 22 sum correct 0.9045516304347826 loss 1.8855552673339844
epoch 78 iter 23 sum correct 0.904052734375 loss 1.8855974674224854
epoch 78 iter 24 sum correct 0.904453125 loss 1.88556969165802
epoch 78 iter 25 sum correct 0.9041466346153846 loss 1.8855905532836914
epoch 78 iter 26 sum correct 0.9034288194444444 loss 1.8856372833251953
epoch 78 iter 27 sum correct 0.9036690848214286 loss 1.8856207132339478
epoch 78 iter 28 sum correct 0.9041621767241379 loss 1.8855807781219482
epoch 78 iter 29 sum correct 0.9042317708333333 loss 1.885577917098999
epoch 78 iter 30 sum correct 0.9043598790322581 loss 1.8855621814727783
epoch 78 iter 31 sum correct 0.9049072265625 loss 1.8855185508728027
epoch 78 iter 32 sum correct 0.904000946969697 loss 1.885591983795166
epoch 78 iter 33 sum correct 0.9034352022058824 loss 1.8856427669525146
epoch 78 iter 34 sum correct 0.9033482142857143 loss 1.8856521844863892
epoch 78 iter 35 sum correct 0.9034830729166666 loss 1.8856427669525146
epoch 78 iter 36 sum correct 0.9037690033783784 loss 1.8856208324432373
epoch 78 iter 37 sum correct 0.9035259046052632 loss 1.885642409324646
epoch 78 iter 38 sum correct 0.9029947916666666 loss 1.8856842517852783
epoch 78 iter 39 sum correct 0.903271484375 loss 1.8856658935546875
epoch 78 iter 40 sum correct 0.9029630335365854 loss 1.8856924772262573
epoch 78 iter 41 sum correct 0.9029482886904762 loss 1.8856914043426514
epoch 78 iter 42 sum correct 0.9025708575581395 loss 1.8857208490371704
epoch 78 iter 43 sum correct 0.9022105823863636 loss 1.8857500553131104
epoch 78 iter 44 sum correct 0.9021267361111112 loss 1.8857557773590088
epoch 78 iter 45 sum correct 0.9018342391304348 loss 1.885778546333313
epoch 78 iter 46 sum correct 0.9018866356382979 loss 1.8857746124267578
epoch 78 iter 47 sum correct 0.90185546875 loss 1.8857827186584473
epoch 78 iter 48 sum correct 0.9021444515306123 loss 1.885764241218567
epoch 78 iter 49 sum correct 0.9021875 loss 1.8857629299163818
epoch 78 iter 50 sum correct 0.9019607843137255 loss 1.8857802152633667
epoch 78 iter 51 sum correct 0.9015174278846154 loss 1.885813593864441
epoch 78 iter 52 sum correct 0.9016067216981132 loss 1.8858052492141724
epoch 78 iter 53 sum correct 0.9016203703703703 loss 1.8858089447021484
epoch 78 iter 54 sum correct 0.9015625 loss 1.8858137130737305
epoch 78 iter 55 sum correct 0.9020298549107143 loss 1.885774850845337
epoch 78 iter 56 sum correct 0.8872669956140351 loss 1.8858616352081299
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.60it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.23it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.38it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.65it/s]8it [00:01,  5.61it/s]                                 macro  0.5267356529294736
micro  0.6174421844524938
[[279   0  36  16  90  12  34]
 [ 31   0   4   3  15   0   3]
 [ 68   0 202   8 137  41  40]
 [ 44   0  20 672  68  16  75]
 [ 72   0  66  13 399  11  92]
 [ 15   0  20  16  14 335  15]
 [ 56   0  29  27 156  10 329]]
              precision    recall  f1-score   support

           0       0.49      0.60      0.54       467
           1       0.00      0.00      0.00        56
           2       0.54      0.41      0.46       496
           3       0.89      0.75      0.81       895
           4       0.45      0.61      0.52       653
           5       0.79      0.81      0.80       415
           6       0.56      0.54      0.55       607

    accuracy                           0.62      3589
   macro avg       0.53      0.53      0.53      3589
weighted avg       0.63      0.62      0.62      3589

correct 0.6174421844524938 
f1 0.5267356529294736 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.73it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.32it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.80it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.12it/s]8it [00:01,  6.00it/s]                                 macro  0.5378239736114262
micro  0.6310950125383115
[[291   0  35  14  94  13  44]
 [ 25   0   8   1  18   1   2]
 [ 69   0 237   9 136  36  41]
 [ 34   0  12 698  63  20  52]
 [ 71   0  49  15 371   9  79]
 [ 12   0  32  20  20 316  16]
 [ 65   0  29  13 152  15 352]]
              precision    recall  f1-score   support

           0       0.51      0.59      0.55       491
           1       0.00      0.00      0.00        55
           2       0.59      0.45      0.51       528
           3       0.91      0.79      0.85       879
           4       0.43      0.62      0.51       594
           5       0.77      0.76      0.77       416
           6       0.60      0.56      0.58       626

    accuracy                           0.63      3589
   macro avg       0.55      0.54      0.54      3589
weighted avg       0.64      0.63      0.63      3589

correct 0.6310950125383115 
f1 0.5378239736114262 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:39,  1.41it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.32it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.81it/s]  7%|▋         | 4/56.072265625 [00:01<00:16,  3.22it/s]  9%|▉         | 5/56.072265625 [00:01<00:15,  3.40it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.65it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.72it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.90it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.90it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.02it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.94it/s] 21%|██▏       | 12/56.072265625 [00:03<00:11,  3.99it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  3.92it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  3.98it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  3.92it/s] 29%|██▊       | 16/56.072265625 [00:04<00:10,  3.98it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  3.91it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  3.97it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  3.87it/s] 36%|███▌      | 20/56.072265625 [00:05<00:09,  3.99it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  3.96it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  3.98it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  3.95it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.06it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.02it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.10it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.02it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.10it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.05it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.13it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.07it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.14it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.08it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.07it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  3.96it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:05,  4.00it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  3.93it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.02it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  3.86it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:04,  3.94it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  3.92it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.03it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  3.98it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.07it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  3.93it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  3.98it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  3.92it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:02,  4.03it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  3.88it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  3.97it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  3.97it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:01,  3.88it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  3.91it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.03it/s] 98%|█████████▊| 55/56.072265625 [00:14<00:00,  4.02it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.10it/s]57it [00:14,  4.97it/s]                                  57it [00:14,  3.89it/s]epoch 79 iter 0 sum correct 0.908203125 loss 1.8852765560150146
epoch 79 iter 1 sum correct 0.8984375 loss 1.8860403299331665
epoch 79 iter 2 sum correct 0.8971354166666666 loss 1.8862314224243164
epoch 79 iter 3 sum correct 0.900390625 loss 1.885960578918457
epoch 79 iter 4 sum correct 0.9015625 loss 1.8858832120895386
epoch 79 iter 5 sum correct 0.9036458333333334 loss 1.8857002258300781
epoch 79 iter 6 sum correct 0.9037388392857143 loss 1.8856761455535889
epoch 79 iter 7 sum correct 0.903564453125 loss 1.8856748342514038
epoch 79 iter 8 sum correct 0.9025607638888888 loss 1.8857612609863281
epoch 79 iter 9 sum correct 0.9037109375 loss 1.8856613636016846
epoch 79 iter 10 sum correct 0.9039417613636364 loss 1.885642170906067
epoch 79 iter 11 sum correct 0.9005533854166666 loss 1.8858952522277832
epoch 79 iter 12 sum correct 0.9008413461538461 loss 1.8858606815338135
epoch 79 iter 13 sum correct 0.9029017857142857 loss 1.8856948614120483
epoch 79 iter 14 sum correct 0.9045572916666667 loss 1.8855582475662231
epoch 79 iter 15 sum correct 0.90478515625 loss 1.8855321407318115
epoch 79 iter 16 sum correct 0.9051011029411765 loss 1.8855102062225342
epoch 79 iter 17 sum correct 0.9050564236111112 loss 1.8855175971984863
epoch 79 iter 18 sum correct 0.9049136513157895 loss 1.8855245113372803
epoch 79 iter 19 sum correct 0.905078125 loss 1.8855104446411133
epoch 79 iter 20 sum correct 0.9054129464285714 loss 1.8854879140853882
epoch 79 iter 21 sum correct 0.9060724431818182 loss 1.885430097579956
epoch 79 iter 22 sum correct 0.9065896739130435 loss 1.885383129119873
epoch 79 iter 23 sum correct 0.9072265625 loss 1.8853299617767334
epoch 79 iter 24 sum correct 0.907578125 loss 1.8852981328964233
epoch 79 iter 25 sum correct 0.9076021634615384 loss 1.885300874710083
epoch 79 iter 26 sum correct 0.9077690972222222 loss 1.885290503501892
epoch 79 iter 27 sum correct 0.9079938616071429 loss 1.8852770328521729
epoch 79 iter 28 sum correct 0.9080684267241379 loss 1.885271430015564
epoch 79 iter 29 sum correct 0.9074869791666667 loss 1.8853232860565186
epoch 79 iter 30 sum correct 0.9071320564516129 loss 1.8853561878204346
epoch 79 iter 31 sum correct 0.9073486328125 loss 1.885342001914978
epoch 79 iter 32 sum correct 0.9077296401515151 loss 1.8853107690811157
epoch 79 iter 33 sum correct 0.9079159007352942 loss 1.8852933645248413
epoch 79 iter 34 sum correct 0.9078683035714286 loss 1.8852957487106323
epoch 79 iter 35 sum correct 0.9080946180555556 loss 1.885274052619934
epoch 79 iter 36 sum correct 0.9071473817567568 loss 1.8853471279144287
epoch 79 iter 37 sum correct 0.9067639802631579 loss 1.8853741884231567
epoch 79 iter 38 sum correct 0.9072516025641025 loss 1.8853343725204468
epoch 79 iter 39 sum correct 0.906787109375 loss 1.8853685855865479
epoch 79 iter 40 sum correct 0.9067263719512195 loss 1.885377049446106
epoch 79 iter 41 sum correct 0.9066220238095238 loss 1.8853859901428223
epoch 79 iter 42 sum correct 0.9064771075581395 loss 1.8853992223739624
epoch 79 iter 43 sum correct 0.9063387784090909 loss 1.8854089975357056
epoch 79 iter 44 sum correct 0.9063802083333333 loss 1.8854055404663086
epoch 79 iter 45 sum correct 0.9061650815217391 loss 1.8854206800460815
epoch 79 iter 46 sum correct 0.9058759973404256 loss 1.8854438066482544
epoch 79 iter 47 sum correct 0.9059651692708334 loss 1.8854377269744873
epoch 79 iter 48 sum correct 0.9062898596938775 loss 1.8854122161865234
epoch 79 iter 49 sum correct 0.906015625 loss 1.885431170463562
epoch 79 iter 50 sum correct 0.90625 loss 1.8854126930236816
epoch 79 iter 51 sum correct 0.9058743990384616 loss 1.885442852973938
epoch 79 iter 52 sum correct 0.9055129716981132 loss 1.885473608970642
epoch 79 iter 53 sum correct 0.9054542824074074 loss 1.885481834411621
epoch 79 iter 54 sum correct 0.9053622159090909 loss 1.8854897022247314
epoch 79 iter 55 sum correct 0.90478515625 loss 1.885538101196289
epoch 79 iter 56 sum correct 0.8900082236842105 loss 1.885593056678772
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.56it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.16it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.66it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.85it/s]8it [00:01,  5.74it/s]                                 macro  0.5248197801187914
micro  0.6213429924770131
[[259   0  58  19  48  21  62]
 [ 20   0  13   4   8   2   9]
 [ 54   0 236  11  71  60  64]
 [ 35   0  22 709  29  26  74]
 [ 66   0  96  32 281  17 161]
 [  7   0  15  13   8 358  14]
 [ 37   0  57  45  70  11 387]]
              precision    recall  f1-score   support

           0       0.54      0.55      0.55       467
           1       0.00      0.00      0.00        56
           2       0.47      0.48      0.48       496
           3       0.85      0.79      0.82       895
           4       0.55      0.43      0.48       653
           5       0.72      0.86      0.79       415
           6       0.50      0.64      0.56       607

    accuracy                           0.62      3589
   macro avg       0.52      0.54      0.52      3589
weighted avg       0.62      0.62      0.62      3589

correct 0.6213429924770131 
f1 0.5248197801187914 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.78it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.49it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.97it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.20it/s]8it [00:01,  6.18it/s]                                 macro  0.5308785500404765
micro  0.6349958205628309
[[263   0  68  16  60  17  67]
 [ 19   0  16   1  10   2   7]
 [ 54   0 257  13  75  78  51]
 [ 31   0  20 741  28  25  34]
 [ 58   0  83  22 255  18 158]
 [ 11   0  22  20  11 338  14]
 [ 47   0  54  27  51  22 425]]
              precision    recall  f1-score   support

           0       0.54      0.54      0.54       491
           1       0.00      0.00      0.00        55
           2       0.49      0.49      0.49       528
           3       0.88      0.84      0.86       879
           4       0.52      0.43      0.47       594
           5       0.68      0.81      0.74       416
           6       0.56      0.68      0.62       626

    accuracy                           0.63      3589
   macro avg       0.53      0.54      0.53      3589
weighted avg       0.63      0.63      0.63      3589

correct 0.6349958205628309 
f1 0.5308785500404765 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:38,  1.42it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.33it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.85it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.29it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.48it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.71it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.77it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.92it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.91it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.02it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.99it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.09it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  4.05it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.11it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.03it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.11it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.03it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.10it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.03it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.11it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.05it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.11it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.05it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.11it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.04it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.10it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.03it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.10it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.02it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.08it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.01it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.09it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.01it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.08it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.01it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.10it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.04it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.10it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  4.04it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.11it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.03it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.10it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  4.03it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.11it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.04it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.10it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  4.02it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.10it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.03it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.09it/s] 91%|█████████ | 51/56.072265625 [00:12<00:01,  4.03it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:01,  3.98it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  3.93it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.00it/s] 98%|█████████▊| 55/56.072265625 [00:14<00:00,  3.96it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.04it/s]57it [00:14,  4.84it/s]                                  57it [00:14,  3.95it/s]epoch 80 iter 0 sum correct 0.904296875 loss 1.8856287002563477
epoch 80 iter 1 sum correct 0.9033203125 loss 1.8856241703033447
epoch 80 iter 2 sum correct 0.8834635416666666 loss 1.8871488571166992
epoch 80 iter 3 sum correct 0.87646484375 loss 1.887689471244812
epoch 80 iter 4 sum correct 0.87734375 loss 1.887781023979187
epoch 80 iter 5 sum correct 0.8792317708333334 loss 1.8876254558563232
epoch 80 iter 6 sum correct 0.8772321428571429 loss 1.887789249420166
epoch 80 iter 7 sum correct 0.8740234375 loss 1.8880339860916138
epoch 80 iter 8 sum correct 0.8706597222222222 loss 1.8883031606674194
epoch 80 iter 9 sum correct 0.873828125 loss 1.8880611658096313
epoch 80 iter 10 sum correct 0.8728693181818182 loss 1.8881425857543945
epoch 80 iter 11 sum correct 0.8728841145833334 loss 1.8881311416625977
epoch 80 iter 12 sum correct 0.8728966346153846 loss 1.888124942779541
epoch 80 iter 13 sum correct 0.8743024553571429 loss 1.8880153894424438
epoch 80 iter 14 sum correct 0.8729166666666667 loss 1.8881233930587769
epoch 80 iter 15 sum correct 0.8724365234375 loss 1.8881616592407227
epoch 80 iter 16 sum correct 0.8731617647058824 loss 1.888109803199768
epoch 80 iter 17 sum correct 0.8739149305555556 loss 1.8880420923233032
epoch 80 iter 18 sum correct 0.874280427631579 loss 1.8880218267440796
epoch 80 iter 19 sum correct 0.8740234375 loss 1.888037085533142
epoch 80 iter 20 sum correct 0.8737909226190477 loss 1.8880521059036255
epoch 80 iter 21 sum correct 0.8735795454545454 loss 1.8880634307861328
epoch 80 iter 22 sum correct 0.8738960597826086 loss 1.8880491256713867
epoch 80 iter 23 sum correct 0.8732096354166666 loss 1.8881065845489502
epoch 80 iter 24 sum correct 0.8740625 loss 1.8880442380905151
epoch 80 iter 25 sum correct 0.8736478365384616 loss 1.8880733251571655
epoch 80 iter 26 sum correct 0.8742042824074074 loss 1.8880350589752197
epoch 80 iter 27 sum correct 0.8743722098214286 loss 1.8880239725112915
epoch 80 iter 28 sum correct 0.875067349137931 loss 1.8879650831222534
epoch 80 iter 29 sum correct 0.8756510416666666 loss 1.8879151344299316
epoch 80 iter 30 sum correct 0.8760710685483871 loss 1.887880802154541
epoch 80 iter 31 sum correct 0.87664794921875 loss 1.8878358602523804
epoch 80 iter 32 sum correct 0.8768939393939394 loss 1.8878134489059448
epoch 80 iter 33 sum correct 0.876953125 loss 1.8878090381622314
epoch 80 iter 34 sum correct 0.8767857142857143 loss 1.8878180980682373
epoch 80 iter 35 sum correct 0.8768446180555556 loss 1.8878116607666016
epoch 80 iter 36 sum correct 0.8767419763513513 loss 1.8878158330917358
epoch 80 iter 37 sum correct 0.8770559210526315 loss 1.8877900838851929
epoch 80 iter 38 sum correct 0.8769030448717948 loss 1.887802004814148
epoch 80 iter 39 sum correct 0.87724609375 loss 1.8877779245376587
epoch 80 iter 40 sum correct 0.8773818597560976 loss 1.8877633810043335
epoch 80 iter 41 sum correct 0.8779296875 loss 1.887715458869934
epoch 80 iter 42 sum correct 0.8780432412790697 loss 1.8877121210098267
epoch 80 iter 43 sum correct 0.8784623579545454 loss 1.887675404548645
epoch 80 iter 44 sum correct 0.8786024305555555 loss 1.887660264968872
epoch 80 iter 45 sum correct 0.8789911684782609 loss 1.8876317739486694
epoch 80 iter 46 sum correct 0.8791140292553191 loss 1.8876253366470337
epoch 80 iter 47 sum correct 0.8794759114583334 loss 1.8875956535339355
epoch 80 iter 48 sum correct 0.8796635841836735 loss 1.8875826597213745
epoch 80 iter 49 sum correct 0.8799609375 loss 1.8875620365142822
epoch 80 iter 50 sum correct 0.8803232230392157 loss 1.887535810470581
epoch 80 iter 51 sum correct 0.8807091346153846 loss 1.887506365776062
epoch 80 iter 52 sum correct 0.880859375 loss 1.8874961137771606
epoch 80 iter 53 sum correct 0.8807508680555556 loss 1.8875020742416382
epoch 80 iter 54 sum correct 0.8809659090909091 loss 1.8874846696853638
epoch 80 iter 55 sum correct 0.8810337611607143 loss 1.88748037815094
epoch 80 iter 56 sum correct 0.8666392543859649 loss 1.887539029121399
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.59it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.17it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.81it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.02it/s]8it [00:01,  5.87it/s]                                 macro  0.5201811978453793
micro  0.6177208135971023
[[211   0  71  17  88  30  50]
 [ 15   0   9   2  20   1   9]
 [ 37   0 219   9 111  62  58]
 [ 13   0  21 700  53  43  65]
 [ 35   0  77  25 373  15 128]
 [  7   0  16  11  15 356  10]
 [ 22   0  41  47 125  14 358]]
              precision    recall  f1-score   support

           0       0.62      0.45      0.52       467
           1       0.00      0.00      0.00        56
           2       0.48      0.44      0.46       496
           3       0.86      0.78      0.82       895
           4       0.48      0.57      0.52       653
           5       0.68      0.86      0.76       415
           6       0.53      0.59      0.56       607

    accuracy                           0.62      3589
   macro avg       0.52      0.53      0.52      3589
weighted avg       0.62      0.62      0.61      3589

correct 0.6177208135971023 
f1 0.5201811978453793 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.75it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.40it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.60it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.78it/s]8it [00:01,  5.83it/s]                                 macro  0.5306821372787597
micro  0.6344385622736138
[[223   0  81  19  85  28  55]
 [ 14   0  13   3  22   2   1]
 [ 37   0 230   8 122  76  55]
 [ 19   0  17 736  42  38  27]
 [ 36   0  59  22 346  15 116]
 [  7   0  20  10  16 345  18]
 [ 27   0  33  30 119  20 397]]
              precision    recall  f1-score   support

           0       0.61      0.45      0.52       491
           1       0.00      0.00      0.00        55
           2       0.51      0.44      0.47       528
           3       0.89      0.84      0.86       879
           4       0.46      0.58      0.51       594
           5       0.66      0.83      0.73       416
           6       0.59      0.63      0.61       626

    accuracy                           0.63      3589
   macro avg       0.53      0.54      0.53      3589
weighted avg       0.63      0.63      0.63      3589

correct 0.6344385622736138 
f1 0.5306821372787597 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:39,  1.41it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.31it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.82it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.27it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.47it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.71it/s] 12%|█▏        | 7/56.072265625 [00:02<00:12,  3.78it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.95it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.91it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.03it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.98it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.07it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  4.03it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.11it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.06it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.14it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.07it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.12it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.06it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.13it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.07it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.14it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.07it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.14it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.08it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.15it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.09it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.16it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.09it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.15it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.07it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.14it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.08it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.15it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.08it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.15it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.09it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.16it/s] 70%|██████▉   | 39/56.072265625 [00:09<00:04,  4.10it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.16it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.10it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.16it/s] 77%|███████▋  | 43/56.072265625 [00:10<00:03,  4.09it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.15it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.09it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.16it/s] 84%|████████▍ | 47/56.072265625 [00:11<00:02,  4.09it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.16it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.09it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.17it/s] 91%|█████████ | 51/56.072265625 [00:12<00:01,  4.11it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.18it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.13it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.19it/s] 98%|█████████▊| 55/56.072265625 [00:13<00:00,  4.14it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.19it/s]57it [00:14,  4.01it/s]                                  epoch 81 iter 0 sum correct 0.916015625 loss 1.8846967220306396
epoch 81 iter 1 sum correct 0.91796875 loss 1.8845090866088867
epoch 81 iter 2 sum correct 0.91015625 loss 1.8851497173309326
epoch 81 iter 3 sum correct 0.89990234375 loss 1.8860094547271729
epoch 81 iter 4 sum correct 0.894140625 loss 1.8864115476608276
epoch 81 iter 5 sum correct 0.8932291666666666 loss 1.8865487575531006
epoch 81 iter 6 sum correct 0.8922991071428571 loss 1.8866503238677979
epoch 81 iter 7 sum correct 0.88916015625 loss 1.8869104385375977
epoch 81 iter 8 sum correct 0.88671875 loss 1.8871105909347534
epoch 81 iter 9 sum correct 0.88828125 loss 1.8869743347167969
epoch 81 iter 10 sum correct 0.8893821022727273 loss 1.8868604898452759
epoch 81 iter 11 sum correct 0.8880208333333334 loss 1.8869637250900269
epoch 81 iter 12 sum correct 0.8871694711538461 loss 1.8870232105255127
epoch 81 iter 13 sum correct 0.8869977678571429 loss 1.8870344161987305
epoch 81 iter 14 sum correct 0.8859375 loss 1.8871220350265503
epoch 81 iter 15 sum correct 0.884521484375 loss 1.88722825050354
epoch 81 iter 16 sum correct 0.8840762867647058 loss 1.887259602546692
epoch 81 iter 17 sum correct 0.8848741319444444 loss 1.8872028589248657
epoch 81 iter 18 sum correct 0.8830180921052632 loss 1.8873546123504639
epoch 81 iter 19 sum correct 0.8830078125 loss 1.8873590230941772
epoch 81 iter 20 sum correct 0.8830915178571429 loss 1.8873518705368042
epoch 81 iter 21 sum correct 0.8834339488636364 loss 1.8873199224472046
epoch 81 iter 22 sum correct 0.8839164402173914 loss 1.8872718811035156
epoch 81 iter 23 sum correct 0.8841145833333334 loss 1.887254238128662
epoch 81 iter 24 sum correct 0.88421875 loss 1.8872485160827637
epoch 81 iter 25 sum correct 0.8836388221153846 loss 1.8872946500778198
epoch 81 iter 26 sum correct 0.8831741898148148 loss 1.887320876121521
epoch 81 iter 27 sum correct 0.8835100446428571 loss 1.8872929811477661
epoch 81 iter 28 sum correct 0.8842941810344828 loss 1.8872324228286743
epoch 81 iter 29 sum correct 0.8846354166666667 loss 1.887197732925415
epoch 81 iter 30 sum correct 0.8856476814516129 loss 1.887117862701416
epoch 81 iter 31 sum correct 0.88580322265625 loss 1.8870989084243774
epoch 81 iter 32 sum correct 0.8862452651515151 loss 1.8870573043823242
epoch 81 iter 33 sum correct 0.8868336397058824 loss 1.8870158195495605
epoch 81 iter 34 sum correct 0.887109375 loss 1.8869901895523071
epoch 81 iter 35 sum correct 0.8876953125 loss 1.8869481086730957
epoch 81 iter 36 sum correct 0.8878272804054054 loss 1.8869354724884033
epoch 81 iter 37 sum correct 0.887952302631579 loss 1.8869221210479736
epoch 81 iter 38 sum correct 0.8876702724358975 loss 1.8869470357894897
epoch 81 iter 39 sum correct 0.88828125 loss 1.8868961334228516
epoch 81 iter 40 sum correct 0.8884336890243902 loss 1.886879324913025
epoch 81 iter 41 sum correct 0.8890438988095238 loss 1.8868314027786255
epoch 81 iter 42 sum correct 0.889171511627907 loss 1.8868211507797241
epoch 81 iter 43 sum correct 0.8896040482954546 loss 1.8867862224578857
epoch 81 iter 44 sum correct 0.8901909722222222 loss 1.8867367506027222
epoch 81 iter 45 sum correct 0.8901579483695652 loss 1.886738896369934
epoch 81 iter 46 sum correct 0.890375664893617 loss 1.8867202997207642
epoch 81 iter 47 sum correct 0.8910319010416666 loss 1.886669635772705
epoch 81 iter 48 sum correct 0.8911830357142857 loss 1.8866581916809082
epoch 81 iter 49 sum correct 0.891015625 loss 1.8866692781448364
epoch 81 iter 50 sum correct 0.8912760416666666 loss 1.8866453170776367
epoch 81 iter 51 sum correct 0.8917142427884616 loss 1.8866130113601685
epoch 81 iter 52 sum correct 0.8921359080188679 loss 1.8865785598754883
epoch 81 iter 53 sum correct 0.8917100694444444 loss 1.8866089582443237
epoch 81 iter 54 sum correct 0.891903409090909 loss 1.8865916728973389
epoch 81 iter 55 sum correct 0.8921595982142857 loss 1.8865703344345093
epoch 81 iter 56 sum correct 0.8776041666666666 loss 1.8866082429885864
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.62it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.18it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.40it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.64it/s]8it [00:01,  5.68it/s]                                 macro  0.5287640866852137
micro  0.6283087210922262
[[252   0  45  28  98  20  24]
 [ 27   0   6   2  16   0   5]
 [ 52   0 202  16 155  44  27]
 [ 14   0  16 751  57  21  36]
 [ 61   0  48  41 426  11  66]
 [ 11   0  17  19  18 341   9]
 [ 47   0  30  57 183   7 283]]
              precision    recall  f1-score   support

           0       0.54      0.54      0.54       467
           1       0.00      0.00      0.00        56
           2       0.55      0.41      0.47       496
           3       0.82      0.84      0.83       895
           4       0.45      0.65      0.53       653
           5       0.77      0.82      0.79       415
           6       0.63      0.47      0.54       607

    accuracy                           0.63      3589
   macro avg       0.54      0.53      0.53      3589
weighted avg       0.63      0.63      0.62      3589

correct 0.6283087210922262 
f1 0.5287640866852137 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.72it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.41it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.76it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.03it/s]8it [00:01,  5.96it/s]                                 macro  0.535282083454497
micro  0.636110337141265
[[275   0  52  28  86  13  37]
 [ 24   0   8   3  17   2   1]
 [ 50   0 227  31 144  50  26]
 [ 10   0  16 774  45  17  17]
 [ 62   0  43  38 372  11  68]
 [  8   0  29  27  15 328   9]
 [ 56   0  36  49 168  10 307]]
              precision    recall  f1-score   support

           0       0.57      0.56      0.56       491
           1       0.00      0.00      0.00        55
           2       0.55      0.43      0.48       528
           3       0.81      0.88      0.85       879
           4       0.44      0.63      0.52       594
           5       0.76      0.79      0.77       416
           6       0.66      0.49      0.56       626

    accuracy                           0.64      3589
   macro avg       0.54      0.54      0.54      3589
weighted avg       0.63      0.64      0.63      3589

correct 0.636110337141265 
f1 0.535282083454497 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:38,  1.42it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.31it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.80it/s]  7%|▋         | 4/56.072265625 [00:01<00:16,  3.23it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.43it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.68it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.74it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.90it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.89it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.01it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.98it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.07it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  4.03it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.11it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.04it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.12it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.07it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.13it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.07it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.13it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.03it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  3.85it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  3.84it/s] 43%|████▎     | 24/56.072265625 [00:06<00:08,  3.97it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  3.95it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.05it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  3.98it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.06it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.01it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.10it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.03it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.11it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.07it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.12it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.04it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.08it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  3.98it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.07it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  4.00it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.09it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.01it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.10it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  4.02it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.10it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.01it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.10it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  4.01it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.10it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.03it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.11it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  4.05it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.13it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.05it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.13it/s] 98%|█████████▊| 55/56.072265625 [00:14<00:00,  4.06it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.13it/s]57it [00:14,  4.99it/s]                                  57it [00:14,  3.94it/s]epoch 82 iter 0 sum correct 0.900390625 loss 1.8857781887054443
epoch 82 iter 1 sum correct 0.9072265625 loss 1.8851823806762695
epoch 82 iter 2 sum correct 0.9010416666666666 loss 1.8857052326202393
epoch 82 iter 3 sum correct 0.89892578125 loss 1.8859028816223145
epoch 82 iter 4 sum correct 0.899609375 loss 1.8858550786972046
epoch 82 iter 5 sum correct 0.9016927083333334 loss 1.8857266902923584
epoch 82 iter 6 sum correct 0.9006696428571429 loss 1.8858084678649902
epoch 82 iter 7 sum correct 0.900390625 loss 1.885831594467163
epoch 82 iter 8 sum correct 0.90234375 loss 1.8856912851333618
epoch 82 iter 9 sum correct 0.9005859375 loss 1.8858331441879272
epoch 82 iter 10 sum correct 0.9000355113636364 loss 1.8858660459518433
epoch 82 iter 11 sum correct 0.900390625 loss 1.8858389854431152
epoch 82 iter 12 sum correct 0.9006911057692307 loss 1.8858251571655273
epoch 82 iter 13 sum correct 0.9006696428571429 loss 1.8858217000961304
epoch 82 iter 14 sum correct 0.901171875 loss 1.885776400566101
epoch 82 iter 15 sum correct 0.90185546875 loss 1.8857142925262451
epoch 82 iter 16 sum correct 0.9007352941176471 loss 1.88580322265625
epoch 82 iter 17 sum correct 0.8998480902777778 loss 1.885873794555664
epoch 82 iter 18 sum correct 0.900390625 loss 1.885823130607605
epoch 82 iter 19 sum correct 0.90048828125 loss 1.885815978050232
epoch 82 iter 20 sum correct 0.9007626488095238 loss 1.8857940435409546
epoch 82 iter 21 sum correct 0.9013671875 loss 1.885746717453003
epoch 82 iter 22 sum correct 0.901749320652174 loss 1.8857111930847168
epoch 82 iter 23 sum correct 0.901611328125 loss 1.8857206106185913
epoch 82 iter 24 sum correct 0.9025 loss 1.885657787322998
epoch 82 iter 25 sum correct 0.9027193509615384 loss 1.8856266736984253
epoch 82 iter 26 sum correct 0.9034288194444444 loss 1.885569453239441
epoch 82 iter 27 sum correct 0.9033900669642857 loss 1.8855689764022827
epoch 82 iter 28 sum correct 0.9036907327586207 loss 1.8855438232421875
epoch 82 iter 29 sum correct 0.9047526041666667 loss 1.885466456413269
epoch 82 iter 30 sum correct 0.9051789314516129 loss 1.8854331970214844
epoch 82 iter 31 sum correct 0.90557861328125 loss 1.88539719581604
epoch 82 iter 32 sum correct 0.9057765151515151 loss 1.885380744934082
epoch 82 iter 33 sum correct 0.9058478860294118 loss 1.8853777647018433
epoch 82 iter 34 sum correct 0.90625 loss 1.8853435516357422
epoch 82 iter 35 sum correct 0.9059787326388888 loss 1.8853658437728882
epoch 82 iter 36 sum correct 0.90625 loss 1.8853378295898438
epoch 82 iter 37 sum correct 0.9057360197368421 loss 1.8853809833526611
epoch 82 iter 38 sum correct 0.9057491987179487 loss 1.8853826522827148
epoch 82 iter 39 sum correct 0.905859375 loss 1.8853737115859985
epoch 82 iter 40 sum correct 0.9055830792682927 loss 1.885400414466858
epoch 82 iter 41 sum correct 0.9046688988095238 loss 1.8854706287384033
epoch 82 iter 42 sum correct 0.9043877180232558 loss 1.8854954242706299
epoch 82 iter 43 sum correct 0.9041637073863636 loss 1.8855128288269043
epoch 82 iter 44 sum correct 0.9046875 loss 1.8854725360870361
epoch 82 iter 45 sum correct 0.9045940896739131 loss 1.885479211807251
epoch 82 iter 46 sum correct 0.9044630984042553 loss 1.885488748550415
epoch 82 iter 47 sum correct 0.9041341145833334 loss 1.8855170011520386
epoch 82 iter 48 sum correct 0.9040178571428571 loss 1.8855278491973877
epoch 82 iter 49 sum correct 0.9040234375 loss 1.88552725315094
epoch 82 iter 50 sum correct 0.9037990196078431 loss 1.8855470418930054
epoch 82 iter 51 sum correct 0.9038461538461539 loss 1.8855446577072144
epoch 82 iter 52 sum correct 0.9038178066037735 loss 1.8855441808700562
epoch 82 iter 53 sum correct 0.9036096643518519 loss 1.8855621814727783
epoch 82 iter 54 sum correct 0.9035511363636364 loss 1.885568618774414
epoch 82 iter 55 sum correct 0.9036342075892857 loss 1.885565996170044
epoch 82 iter 56 sum correct 0.8888089364035088 loss 1.8856751918792725
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.67it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.37it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.87it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.09it/s]8it [00:01,  6.01it/s]                                 macro  0.5390656341761006
micro  0.6397325160211759
[[237   0  57  36  57  21  59]
 [ 21   0  11   4  10   0  10]
 [ 43   0 240  23  86  48  56]
 [  6   0  18 764  41  16  50]
 [ 44   0  97  41 332  11 128]
 [  6   0  19  24  13 339  14]
 [ 21   0  29  59 105   9 384]]
              precision    recall  f1-score   support

           0       0.63      0.51      0.56       467
           1       0.00      0.00      0.00        56
           2       0.51      0.48      0.50       496
           3       0.80      0.85      0.83       895
           4       0.52      0.51      0.51       653
           5       0.76      0.82      0.79       415
           6       0.55      0.63      0.59       607

    accuracy                           0.64      3589
   macro avg       0.54      0.54      0.54      3589
weighted avg       0.63      0.64      0.63      3589

correct 0.6397325160211759 
f1 0.5390656341761006 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.71it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.29it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.78it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.97it/s]8it [00:01,  5.91it/s]                                 macro  0.5383882058771311
micro  0.6436333240456952
[[245   0  60  29  72  17  68]
 [ 16   0  15   7  11   2   4]
 [ 43   0 254  30  87  53  61]
 [  8   0  15 789  31  17  19]
 [ 47   0  91  47 282   9 118]
 [  3   0  29  29  13 324  18]
 [ 23   0  35  51  90  11 416]]
              precision    recall  f1-score   support

           0       0.64      0.50      0.56       491
           1       0.00      0.00      0.00        55
           2       0.51      0.48      0.49       528
           3       0.80      0.90      0.85       879
           4       0.48      0.47      0.48       594
           5       0.75      0.78      0.76       416
           6       0.59      0.66      0.63       626

    accuracy                           0.64      3589
   macro avg       0.54      0.54      0.54      3589
weighted avg       0.63      0.64      0.63      3589

correct 0.6436333240456952 
f1 0.5383882058771311 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:38,  1.42it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.34it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.85it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.29it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.48it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.71it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.73it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.90it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.91it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.01it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.97it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.07it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  4.03it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.11it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.04it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.12it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.04it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.12it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.04it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.12it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.06it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.13it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.08it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.14it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.05it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.10it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.04it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.11it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.06it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.13it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  3.98it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.01it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  3.91it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  3.97it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  3.90it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:05,  3.96it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  3.91it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  3.97it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  3.88it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:04,  3.94it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  3.86it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  3.96it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  3.93it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.03it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  3.99it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.08it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  4.03it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.12it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.05it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.14it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  4.10it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.17it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.11it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.17it/s] 98%|█████████▊| 55/56.072265625 [00:14<00:00,  4.12it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.18it/s]57it [00:14,  5.06it/s]                                  57it [00:14,  3.95it/s]epoch 83 iter 0 sum correct 0.94140625 loss 1.8826789855957031
epoch 83 iter 1 sum correct 0.919921875 loss 1.8842805624008179
epoch 83 iter 2 sum correct 0.9127604166666666 loss 1.8848493099212646
epoch 83 iter 3 sum correct 0.91162109375 loss 1.8849427700042725
epoch 83 iter 4 sum correct 0.9046875 loss 1.8855282068252563
epoch 83 iter 5 sum correct 0.9007161458333334 loss 1.8858393430709839
epoch 83 iter 6 sum correct 0.9029017857142857 loss 1.8856654167175293
epoch 83 iter 7 sum correct 0.90283203125 loss 1.8856703042984009
epoch 83 iter 8 sum correct 0.904296875 loss 1.8855631351470947
epoch 83 iter 9 sum correct 0.9048828125 loss 1.8855444192886353
epoch 83 iter 10 sum correct 0.9046519886363636 loss 1.8855773210525513
epoch 83 iter 11 sum correct 0.9013671875 loss 1.8858280181884766
epoch 83 iter 12 sum correct 0.9012920673076923 loss 1.8858414888381958
epoch 83 iter 13 sum correct 0.900390625 loss 1.885890245437622
epoch 83 iter 14 sum correct 0.9001302083333333 loss 1.8859044313430786
epoch 83 iter 15 sum correct 0.89892578125 loss 1.8859988451004028
epoch 83 iter 16 sum correct 0.8990119485294118 loss 1.8859894275665283
epoch 83 iter 17 sum correct 0.8989800347222222 loss 1.8859888315200806
epoch 83 iter 18 sum correct 0.8992598684210527 loss 1.8859663009643555
epoch 83 iter 19 sum correct 0.89794921875 loss 1.8860714435577393
epoch 83 iter 20 sum correct 0.8971354166666666 loss 1.8861349821090698
epoch 83 iter 21 sum correct 0.8958629261363636 loss 1.886229157447815
epoch 83 iter 22 sum correct 0.8955502717391305 loss 1.8862601518630981
epoch 83 iter 23 sum correct 0.8953450520833334 loss 1.886285662651062
epoch 83 iter 24 sum correct 0.89515625 loss 1.8863035440444946
epoch 83 iter 25 sum correct 0.8954326923076923 loss 1.8862804174423218
epoch 83 iter 26 sum correct 0.8956163194444444 loss 1.8862709999084473
epoch 83 iter 27 sum correct 0.8949497767857143 loss 1.8863260746002197
epoch 83 iter 28 sum correct 0.8954067887931034 loss 1.886288046836853
epoch 83 iter 29 sum correct 0.8952473958333333 loss 1.886298418045044
epoch 83 iter 30 sum correct 0.8959173387096774 loss 1.8862454891204834
epoch 83 iter 31 sum correct 0.89556884765625 loss 1.8862732648849487
epoch 83 iter 32 sum correct 0.894827178030303 loss 1.8863331079483032
epoch 83 iter 33 sum correct 0.89453125 loss 1.8863637447357178
epoch 83 iter 34 sum correct 0.8946986607142857 loss 1.886347770690918
epoch 83 iter 35 sum correct 0.8943142361111112 loss 1.886375069618225
epoch 83 iter 36 sum correct 0.8945840371621622 loss 1.8863548040390015
epoch 83 iter 37 sum correct 0.8946854440789473 loss 1.886344075202942
epoch 83 iter 38 sum correct 0.8949819711538461 loss 1.886322259902954
epoch 83 iter 39 sum correct 0.89521484375 loss 1.886299967765808
epoch 83 iter 40 sum correct 0.895531631097561 loss 1.8862793445587158
epoch 83 iter 41 sum correct 0.8954148065476191 loss 1.8862884044647217
epoch 83 iter 42 sum correct 0.8956667877906976 loss 1.886266827583313
epoch 83 iter 43 sum correct 0.8952414772727273 loss 1.8863013982772827
epoch 83 iter 44 sum correct 0.8947916666666667 loss 1.8863366842269897
epoch 83 iter 45 sum correct 0.8946586277173914 loss 1.8863435983657837
epoch 83 iter 46 sum correct 0.8944065824468085 loss 1.886364459991455
epoch 83 iter 47 sum correct 0.8947347005208334 loss 1.886340618133545
epoch 83 iter 48 sum correct 0.8950095663265306 loss 1.8863168954849243
epoch 83 iter 49 sum correct 0.8948828125 loss 1.8863259553909302
epoch 83 iter 50 sum correct 0.8948759191176471 loss 1.88632333278656
epoch 83 iter 51 sum correct 0.8946063701923077 loss 1.8863438367843628
epoch 83 iter 52 sum correct 0.894752358490566 loss 1.8863310813903809
epoch 83 iter 53 sum correct 0.8946035879629629 loss 1.8863461017608643
epoch 83 iter 54 sum correct 0.8947798295454545 loss 1.8863298892974854
epoch 83 iter 55 sum correct 0.8948102678571429 loss 1.8863259553909302
epoch 83 iter 56 sum correct 0.880139802631579 loss 1.8864449262619019
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.61it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.22it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.64it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.80it/s]8it [00:01,  5.73it/s]                                 macro  0.5142861168255551
micro  0.614934522151017
[[279   0  68  34  45  15  26]
 [ 29   0  15   5   4   0   3]
 [ 57   0 246  26  66  59  42]
 [ 28   0  27 769  10  19  42]
 [ 88   0 133  55 272  19  86]
 [  9   0  19  20   8 352   7]
 [ 84   0  68  64  75  27 289]]
              precision    recall  f1-score   support

           0       0.49      0.60      0.54       467
           1       0.00      0.00      0.00        56
           2       0.43      0.50      0.46       496
           3       0.79      0.86      0.82       895
           4       0.57      0.42      0.48       653
           5       0.72      0.85      0.78       415
           6       0.58      0.48      0.52       607

    accuracy                           0.61      3589
   macro avg       0.51      0.53      0.51      3589
weighted avg       0.60      0.61      0.60      3589

correct 0.614934522151017 
f1 0.5142861168255551 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.73it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.39it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.52it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.85it/s]8it [00:01,  5.91it/s]                                 macro  0.5236406513484216
micro  0.6291446085260518
[[280   0  80  37  42  24  28]
 [ 25   0  14   9   3   2   2]
 [ 58   0 282  33  65  59  31]
 [  9   0  21 788  17  22  22]
 [ 79   0 128  47 234  19  87]
 [ 10   0  30  22   8 341   5]
 [ 63   0  65  50  76  39 333]]
              precision    recall  f1-score   support

           0       0.53      0.57      0.55       491
           1       0.00      0.00      0.00        55
           2       0.45      0.53      0.49       528
           3       0.80      0.90      0.85       879
           4       0.53      0.39      0.45       594
           5       0.67      0.82      0.74       416
           6       0.66      0.53      0.59       626

    accuracy                           0.63      3589
   macro avg       0.52      0.54      0.52      3589
weighted avg       0.62      0.63      0.62      3589

correct 0.6291446085260518 
f1 0.5236406513484216 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:39,  1.41it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.32it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.83it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.28it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.48it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.73it/s] 12%|█▏        | 7/56.072265625 [00:02<00:12,  3.80it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.96it/s] 16%|█▌        | 9/56.072265625 [00:02<00:11,  3.93it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  3.98it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.96it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.06it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  4.02it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.10it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.04it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.11it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.03it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.03it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  3.99it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.04it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.00it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.09it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.04it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.11it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.05it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.06it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.00it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.09it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.03it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.04it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  3.98it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:06,  3.98it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  3.89it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.00it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  3.97it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.07it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  3.97it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.00it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  3.91it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.03it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  3.98it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.07it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  4.03it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.04it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  3.91it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  3.96it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  3.88it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:02,  3.63it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  3.66it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  3.80it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  3.85it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:01,  3.98it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  3.96it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.06it/s] 98%|█████████▊| 55/56.072265625 [00:14<00:00,  4.04it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.11it/s]57it [00:14,  4.99it/s]                                  57it [00:14,  3.90it/s]epoch 84 iter 0 sum correct 0.89453125 loss 1.8863388299942017
epoch 84 iter 1 sum correct 0.8857421875 loss 1.8869727849960327
epoch 84 iter 2 sum correct 0.8919270833333334 loss 1.8864926099777222
epoch 84 iter 3 sum correct 0.89453125 loss 1.886279821395874
epoch 84 iter 4 sum correct 0.89453125 loss 1.886307954788208
epoch 84 iter 5 sum correct 0.896484375 loss 1.886141300201416
epoch 84 iter 6 sum correct 0.8959263392857143 loss 1.8861849308013916
epoch 84 iter 7 sum correct 0.89599609375 loss 1.8861891031265259
epoch 84 iter 8 sum correct 0.8962673611111112 loss 1.886177897453308
epoch 84 iter 9 sum correct 0.8962890625 loss 1.8861888647079468
epoch 84 iter 10 sum correct 0.8957741477272727 loss 1.8862171173095703
epoch 84 iter 11 sum correct 0.8963216145833334 loss 1.886197805404663
epoch 84 iter 12 sum correct 0.8970853365384616 loss 1.8861403465270996
epoch 84 iter 13 sum correct 0.8981584821428571 loss 1.8860517740249634
epoch 84 iter 14 sum correct 0.8998697916666667 loss 1.885915994644165
epoch 84 iter 15 sum correct 0.900390625 loss 1.8858693838119507
epoch 84 iter 16 sum correct 0.8998161764705882 loss 1.8859204053878784
epoch 84 iter 17 sum correct 0.9004991319444444 loss 1.8858598470687866
epoch 84 iter 18 sum correct 0.8999794407894737 loss 1.8858877420425415
epoch 84 iter 19 sum correct 0.9009765625 loss 1.8858143091201782
epoch 84 iter 20 sum correct 0.9017857142857143 loss 1.885742425918579
epoch 84 iter 21 sum correct 0.9008345170454546 loss 1.8858184814453125
epoch 84 iter 22 sum correct 0.9013247282608695 loss 1.8857853412628174
epoch 84 iter 23 sum correct 0.9013671875 loss 1.8857845067977905
epoch 84 iter 24 sum correct 0.902109375 loss 1.885725975036621
epoch 84 iter 25 sum correct 0.9018179086538461 loss 1.8857399225234985
epoch 84 iter 26 sum correct 0.9019097222222222 loss 1.8857300281524658
epoch 84 iter 27 sum correct 0.9015764508928571 loss 1.8857556581497192
epoch 84 iter 28 sum correct 0.9020070043103449 loss 1.8857197761535645
epoch 84 iter 29 sum correct 0.9015625 loss 1.8857524394989014
epoch 84 iter 30 sum correct 0.9026587701612904 loss 1.8856645822525024
epoch 84 iter 31 sum correct 0.9029541015625 loss 1.885642647743225
epoch 84 iter 32 sum correct 0.902639678030303 loss 1.8856632709503174
epoch 84 iter 33 sum correct 0.9033203125 loss 1.8856154680252075
epoch 84 iter 34 sum correct 0.903125 loss 1.8856325149536133
epoch 84 iter 35 sum correct 0.9044053819444444 loss 1.8855366706848145
epoch 84 iter 36 sum correct 0.9043496621621622 loss 1.885538935661316
epoch 84 iter 37 sum correct 0.9038856907894737 loss 1.885575532913208
epoch 84 iter 38 sum correct 0.9033954326923077 loss 1.885608196258545
epoch 84 iter 39 sum correct 0.903369140625 loss 1.8856089115142822
epoch 84 iter 40 sum correct 0.903344131097561 loss 1.8856152296066284
epoch 84 iter 41 sum correct 0.9030412946428571 loss 1.8856383562088013
epoch 84 iter 42 sum correct 0.9033884447674418 loss 1.8856146335601807
epoch 84 iter 43 sum correct 0.9033203125 loss 1.8856192827224731
epoch 84 iter 44 sum correct 0.9025173611111111 loss 1.8856817483901978
epoch 84 iter 45 sum correct 0.9029806385869565 loss 1.8856433629989624
epoch 84 iter 46 sum correct 0.9029255319148937 loss 1.8856401443481445
epoch 84 iter 47 sum correct 0.9027913411458334 loss 1.8856481313705444
epoch 84 iter 48 sum correct 0.9025829081632653 loss 1.8856643438339233
epoch 84 iter 49 sum correct 0.9025390625 loss 1.8856669664382935
epoch 84 iter 50 sum correct 0.9025352328431373 loss 1.885668158531189
epoch 84 iter 51 sum correct 0.9023813100961539 loss 1.8856791257858276
epoch 84 iter 52 sum correct 0.9022331957547169 loss 1.885690689086914
epoch 84 iter 53 sum correct 0.9021267361111112 loss 1.885697364807129
epoch 84 iter 54 sum correct 0.9017400568181818 loss 1.8857282400131226
epoch 84 iter 55 sum correct 0.9019252232142857 loss 1.885711431503296
epoch 84 iter 56 sum correct 0.8872669956140351 loss 1.8856887817382812
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.62it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.22it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.70it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.97it/s]8it [00:01,  5.86it/s]                                 macro  0.5354208359762469
micro  0.63137364168292
[[248   0  50  26  63  12  68]
 [ 26   0   7   3  10   0  10]
 [ 43   0 230  12  99  40  72]
 [ 13   0  24 730  20  15  93]
 [ 58   0  86  31 328   5 145]
 [ 11   0  28  19  10 329  18]
 [ 30   0  29  38 103   6 401]]
              precision    recall  f1-score   support

           0       0.58      0.53      0.55       467
           1       0.00      0.00      0.00        56
           2       0.51      0.46      0.48       496
           3       0.85      0.82      0.83       895
           4       0.52      0.50      0.51       653
           5       0.81      0.79      0.80       415
           6       0.50      0.66      0.57       607

    accuracy                           0.63      3589
   macro avg       0.54      0.54      0.54      3589
weighted avg       0.63      0.63      0.63      3589

correct 0.63137364168292 
f1 0.5354208359762469 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.86it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.58it/s] 71%|███████▏  | 5/7.009765625 [00:00<00:00,  6.05it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.20it/s]8it [00:01,  6.24it/s]                                 macro  0.5511685404358092
micro  0.6536639732516021
[[275   0  53  23  59  10  71]
 [ 27   0   8   3   9   2   6]
 [ 51   0 249  19 108  43  58]
 [ 13   0  18 753  30  11  54]
 [ 49   0  68  30 312   7 128]
 [  8   0  44  26  14 309  15]
 [ 28   0  25  20  96   9 448]]
              precision    recall  f1-score   support

           0       0.61      0.56      0.58       491
           1       0.00      0.00      0.00        55
           2       0.54      0.47      0.50       528
           3       0.86      0.86      0.86       879
           4       0.50      0.53      0.51       594
           5       0.79      0.74      0.77       416
           6       0.57      0.72      0.64       626

    accuracy                           0.65      3589
   macro avg       0.55      0.55      0.55      3589
weighted avg       0.65      0.65      0.65      3589

correct 0.6536639732516021 
f1 0.5511685404358092 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:38,  1.43it/s]  4%|▎         | 2/56.072265625 [00:00<00:22,  2.36it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.87it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.29it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.47it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.70it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.76it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.92it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.89it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  3.99it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.92it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.03it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  3.92it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.02it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  3.97it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.07it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.03it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.12it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.05it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.13it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.07it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.14it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.06it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.13it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.06it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.12it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.04it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.11it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.05it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.12it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.05it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.13it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.06it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.13it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.07it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.14it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.06it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.13it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  4.07it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.14it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.08it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.15it/s] 77%|███████▋  | 43/56.072265625 [00:10<00:03,  4.08it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.15it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.06it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.14it/s] 84%|████████▍ | 47/56.072265625 [00:11<00:02,  4.08it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.15it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.08it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.16it/s] 91%|█████████ | 51/56.072265625 [00:12<00:01,  4.10it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.17it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.11it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.17it/s] 98%|█████████▊| 55/56.072265625 [00:13<00:00,  4.11it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.18it/s]57it [00:14,  5.05it/s]                                  57it [00:14,  3.98it/s]epoch 85 iter 0 sum correct 0.939453125 loss 1.8827241659164429
epoch 85 iter 1 sum correct 0.92578125 loss 1.883704662322998
epoch 85 iter 2 sum correct 0.9114583333333334 loss 1.884826898574829
epoch 85 iter 3 sum correct 0.91064453125 loss 1.8850147724151611
epoch 85 iter 4 sum correct 0.9125 loss 1.8848623037338257
epoch 85 iter 5 sum correct 0.9147135416666666 loss 1.8846887350082397
epoch 85 iter 6 sum correct 0.9126674107142857 loss 1.8848515748977661
epoch 85 iter 7 sum correct 0.912841796875 loss 1.8848432302474976
epoch 85 iter 8 sum correct 0.9136284722222222 loss 1.884765625
epoch 85 iter 9 sum correct 0.9123046875 loss 1.8848680257797241
epoch 85 iter 10 sum correct 0.9110440340909091 loss 1.8849519491195679
epoch 85 iter 11 sum correct 0.9114583333333334 loss 1.884907603263855
epoch 85 iter 12 sum correct 0.9118088942307693 loss 1.884872555732727
epoch 85 iter 13 sum correct 0.9135044642857143 loss 1.8847427368164062
epoch 85 iter 14 sum correct 0.9134114583333334 loss 1.8847522735595703
epoch 85 iter 15 sum correct 0.9124755859375 loss 1.8848378658294678
epoch 85 iter 16 sum correct 0.9124540441176471 loss 1.8848419189453125
epoch 85 iter 17 sum correct 0.9124348958333334 loss 1.88484787940979
epoch 85 iter 18 sum correct 0.9115953947368421 loss 1.8849149942398071
epoch 85 iter 19 sum correct 0.9107421875 loss 1.8849886655807495
epoch 85 iter 20 sum correct 0.9095982142857143 loss 1.8850793838500977
epoch 85 iter 21 sum correct 0.9094460227272727 loss 1.8850924968719482
epoch 85 iter 22 sum correct 0.9093919836956522 loss 1.8851032257080078
epoch 85 iter 23 sum correct 0.9083658854166666 loss 1.8851804733276367
epoch 85 iter 24 sum correct 0.90921875 loss 1.8851121664047241
epoch 85 iter 25 sum correct 0.9098557692307693 loss 1.8850661516189575
epoch 85 iter 26 sum correct 0.9108072916666666 loss 1.8849900960922241
epoch 85 iter 27 sum correct 0.9107840401785714 loss 1.8849834203720093
epoch 85 iter 28 sum correct 0.9100215517241379 loss 1.885040521621704
epoch 85 iter 29 sum correct 0.91015625 loss 1.885027527809143
epoch 85 iter 30 sum correct 0.9099042338709677 loss 1.8850483894348145
epoch 85 iter 31 sum correct 0.910400390625 loss 1.8850133419036865
epoch 85 iter 32 sum correct 0.9096235795454546 loss 1.8850711584091187
epoch 85 iter 33 sum correct 0.9098690257352942 loss 1.8850573301315308
epoch 85 iter 34 sum correct 0.9094308035714286 loss 1.8850913047790527
epoch 85 iter 35 sum correct 0.9094509548611112 loss 1.885092854499817
epoch 85 iter 36 sum correct 0.9095228040540541 loss 1.8850970268249512
epoch 85 iter 37 sum correct 0.9092824835526315 loss 1.885115385055542
epoch 85 iter 38 sum correct 0.9090544871794872 loss 1.8851349353790283
epoch 85 iter 39 sum correct 0.9083984375 loss 1.8851851224899292
epoch 85 iter 40 sum correct 0.9081554878048781 loss 1.8852084875106812
epoch 85 iter 41 sum correct 0.9078311011904762 loss 1.8852347135543823
epoch 85 iter 42 sum correct 0.9079760174418605 loss 1.8852226734161377
epoch 85 iter 43 sum correct 0.9081143465909091 loss 1.885216474533081
epoch 85 iter 44 sum correct 0.9080729166666667 loss 1.8852165937423706
epoch 85 iter 45 sum correct 0.9079908288043478 loss 1.885223150253296
epoch 85 iter 46 sum correct 0.9078706781914894 loss 1.8852308988571167
epoch 85 iter 47 sum correct 0.907958984375 loss 1.885227084159851
epoch 85 iter 48 sum correct 0.9073660714285714 loss 1.8852752447128296
epoch 85 iter 49 sum correct 0.9077734375 loss 1.8852416276931763
epoch 85 iter 50 sum correct 0.9078584558823529 loss 1.885231852531433
epoch 85 iter 51 sum correct 0.9081280048076923 loss 1.885208249092102
epoch 85 iter 52 sum correct 0.9080925707547169 loss 1.8852121829986572
epoch 85 iter 53 sum correct 0.9074435763888888 loss 1.8852612972259521
epoch 85 iter 54 sum correct 0.907421875 loss 1.8852614164352417
epoch 85 iter 55 sum correct 0.9069475446428571 loss 1.885302186012268
epoch 85 iter 56 sum correct 0.8920641447368421 loss 1.8854306936264038
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.65it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.26it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.60it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.69it/s]8it [00:01,  5.74it/s]                                 macro  0.5392036585263994
micro  0.64084703259961
[[288   0  32  25  57   9  56]
 [ 26   0   9   5  10   0   6]
 [ 56   0 191  24 121  42  62]
 [ 35   0   8 765  25  13  49]
 [ 81   0  58  39 336   5 134]
 [ 16   0  21  21  11 330  16]
 [ 36   0  30  55  92   4 390]]
              precision    recall  f1-score   support

           0       0.54      0.62      0.57       467
           1       0.00      0.00      0.00        56
           2       0.55      0.39      0.45       496
           3       0.82      0.85      0.84       895
           4       0.52      0.51      0.51       653
           5       0.82      0.80      0.81       415
           6       0.55      0.64      0.59       607

    accuracy                           0.64      3589
   macro avg       0.54      0.54      0.54      3589
weighted avg       0.63      0.64      0.63      3589

correct 0.64084703259961 
f1 0.5392036585263994 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.77it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.50it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.59it/s] 86%|████████▌ | 6/7.009765625 [00:01<00:00,  6.34it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.97it/s]8it [00:01,  5.91it/s]                                 macro  0.546314245470532
micro  0.651992198383951
[[297   0  36  27  72   2  57]
 [ 27   0   7   6   9   2   4]
 [ 75   0 210  30 106  39  68]
 [ 21   0  12 781  27  15  23]
 [ 59   0  44  37 318   6 130]
 [ 12   0  28  34  12 315  15]
 [ 40   0  24  40  84  19 419]]
              precision    recall  f1-score   support

           0       0.56      0.60      0.58       491
           1       0.00      0.00      0.00        55
           2       0.58      0.40      0.47       528
           3       0.82      0.89      0.85       879
           4       0.51      0.54      0.52       594
           5       0.79      0.76      0.77       416
           6       0.59      0.67      0.62       626

    accuracy                           0.65      3589
   macro avg       0.55      0.55      0.55      3589
weighted avg       0.64      0.65      0.64      3589

correct 0.651992198383951 
f1 0.546314245470532 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:38,  1.45it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.35it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.86it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.30it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.49it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.70it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.77it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.92it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.90it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.00it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.98it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.06it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  4.00it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.07it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.02it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.09it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.02it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.09it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.04it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.10it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.03it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.09it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.02it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.08it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.01it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.08it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.02it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.08it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.02it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.09it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.03it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.10it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.05it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.11it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.03it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.09it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.04it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.10it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  4.04it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.11it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.03it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.10it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  4.02it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.09it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.02it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.09it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  4.02it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.09it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.02it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.09it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  4.05it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.11it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.05it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.11it/s] 98%|█████████▊| 55/56.072265625 [00:13<00:00,  4.06it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.12it/s]57it [00:14,  4.94it/s]                                  57it [00:14,  3.96it/s]epoch 86 iter 0 sum correct 0.90625 loss 1.8853774070739746
epoch 86 iter 1 sum correct 0.9052734375 loss 1.8854317665100098
epoch 86 iter 2 sum correct 0.9069010416666666 loss 1.8853607177734375
epoch 86 iter 3 sum correct 0.90869140625 loss 1.885182499885559
epoch 86 iter 4 sum correct 0.90859375 loss 1.8852037191390991
epoch 86 iter 5 sum correct 0.9088541666666666 loss 1.885176181793213
epoch 86 iter 6 sum correct 0.9084821428571429 loss 1.8851943016052246
epoch 86 iter 7 sum correct 0.91015625 loss 1.8850680589675903
epoch 86 iter 8 sum correct 0.9095052083333334 loss 1.8851125240325928
epoch 86 iter 9 sum correct 0.9103515625 loss 1.885049819946289
epoch 86 iter 10 sum correct 0.9105113636363636 loss 1.8850398063659668
epoch 86 iter 11 sum correct 0.9114583333333334 loss 1.884955883026123
epoch 86 iter 12 sum correct 0.9113581730769231 loss 1.8849694728851318
epoch 86 iter 13 sum correct 0.9118303571428571 loss 1.8849306106567383
epoch 86 iter 14 sum correct 0.9118489583333333 loss 1.8849204778671265
epoch 86 iter 15 sum correct 0.9111328125 loss 1.8849663734436035
epoch 86 iter 16 sum correct 0.9114200367647058 loss 1.8849458694458008
epoch 86 iter 17 sum correct 0.9098307291666666 loss 1.885074257850647
epoch 86 iter 18 sum correct 0.908203125 loss 1.8852077722549438
epoch 86 iter 19 sum correct 0.90751953125 loss 1.8852685689926147
epoch 86 iter 20 sum correct 0.9070870535714286 loss 1.8853040933609009
epoch 86 iter 21 sum correct 0.9070490056818182 loss 1.885310173034668
epoch 86 iter 22 sum correct 0.9077785326086957 loss 1.8852458000183105
epoch 86 iter 23 sum correct 0.9070638020833334 loss 1.8853046894073486
epoch 86 iter 24 sum correct 0.90671875 loss 1.885327696800232
epoch 86 iter 25 sum correct 0.9070763221153846 loss 1.885306477546692
epoch 86 iter 26 sum correct 0.9067563657407407 loss 1.8853291273117065
epoch 86 iter 27 sum correct 0.9074358258928571 loss 1.8852710723876953
epoch 86 iter 28 sum correct 0.9079337284482759 loss 1.8852323293685913
epoch 86 iter 29 sum correct 0.9077473958333333 loss 1.8852479457855225
epoch 86 iter 30 sum correct 0.9078881048387096 loss 1.8852393627166748
epoch 86 iter 31 sum correct 0.90771484375 loss 1.885252594947815
epoch 86 iter 32 sum correct 0.9081439393939394 loss 1.8852139711380005
epoch 86 iter 33 sum correct 0.9079159007352942 loss 1.8852348327636719
epoch 86 iter 34 sum correct 0.9080357142857143 loss 1.8852229118347168
epoch 86 iter 35 sum correct 0.9075520833333334 loss 1.8852568864822388
epoch 86 iter 36 sum correct 0.907886402027027 loss 1.8852355480194092
epoch 86 iter 37 sum correct 0.9080489309210527 loss 1.8852274417877197
epoch 86 iter 38 sum correct 0.9085036057692307 loss 1.8851903676986694
epoch 86 iter 39 sum correct 0.9083984375 loss 1.8852049112319946
epoch 86 iter 40 sum correct 0.9082507621951219 loss 1.885210633277893
epoch 86 iter 41 sum correct 0.9076915922619048 loss 1.8852555751800537
epoch 86 iter 42 sum correct 0.9073401162790697 loss 1.8852874040603638
epoch 86 iter 43 sum correct 0.9073153409090909 loss 1.8852903842926025
epoch 86 iter 44 sum correct 0.9069878472222223 loss 1.8853156566619873
epoch 86 iter 45 sum correct 0.9070142663043478 loss 1.8853076696395874
epoch 86 iter 46 sum correct 0.9065408909574468 loss 1.8853423595428467
epoch 86 iter 47 sum correct 0.9065755208333334 loss 1.8853405714035034
epoch 86 iter 48 sum correct 0.9064891581632653 loss 1.8853555917739868
epoch 86 iter 49 sum correct 0.9065625 loss 1.885353446006775
epoch 86 iter 50 sum correct 0.9067095588235294 loss 1.885341763496399
epoch 86 iter 51 sum correct 0.9066631610576923 loss 1.8853445053100586
epoch 86 iter 52 sum correct 0.9063605542452831 loss 1.8853665590286255
epoch 86 iter 53 sum correct 0.9065393518518519 loss 1.8853508234024048
epoch 86 iter 54 sum correct 0.9067826704545454 loss 1.8853330612182617
epoch 86 iter 55 sum correct 0.906494140625 loss 1.8853551149368286
epoch 86 iter 56 sum correct 0.8916186951754386 loss 1.8855009078979492
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.51it/s] 43%|████▎     | 3/7.009765625 [00:00<00:01,  4.00it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.53it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.70it/s]8it [00:01,  5.63it/s]                                 macro  0.5274386897133276
micro  0.6285873502368348
[[275   0  52  32  46   9  53]
 [ 30   0   7   8   5   0   6]
 [ 63   0 243  28  62  35  65]
 [ 19   0  25 804   9  11  27]
 [ 84   0 103  69 265   5 127]
 [ 16   0  40  26   5 320   8]
 [ 43   0  43  99  69   4 349]]
              precision    recall  f1-score   support

           0       0.52      0.59      0.55       467
           1       0.00      0.00      0.00        56
           2       0.47      0.49      0.48       496
           3       0.75      0.90      0.82       895
           4       0.57      0.41      0.48       653
           5       0.83      0.77      0.80       415
           6       0.55      0.57      0.56       607

    accuracy                           0.63      3589
   macro avg       0.53      0.53      0.53      3589
weighted avg       0.61      0.63      0.62      3589

correct 0.6285873502368348 
f1 0.5274386897133276 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.70it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.38it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.78it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.92it/s]8it [00:01,  5.91it/s]                                 macro  0.531770392624079
micro  0.636110337141265
[[303   0  59  33  52   2  42]
 [ 32   0  10   9   2   1   1]
 [ 79   0 254  43  68  37  47]
 [ 16   0  20 803  14  14  12]
 [ 72   0  87  69 236   8 122]
 [ 17   0  47  30   9 304   9]
 [ 45   0  53  72  63  10 383]]
              precision    recall  f1-score   support

           0       0.54      0.62      0.57       491
           1       0.00      0.00      0.00        55
           2       0.48      0.48      0.48       528
           3       0.76      0.91      0.83       879
           4       0.53      0.40      0.45       594
           5       0.81      0.73      0.77       416
           6       0.62      0.61      0.62       626

    accuracy                           0.64      3589
   macro avg       0.53      0.54      0.53      3589
weighted avg       0.62      0.64      0.62      3589

correct 0.636110337141265 
f1 0.531770392624079 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:39,  1.41it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.32it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.84it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.29it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.49it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.73it/s] 12%|█▏        | 7/56.072265625 [00:02<00:12,  3.79it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.95it/s] 16%|█▌        | 9/56.072265625 [00:02<00:11,  3.95it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.03it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.99it/s] 21%|██▏       | 12/56.072265625 [00:03<00:11,  3.73it/s] 23%|██▎       | 13/56.072265625 [00:03<00:11,  3.79it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  3.94it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  3.82it/s] 29%|██▊       | 16/56.072265625 [00:04<00:10,  3.88it/s] 30%|███       | 17/56.072265625 [00:04<00:10,  3.85it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  3.96it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  3.89it/s] 36%|███▌      | 20/56.072265625 [00:05<00:09,  3.95it/s] 37%|███▋      | 21/56.072265625 [00:05<00:09,  3.85it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  3.95it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  3.87it/s] 43%|████▎     | 24/56.072265625 [00:06<00:08,  4.00it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  3.98it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.08it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.04it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.12it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.07it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.14it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.03it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.11it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.02it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.08it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.01it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.07it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  3.99it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.00it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  3.87it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:04,  3.92it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  3.85it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  3.93it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  3.89it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:03,  3.93it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  3.86it/s] 82%|████████▏ | 46/56.072265625 [00:12<00:02,  3.93it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  3.87it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:02,  3.94it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  3.82it/s] 89%|████████▉ | 50/56.072265625 [00:13<00:01,  3.95it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  3.96it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:01,  4.00it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  3.99it/s] 96%|█████████▋| 54/56.072265625 [00:14<00:00,  4.08it/s] 98%|█████████▊| 55/56.072265625 [00:14<00:00,  4.05it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.12it/s]57it [00:14,  4.98it/s]                                  57it [00:14,  3.88it/s]epoch 87 iter 0 sum correct 0.91015625 loss 1.8851722478866577
epoch 87 iter 1 sum correct 0.912109375 loss 1.8848932981491089
epoch 87 iter 2 sum correct 0.91015625 loss 1.8850698471069336
epoch 87 iter 3 sum correct 0.90478515625 loss 1.885491967201233
epoch 87 iter 4 sum correct 0.898046875 loss 1.8860338926315308
epoch 87 iter 5 sum correct 0.8948567708333334 loss 1.8862838745117188
epoch 87 iter 6 sum correct 0.8917410714285714 loss 1.8865430355072021
epoch 87 iter 7 sum correct 0.89111328125 loss 1.8866064548492432
epoch 87 iter 8 sum correct 0.8884548611111112 loss 1.8868353366851807
epoch 87 iter 9 sum correct 0.885546875 loss 1.8870652914047241
epoch 87 iter 10 sum correct 0.8824573863636364 loss 1.8873144388198853
epoch 87 iter 11 sum correct 0.8821614583333334 loss 1.8873226642608643
epoch 87 iter 12 sum correct 0.8841646634615384 loss 1.887169599533081
epoch 87 iter 13 sum correct 0.8822544642857143 loss 1.887312889099121
epoch 87 iter 14 sum correct 0.880859375 loss 1.8874332904815674
epoch 87 iter 15 sum correct 0.8807373046875 loss 1.8874387741088867
epoch 87 iter 16 sum correct 0.8805147058823529 loss 1.8874554634094238
epoch 87 iter 17 sum correct 0.8799913194444444 loss 1.8875080347061157
epoch 87 iter 18 sum correct 0.8798314144736842 loss 1.887523889541626
epoch 87 iter 19 sum correct 0.87958984375 loss 1.8875385522842407
epoch 87 iter 20 sum correct 0.8796502976190477 loss 1.8875430822372437
epoch 87 iter 21 sum correct 0.8794389204545454 loss 1.887558937072754
epoch 87 iter 22 sum correct 0.8778872282608695 loss 1.8876855373382568
epoch 87 iter 23 sum correct 0.8778483072916666 loss 1.8876956701278687
epoch 87 iter 24 sum correct 0.878828125 loss 1.8876192569732666
epoch 87 iter 25 sum correct 0.8791316105769231 loss 1.8876097202301025
epoch 87 iter 26 sum correct 0.8791956018518519 loss 1.8876092433929443
epoch 87 iter 27 sum correct 0.8800920758928571 loss 1.8875391483306885
epoch 87 iter 28 sum correct 0.8801185344827587 loss 1.8875325918197632
epoch 87 iter 29 sum correct 0.88046875 loss 1.887497067451477
epoch 87 iter 30 sum correct 0.8812373991935484 loss 1.887436032295227
epoch 87 iter 31 sum correct 0.88165283203125 loss 1.8873988389968872
epoch 87 iter 32 sum correct 0.8817471590909091 loss 1.8873871564865112
epoch 87 iter 33 sum correct 0.8815487132352942 loss 1.8874015808105469
epoch 87 iter 34 sum correct 0.8816964285714286 loss 1.8873919248580933
epoch 87 iter 35 sum correct 0.8823784722222222 loss 1.8873441219329834
epoch 87 iter 36 sum correct 0.8829708614864865 loss 1.8872928619384766
epoch 87 iter 37 sum correct 0.8837890625 loss 1.8872283697128296
epoch 87 iter 38 sum correct 0.8835637019230769 loss 1.887246012687683
epoch 87 iter 39 sum correct 0.883837890625 loss 1.8872148990631104
epoch 87 iter 40 sum correct 0.883812881097561 loss 1.8872175216674805
epoch 87 iter 41 sum correct 0.8839750744047619 loss 1.8872087001800537
epoch 87 iter 42 sum correct 0.8840388808139535 loss 1.8872026205062866
epoch 87 iter 43 sum correct 0.8842329545454546 loss 1.8871841430664062
epoch 87 iter 44 sum correct 0.8845920138888889 loss 1.8871533870697021
epoch 87 iter 45 sum correct 0.8851902173913043 loss 1.8871098756790161
epoch 87 iter 46 sum correct 0.8853889627659575 loss 1.8870899677276611
epoch 87 iter 47 sum correct 0.8853759765625 loss 1.8870861530303955
epoch 87 iter 48 sum correct 0.8857222576530612 loss 1.8870598077774048
epoch 87 iter 49 sum correct 0.8860546875 loss 1.8870307207107544
epoch 87 iter 50 sum correct 0.8857613357843137 loss 1.8870537281036377
epoch 87 iter 51 sum correct 0.8857421875 loss 1.8870543241500854
epoch 87 iter 52 sum correct 0.8861291273584906 loss 1.887021780014038
epoch 87 iter 53 sum correct 0.8864655671296297 loss 1.88699209690094
epoch 87 iter 54 sum correct 0.8867897727272728 loss 1.8869643211364746
epoch 87 iter 55 sum correct 0.8873465401785714 loss 1.8869171142578125
epoch 87 iter 56 sum correct 0.8729098135964912 loss 1.8869109153747559
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.57it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.20it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.42it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.62it/s]8it [00:01,  5.62it/s]                                 macro  0.5188366677685532
micro  0.6118696015603232
[[251   0  82  16  36  16  66]
 [ 27   0  13   4   3   0   9]
 [ 43   0 249  12  72  39  81]
 [ 37   0  28 699  14  23  94]
 [ 64   0 134  19 254  11 171]
 [ 10   0  31  16   7 329  22]
 [ 32   0  57  33  66   5 414]]
              precision    recall  f1-score   support

           0       0.54      0.54      0.54       467
           1       0.00      0.00      0.00        56
           2       0.42      0.50      0.46       496
           3       0.87      0.78      0.83       895
           4       0.56      0.39      0.46       653
           5       0.78      0.79      0.79       415
           6       0.48      0.68      0.57       607

    accuracy                           0.61      3589
   macro avg       0.52      0.53      0.52      3589
weighted avg       0.62      0.61      0.61      3589

correct 0.6118696015603232 
f1 0.5188366677685532 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.70it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.26it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.69it/s] 86%|████████▌ | 6/7.009765625 [00:01<00:00,  6.43it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.76it/s]8it [00:01,  5.71it/s]                                 macro  0.5337129624245407
micro  0.6327667874059627
[[284   0  80  12  41   9  65]
 [ 30   0  12   4   2   1   6]
 [ 63   0 275  17  67  49  57]
 [ 25   0  30 714  27  21  62]
 [ 67   0 123  23 231   5 145]
 [ 10   0  41  22   4 316  23]
 [ 31   0  62  18  53  11 451]]
              precision    recall  f1-score   support

           0       0.56      0.58      0.57       491
           1       0.00      0.00      0.00        55
           2       0.44      0.52      0.48       528
           3       0.88      0.81      0.85       879
           4       0.54      0.39      0.45       594
           5       0.77      0.76      0.76       416
           6       0.56      0.72      0.63       626

    accuracy                           0.63      3589
   macro avg       0.54      0.54      0.53      3589
weighted avg       0.63      0.63      0.63      3589

correct 0.6327667874059627 
f1 0.5337129624245407 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:38,  1.43it/s]  4%|▎         | 2/56.072265625 [00:00<00:22,  2.35it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.86it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.30it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.50it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.72it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.68it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.86it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.85it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  3.97it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.93it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.04it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  4.01it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.10it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.03it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.11it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.05it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.13it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.06it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.13it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.07it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.11it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.04it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.12it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  3.95it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  3.99it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  3.95it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.05it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.02it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.10it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.05it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.13it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  3.95it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  3.99it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  3.92it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:05,  3.98it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  3.89it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  3.95it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  3.89it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:04,  4.00it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  3.96it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.05it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  4.00it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.09it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.02it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.10it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  4.03it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.11it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.05it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.13it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  4.07it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.14it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.07it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.15it/s] 98%|█████████▊| 55/56.072265625 [00:14<00:00,  4.09it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.15it/s]57it [00:14,  4.99it/s]                                  57it [00:14,  3.94it/s]epoch 88 iter 0 sum correct 0.916015625 loss 1.8846267461776733
epoch 88 iter 1 sum correct 0.9140625 loss 1.884781837463379
epoch 88 iter 2 sum correct 0.9069010416666666 loss 1.8852720260620117
epoch 88 iter 3 sum correct 0.90283203125 loss 1.8855751752853394
epoch 88 iter 4 sum correct 0.90234375 loss 1.8856182098388672
epoch 88 iter 5 sum correct 0.8990885416666666 loss 1.8858541250228882
epoch 88 iter 6 sum correct 0.8981584821428571 loss 1.885960578918457
epoch 88 iter 7 sum correct 0.897216796875 loss 1.8860440254211426
epoch 88 iter 8 sum correct 0.8993055555555556 loss 1.885906457901001
epoch 88 iter 9 sum correct 0.8994140625 loss 1.8859024047851562
epoch 88 iter 10 sum correct 0.8998579545454546 loss 1.885855793952942
epoch 88 iter 11 sum correct 0.8981119791666666 loss 1.8859846591949463
epoch 88 iter 12 sum correct 0.8982872596153846 loss 1.8859652280807495
epoch 88 iter 13 sum correct 0.8988560267857143 loss 1.8859138488769531
epoch 88 iter 14 sum correct 0.898828125 loss 1.885907769203186
epoch 88 iter 15 sum correct 0.8990478515625 loss 1.8858959674835205
epoch 88 iter 16 sum correct 0.8995863970588235 loss 1.885852336883545
epoch 88 iter 17 sum correct 0.8996310763888888 loss 1.8858598470687866
epoch 88 iter 18 sum correct 0.9004934210526315 loss 1.8857899904251099
epoch 88 iter 19 sum correct 0.898828125 loss 1.8859304189682007
epoch 88 iter 20 sum correct 0.8980654761904762 loss 1.8859801292419434
epoch 88 iter 21 sum correct 0.8987926136363636 loss 1.8859236240386963
epoch 88 iter 22 sum correct 0.8993716032608695 loss 1.8858805894851685
epoch 88 iter 23 sum correct 0.8992513020833334 loss 1.8859024047851562
epoch 88 iter 24 sum correct 0.89875 loss 1.885947585105896
epoch 88 iter 25 sum correct 0.8987379807692307 loss 1.8859477043151855
epoch 88 iter 26 sum correct 0.8981481481481481 loss 1.8859987258911133
epoch 88 iter 27 sum correct 0.8976004464285714 loss 1.8860394954681396
epoch 88 iter 28 sum correct 0.8976966594827587 loss 1.8860340118408203
epoch 88 iter 29 sum correct 0.8981770833333333 loss 1.8859957456588745
epoch 88 iter 30 sum correct 0.8980594758064516 loss 1.886003017425537
epoch 88 iter 31 sum correct 0.89776611328125 loss 1.8860255479812622
epoch 88 iter 32 sum correct 0.8977864583333334 loss 1.886027455329895
epoch 88 iter 33 sum correct 0.8974609375 loss 1.8860580921173096
epoch 88 iter 34 sum correct 0.8979352678571428 loss 1.8860236406326294
epoch 88 iter 35 sum correct 0.8976779513888888 loss 1.88603675365448
epoch 88 iter 36 sum correct 0.8976456925675675 loss 1.886040210723877
epoch 88 iter 37 sum correct 0.8976665296052632 loss 1.8860379457473755
epoch 88 iter 38 sum correct 0.8980368589743589 loss 1.8860112428665161
epoch 88 iter 39 sum correct 0.8978515625 loss 1.8860273361206055
epoch 88 iter 40 sum correct 0.897532393292683 loss 1.886051058769226
epoch 88 iter 41 sum correct 0.8975539434523809 loss 1.8860483169555664
epoch 88 iter 42 sum correct 0.8976199127906976 loss 1.8860423564910889
epoch 88 iter 43 sum correct 0.8973277698863636 loss 1.8860628604888916
epoch 88 iter 44 sum correct 0.8969618055555556 loss 1.8860933780670166
epoch 88 iter 45 sum correct 0.8973760190217391 loss 1.8860613107681274
epoch 88 iter 46 sum correct 0.8978141622340425 loss 1.8860313892364502
epoch 88 iter 47 sum correct 0.8982747395833334 loss 1.8859952688217163
epoch 88 iter 48 sum correct 0.8977598852040817 loss 1.8860334157943726
epoch 88 iter 49 sum correct 0.897578125 loss 1.8860485553741455
epoch 88 iter 50 sum correct 0.8970205269607843 loss 1.8860958814620972
epoch 88 iter 51 sum correct 0.8970853365384616 loss 1.886088490486145
epoch 88 iter 52 sum correct 0.8967791863207547 loss 1.8861156702041626
epoch 88 iter 53 sum correct 0.8967013888888888 loss 1.8861218690872192
epoch 88 iter 54 sum correct 0.8970525568181819 loss 1.8860958814620972
epoch 88 iter 55 sum correct 0.8970772879464286 loss 1.8860915899276733
epoch 88 iter 56 sum correct 0.8824013157894737 loss 1.8861759901046753
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.66it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.35it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.88it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.13it/s]8it [00:01,  5.99it/s]                                 macro  0.517440135800392
micro  0.60490387294511
[[316   0  42  17  36   8  48]
 [ 40   0   4   3   3   2   4]
 [108   0 222  11  78  28  49]
 [ 76   0  21 668  29  21  80]
 [149   0  70  20 287   9 118]
 [ 27   0  30  13  10 318  17]
 [ 99   0  36  30  79   3 360]]
              precision    recall  f1-score   support

           0       0.39      0.68      0.49       467
           1       0.00      0.00      0.00        56
           2       0.52      0.45      0.48       496
           3       0.88      0.75      0.81       895
           4       0.55      0.44      0.49       653
           5       0.82      0.77      0.79       415
           6       0.53      0.59      0.56       607

    accuracy                           0.60      3589
   macro avg       0.53      0.52      0.52      3589
weighted avg       0.63      0.60      0.61      3589

correct 0.60490387294511 
f1 0.517440135800392 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.73it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.30it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.77it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.01it/s]8it [00:01,  5.94it/s]                                 macro  0.5301829620451224
micro  0.6260796879353581
[[359   0  39   9  38   5  41]
 [ 42   0   7   1   1   2   2]
 [108   0 244  16  83  36  41]
 [ 64   0  24 701  27  17  46]
 [135   0  65  22 254   8 110]
 [ 26   0  41  18  13 292  26]
 [100   0  37  19  62  11 397]]
              precision    recall  f1-score   support

           0       0.43      0.73      0.54       491
           1       0.00      0.00      0.00        55
           2       0.53      0.46      0.50       528
           3       0.89      0.80      0.84       879
           4       0.53      0.43      0.47       594
           5       0.79      0.70      0.74       416
           6       0.60      0.63      0.62       626

    accuracy                           0.63      3589
   macro avg       0.54      0.54      0.53      3589
weighted avg       0.64      0.63      0.63      3589

correct 0.6260796879353581 
f1 0.5301829620451224 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:37,  1.48it/s]  4%|▎         | 2/56.072265625 [00:00<00:22,  2.40it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.90it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.31it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.49it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.71it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.77it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.92it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.91it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.02it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.96it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.06it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  3.99it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.09it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.03it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.09it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.02it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.10it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.04it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.11it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.04it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.10it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.02it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.12it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.02it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  3.84it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  3.86it/s] 50%|████▉     | 28/56.072265625 [00:07<00:07,  3.99it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  3.96it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.04it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.01it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.10it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.03it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.09it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.02it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.10it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.03it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.09it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  4.02it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.10it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.06it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.08it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  4.02it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.10it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.03it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.09it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  4.04it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.12it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.04it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.10it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  4.05it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.13it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.10it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.15it/s] 98%|█████████▊| 55/56.072265625 [00:13<00:00,  4.08it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.15it/s]57it [00:14,  5.02it/s]                                  57it [00:14,  3.96it/s]epoch 89 iter 0 sum correct 0.916015625 loss 1.884673833847046
epoch 89 iter 1 sum correct 0.9140625 loss 1.8847861289978027
epoch 89 iter 2 sum correct 0.9114583333333334 loss 1.8850640058517456
epoch 89 iter 3 sum correct 0.90966796875 loss 1.885229468345642
epoch 89 iter 4 sum correct 0.90703125 loss 1.8854461908340454
epoch 89 iter 5 sum correct 0.9046223958333334 loss 1.8856191635131836
epoch 89 iter 6 sum correct 0.9015066964285714 loss 1.8858357667922974
epoch 89 iter 7 sum correct 0.900146484375 loss 1.8859210014343262
epoch 89 iter 8 sum correct 0.8975694444444444 loss 1.8861088752746582
epoch 89 iter 9 sum correct 0.89765625 loss 1.8860893249511719
epoch 89 iter 10 sum correct 0.8984375 loss 1.88601815700531
epoch 89 iter 11 sum correct 0.8994140625 loss 1.8859379291534424
epoch 89 iter 12 sum correct 0.8991887019230769 loss 1.8859549760818481
epoch 89 iter 13 sum correct 0.8980189732142857 loss 1.8860372304916382
epoch 89 iter 14 sum correct 0.8985677083333333 loss 1.885988712310791
epoch 89 iter 15 sum correct 0.8980712890625 loss 1.8860211372375488
epoch 89 iter 16 sum correct 0.8980928308823529 loss 1.8860198259353638
epoch 89 iter 17 sum correct 0.8978949652777778 loss 1.8860238790512085
epoch 89 iter 18 sum correct 0.8978207236842105 loss 1.8860355615615845
epoch 89 iter 19 sum correct 0.8978515625 loss 1.8860317468643188
epoch 89 iter 20 sum correct 0.8977864583333334 loss 1.8860292434692383
epoch 89 iter 21 sum correct 0.8971946022727273 loss 1.8860925436019897
epoch 89 iter 22 sum correct 0.8973335597826086 loss 1.8860878944396973
epoch 89 iter 23 sum correct 0.8965657552083334 loss 1.8861441612243652
epoch 89 iter 24 sum correct 0.896875 loss 1.886125087738037
epoch 89 iter 25 sum correct 0.8975360576923077 loss 1.8860719203948975
epoch 89 iter 26 sum correct 0.8975694444444444 loss 1.8860726356506348
epoch 89 iter 27 sum correct 0.8971121651785714 loss 1.8861095905303955
epoch 89 iter 28 sum correct 0.8974272629310345 loss 1.8860890865325928
epoch 89 iter 29 sum correct 0.8979166666666667 loss 1.8860499858856201
epoch 89 iter 30 sum correct 0.8975554435483871 loss 1.886077880859375
epoch 89 iter 31 sum correct 0.89776611328125 loss 1.886065125465393
epoch 89 iter 32 sum correct 0.8978456439393939 loss 1.8860589265823364
epoch 89 iter 33 sum correct 0.8982651654411765 loss 1.8860307931900024
epoch 89 iter 34 sum correct 0.8982142857142857 loss 1.8860290050506592
epoch 89 iter 35 sum correct 0.8980577256944444 loss 1.8860416412353516
epoch 89 iter 36 sum correct 0.8979624155405406 loss 1.8860441446304321
epoch 89 iter 37 sum correct 0.8982833059210527 loss 1.886017918586731
epoch 89 iter 38 sum correct 0.8988882211538461 loss 1.8859665393829346
epoch 89 iter 39 sum correct 0.8986328125 loss 1.8859840631484985
epoch 89 iter 40 sum correct 0.8986756859756098 loss 1.8859790563583374
epoch 89 iter 41 sum correct 0.8989490327380952 loss 1.88595712184906
epoch 89 iter 42 sum correct 0.8987554505813954 loss 1.8859670162200928
epoch 89 iter 43 sum correct 0.8987482244318182 loss 1.8859648704528809
epoch 89 iter 44 sum correct 0.8986111111111111 loss 1.8859760761260986
epoch 89 iter 45 sum correct 0.8983525815217391 loss 1.8859939575195312
epoch 89 iter 46 sum correct 0.8984790558510638 loss 1.8859859704971313
epoch 89 iter 47 sum correct 0.8983154296875 loss 1.886000156402588
epoch 89 iter 48 sum correct 0.8985570790816326 loss 1.8859814405441284
epoch 89 iter 49 sum correct 0.8983984375 loss 1.8859928846359253
epoch 89 iter 50 sum correct 0.8986289828431373 loss 1.8859729766845703
epoch 89 iter 51 sum correct 0.8987004206730769 loss 1.8859715461730957
epoch 89 iter 52 sum correct 0.8982532429245284 loss 1.8860055208206177
epoch 89 iter 53 sum correct 0.8984013310185185 loss 1.8859951496124268
epoch 89 iter 54 sum correct 0.8984019886363637 loss 1.8859955072402954
epoch 89 iter 55 sum correct 0.8987165178571429 loss 1.8859751224517822
epoch 89 iter 56 sum correct 0.8841145833333334 loss 1.885956883430481
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.63it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.19it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.61it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.86it/s]8it [00:01,  5.80it/s]                                 macro  0.510834316750934
micro  0.6129841181387573
[[196   0  52  37  99  27  56]
 [ 13   0   9   7  22   1   4]
 [ 39   0 194  29 132  48  54]
 [  9   0  16 776  35  19  40]
 [ 43   0  87  56 353   9 105]
 [  3   0  27  22  14 336  13]
 [ 16   0  31  78 131   6 345]]
              precision    recall  f1-score   support

           0       0.61      0.42      0.50       467
           1       0.00      0.00      0.00        56
           2       0.47      0.39      0.43       496
           3       0.77      0.87      0.82       895
           4       0.45      0.54      0.49       653
           5       0.75      0.81      0.78       415
           6       0.56      0.57      0.56       607

    accuracy                           0.61      3589
   macro avg       0.52      0.51      0.51      3589
weighted avg       0.60      0.61      0.60      3589

correct 0.6129841181387573 
f1 0.510834316750934 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.73it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.38it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.72it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.86it/s]8it [00:01,  5.88it/s]                                 macro  0.526463331897386
micro  0.6330454165505712
[[211   0  53  36 111  21  59]
 [ 16   0  11   8  16   2   2]
 [ 39   0 228  37 108  61  55]
 [ 10   0  12 794  26  18  19]
 [ 36   0  73  53 329   7  96]
 [  3   0  34  27  13 328  11]
 [ 20   0  32  52 121  19 382]]
              precision    recall  f1-score   support

           0       0.63      0.43      0.51       491
           1       0.00      0.00      0.00        55
           2       0.51      0.43      0.47       528
           3       0.79      0.90      0.84       879
           4       0.45      0.55      0.50       594
           5       0.72      0.79      0.75       416
           6       0.61      0.61      0.61       626

    accuracy                           0.63      3589
   macro avg       0.53      0.53      0.53      3589
weighted avg       0.62      0.63      0.62      3589

correct 0.6330454165505712 
f1 0.526463331897386 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:38,  1.43it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.33it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.85it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.30it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.49it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.72it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.77it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.92it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.89it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  3.99it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.96it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.06it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  4.01it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.09it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.04it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.12it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.06it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.13it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.07it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.10it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.03it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.12it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.06it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.13it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.07it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.14it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.08it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.15it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.07it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.14it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.07it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.14it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.08it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.15it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.08it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.15it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.08it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.15it/s] 70%|██████▉   | 39/56.072265625 [00:09<00:04,  4.08it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.15it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.08it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.15it/s] 77%|███████▋  | 43/56.072265625 [00:10<00:03,  4.08it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.15it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.08it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.15it/s] 84%|████████▍ | 47/56.072265625 [00:11<00:02,  4.08it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.15it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.08it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.15it/s] 91%|█████████ | 51/56.072265625 [00:12<00:01,  4.09it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.15it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.09it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.16it/s] 98%|█████████▊| 55/56.072265625 [00:13<00:00,  4.10it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.16it/s]57it [00:14,  5.03it/s]                                  57it [00:14,  3.99it/s]epoch 90 iter 0 sum correct 0.919921875 loss 1.884229302406311
epoch 90 iter 1 sum correct 0.908203125 loss 1.8851794004440308
epoch 90 iter 2 sum correct 0.8880208333333334 loss 1.8868387937545776
epoch 90 iter 3 sum correct 0.88623046875 loss 1.8869338035583496
epoch 90 iter 4 sum correct 0.884765625 loss 1.8870452642440796
epoch 90 iter 5 sum correct 0.8834635416666666 loss 1.8871705532073975
epoch 90 iter 6 sum correct 0.8825334821428571 loss 1.8872520923614502
epoch 90 iter 7 sum correct 0.880126953125 loss 1.8874365091323853
epoch 90 iter 8 sum correct 0.8799913194444444 loss 1.8874642848968506
epoch 90 iter 9 sum correct 0.8794921875 loss 1.8875068426132202
epoch 90 iter 10 sum correct 0.8774857954545454 loss 1.8876750469207764
epoch 90 iter 11 sum correct 0.8766276041666666 loss 1.8877609968185425
epoch 90 iter 12 sum correct 0.8768028846153846 loss 1.887760043144226
epoch 90 iter 13 sum correct 0.8776506696428571 loss 1.8877040147781372
epoch 90 iter 14 sum correct 0.8778645833333333 loss 1.887696623802185
epoch 90 iter 15 sum correct 0.8763427734375 loss 1.8878048658370972
epoch 90 iter 16 sum correct 0.8775275735294118 loss 1.8877133131027222
epoch 90 iter 17 sum correct 0.8772786458333334 loss 1.8877216577529907
epoch 90 iter 18 sum correct 0.8767475328947368 loss 1.8877581357955933
epoch 90 iter 19 sum correct 0.8783203125 loss 1.8876276016235352
epoch 90 iter 20 sum correct 0.8790922619047619 loss 1.8875675201416016
epoch 90 iter 21 sum correct 0.8781072443181818 loss 1.8876489400863647
epoch 90 iter 22 sum correct 0.8788213315217391 loss 1.887594223022461
epoch 90 iter 23 sum correct 0.878662109375 loss 1.8876179456710815
epoch 90 iter 24 sum correct 0.878828125 loss 1.887594223022461
epoch 90 iter 25 sum correct 0.8786808894230769 loss 1.8875977993011475
epoch 90 iter 26 sum correct 0.87890625 loss 1.887587070465088
epoch 90 iter 27 sum correct 0.8792550223214286 loss 1.887548804283142
epoch 90 iter 28 sum correct 0.8793103448275862 loss 1.8875291347503662
epoch 90 iter 29 sum correct 0.8796223958333333 loss 1.8875031471252441
epoch 90 iter 30 sum correct 0.8806073588709677 loss 1.8874320983886719
epoch 90 iter 31 sum correct 0.88079833984375 loss 1.8874150514602661
epoch 90 iter 32 sum correct 0.8813328598484849 loss 1.8873727321624756
epoch 90 iter 33 sum correct 0.8805147058823529 loss 1.8874351978302002
epoch 90 iter 34 sum correct 0.88046875 loss 1.8874398469924927
epoch 90 iter 35 sum correct 0.8802083333333334 loss 1.8874520063400269
epoch 90 iter 36 sum correct 0.8802259290540541 loss 1.887442946434021
epoch 90 iter 37 sum correct 0.880602384868421 loss 1.8874127864837646
epoch 90 iter 38 sum correct 0.8810096153846154 loss 1.8873823881149292
epoch 90 iter 39 sum correct 0.881005859375 loss 1.8873780965805054
epoch 90 iter 40 sum correct 0.8806211890243902 loss 1.8874084949493408
epoch 90 iter 41 sum correct 0.880859375 loss 1.8873873949050903
epoch 90 iter 42 sum correct 0.8812681686046512 loss 1.8873555660247803
epoch 90 iter 43 sum correct 0.8814808238636364 loss 1.887337327003479
epoch 90 iter 44 sum correct 0.8814670138888889 loss 1.887339472770691
epoch 90 iter 45 sum correct 0.8817085597826086 loss 1.8873180150985718
epoch 90 iter 46 sum correct 0.8818567154255319 loss 1.887304663658142
epoch 90 iter 47 sum correct 0.8815511067708334 loss 1.8873295783996582
epoch 90 iter 48 sum correct 0.8817362882653061 loss 1.8873133659362793
epoch 90 iter 49 sum correct 0.88171875 loss 1.8873121738433838
epoch 90 iter 50 sum correct 0.8818167892156863 loss 1.887300729751587
epoch 90 iter 51 sum correct 0.8817608173076923 loss 1.887307047843933
epoch 90 iter 52 sum correct 0.8818175117924528 loss 1.8873025178909302
epoch 90 iter 53 sum correct 0.8819082754629629 loss 1.8872928619384766
epoch 90 iter 54 sum correct 0.8824573863636364 loss 1.88724946975708
epoch 90 iter 55 sum correct 0.8829520089285714 loss 1.887208104133606
epoch 90 iter 56 sum correct 0.8685581140350878 loss 1.8872288465499878
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.61it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.18it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.66it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.81it/s]8it [00:01,  5.78it/s]                                 macro  0.5315496974649155
micro  0.6299804959598774
[[234   0  39  25  89  14  66]
 [ 26   0   9   2  10   0   9]
 [ 43   0 206  15 116  39  77]
 [ 27   0   7 719  41  27  74]
 [ 47   0  58  21 362  12 153]
 [ 10   0  29  14  12 327  23]
 [ 26   0  18  40 102   8 413]]
              precision    recall  f1-score   support

           0       0.57      0.50      0.53       467
           1       0.00      0.00      0.00        56
           2       0.56      0.42      0.48       496
           3       0.86      0.80      0.83       895
           4       0.49      0.55      0.52       653
           5       0.77      0.79      0.78       415
           6       0.51      0.68      0.58       607

    accuracy                           0.63      3589
   macro avg       0.54      0.53      0.53      3589
weighted avg       0.63      0.63      0.63      3589

correct 0.6299804959598774 
f1 0.5315496974649155 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.69it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.38it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.64it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.89it/s]8it [00:01,  5.86it/s]                                 macro  0.5353050955549462
micro  0.6349958205628309
[[249   0  43  22  92   6  79]
 [ 26   0   9   2   9   2   7]
 [ 55   0 224  17 117  42  73]
 [ 12   0   9 738  38  24  58]
 [ 39   0  46  33 315   7 154]
 [  9   0  31  14  13 321  28]
 [ 23   0  32  29  96  14 432]]
              precision    recall  f1-score   support

           0       0.60      0.51      0.55       491
           1       0.00      0.00      0.00        55
           2       0.57      0.42      0.49       528
           3       0.86      0.84      0.85       879
           4       0.46      0.53      0.49       594
           5       0.77      0.77      0.77       416
           6       0.52      0.69      0.59       626

    accuracy                           0.63      3589
   macro avg       0.54      0.54      0.54      3589
weighted avg       0.63      0.63      0.63      3589

correct 0.6349958205628309 
f1 0.5353050955549462 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:38,  1.42it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.35it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.84it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.28it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.46it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.72it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.76it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.93it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.91it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.03it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  4.01it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.09it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  4.03it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.11it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.02it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.10it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.02it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.10it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.04it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.12it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.05it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.12it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.05it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.12it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.06it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.14it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.08it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.15it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.08it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.15it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.09it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.15it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.09it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.15it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.09it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.14it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.03it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.10it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  4.01it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.10it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.03it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.12it/s] 77%|███████▋  | 43/56.072265625 [00:10<00:03,  4.05it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.12it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.07it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.14it/s] 84%|████████▍ | 47/56.072265625 [00:11<00:02,  4.08it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.15it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.09it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.16it/s] 91%|█████████ | 51/56.072265625 [00:12<00:01,  4.11it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:01,  3.98it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  3.97it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.07it/s] 98%|█████████▊| 55/56.072265625 [00:13<00:00,  4.05it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.13it/s]57it [00:14,  3.98it/s]                                  epoch 91 iter 0 sum correct 0.908203125 loss 1.885143756866455
epoch 91 iter 1 sum correct 0.89453125 loss 1.886172890663147
epoch 91 iter 2 sum correct 0.8997395833333334 loss 1.885786771774292
epoch 91 iter 3 sum correct 0.90283203125 loss 1.8855302333831787
epoch 91 iter 4 sum correct 0.903515625 loss 1.885522484779358
epoch 91 iter 5 sum correct 0.8990885416666666 loss 1.8858869075775146
epoch 91 iter 6 sum correct 0.8973214285714286 loss 1.8860387802124023
epoch 91 iter 7 sum correct 0.898193359375 loss 1.8859678506851196
epoch 91 iter 8 sum correct 0.9012586805555556 loss 1.885721206665039
epoch 91 iter 9 sum correct 0.898828125 loss 1.8859102725982666
epoch 91 iter 10 sum correct 0.8973721590909091 loss 1.8860381841659546
epoch 91 iter 11 sum correct 0.896484375 loss 1.886109709739685
epoch 91 iter 12 sum correct 0.8960336538461539 loss 1.8861347436904907
epoch 91 iter 13 sum correct 0.8973214285714286 loss 1.8860286474227905
epoch 91 iter 14 sum correct 0.898046875 loss 1.8859786987304688
epoch 91 iter 15 sum correct 0.8995361328125 loss 1.8858630657196045
epoch 91 iter 16 sum correct 0.8986672794117647 loss 1.8859398365020752
epoch 91 iter 17 sum correct 0.8973524305555556 loss 1.8860414028167725
epoch 91 iter 18 sum correct 0.8966899671052632 loss 1.8860948085784912
epoch 91 iter 19 sum correct 0.89677734375 loss 1.886092185974121
epoch 91 iter 20 sum correct 0.8968563988095238 loss 1.886088252067566
epoch 91 iter 21 sum correct 0.8970170454545454 loss 1.8860740661621094
epoch 91 iter 22 sum correct 0.8969089673913043 loss 1.8860825300216675
epoch 91 iter 23 sum correct 0.8974609375 loss 1.8860385417938232
epoch 91 iter 24 sum correct 0.897421875 loss 1.8860424757003784
epoch 91 iter 25 sum correct 0.8983623798076923 loss 1.885966420173645
epoch 91 iter 26 sum correct 0.8982204861111112 loss 1.8859788179397583
epoch 91 iter 27 sum correct 0.8983677455357143 loss 1.8859692811965942
epoch 91 iter 28 sum correct 0.8979660560344828 loss 1.8859949111938477
epoch 91 iter 29 sum correct 0.8979817708333333 loss 1.8859931230545044
epoch 91 iter 30 sum correct 0.8976184475806451 loss 1.8860220909118652
epoch 91 iter 31 sum correct 0.898193359375 loss 1.8859825134277344
epoch 91 iter 32 sum correct 0.8975497159090909 loss 1.886033296585083
epoch 91 iter 33 sum correct 0.8971737132352942 loss 1.8860608339309692
epoch 91 iter 34 sum correct 0.8976004464285714 loss 1.8860299587249756
epoch 91 iter 35 sum correct 0.8971896701388888 loss 1.8860615491867065
epoch 91 iter 36 sum correct 0.8967483108108109 loss 1.8860938549041748
epoch 91 iter 37 sum correct 0.897203947368421 loss 1.8860549926757812
epoch 91 iter 38 sum correct 0.8973858173076923 loss 1.8860372304916382
epoch 91 iter 39 sum correct 0.8978515625 loss 1.8860012292861938
epoch 91 iter 40 sum correct 0.8988185975609756 loss 1.885925054550171
epoch 91 iter 41 sum correct 0.8985770089285714 loss 1.8859390020370483
epoch 91 iter 42 sum correct 0.8990279796511628 loss 1.8859035968780518
epoch 91 iter 43 sum correct 0.8986594460227273 loss 1.8859354257583618
epoch 91 iter 44 sum correct 0.8984375 loss 1.8859508037567139
epoch 91 iter 45 sum correct 0.8989470108695652 loss 1.8859094381332397
epoch 91 iter 46 sum correct 0.8990608377659575 loss 1.8858997821807861
epoch 91 iter 47 sum correct 0.899169921875 loss 1.8858942985534668
epoch 91 iter 48 sum correct 0.8988759566326531 loss 1.8859188556671143
epoch 91 iter 49 sum correct 0.89890625 loss 1.8859179019927979
epoch 91 iter 50 sum correct 0.8991651348039216 loss 1.8858996629714966
epoch 91 iter 51 sum correct 0.8991887019230769 loss 1.8859013319015503
epoch 91 iter 52 sum correct 0.8992113797169812 loss 1.885901689529419
epoch 91 iter 53 sum correct 0.8990523726851852 loss 1.8859150409698486
epoch 91 iter 54 sum correct 0.8990767045454545 loss 1.8859144449234009
epoch 91 iter 55 sum correct 0.8988909040178571 loss 1.8859320878982544
epoch 91 iter 56 sum correct 0.8841831140350878 loss 1.8860152959823608
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.56it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.09it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.54it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.71it/s]8it [00:01,  5.62it/s]                                 macro  0.5341846655560701
micro  0.636110337141265
[[250   0  32  38  89   9  49]
 [ 24   0   4   6  19   1   2]
 [ 66   0 186  24 135  30  55]
 [ 18   0   5 773  45  15  39]
 [ 60   0  40  43 385   8 117]
 [ 16   0  19  21  21 319  19]
 [ 46   0  16  62 111   2 370]]
              precision    recall  f1-score   support

           0       0.52      0.54      0.53       467
           1       0.00      0.00      0.00        56
           2       0.62      0.38      0.47       496
           3       0.80      0.86      0.83       895
           4       0.48      0.59      0.53       653
           5       0.83      0.77      0.80       415
           6       0.57      0.61      0.59       607

    accuracy                           0.64      3589
   macro avg       0.54      0.53      0.53      3589
weighted avg       0.63      0.64      0.63      3589

correct 0.636110337141265 
f1 0.5341846655560701 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.76it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.42it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.92it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.14it/s]8it [00:01,  6.07it/s]                                 macro  0.5371723935682191
micro  0.6388966285873502
[[272   0  40  33  95  11  40]
 [ 24   0   7   6  15   1   2]
 [ 57   0 206  42 146  37  40]
 [ 14   0  14 774  36  13  28]
 [ 54   0  35  49 354   4  98]
 [ 12   0  25  31  16 310  22]
 [ 49   0  22  62 109   7 377]]
              precision    recall  f1-score   support

           0       0.56      0.55      0.56       491
           1       0.00      0.00      0.00        55
           2       0.59      0.39      0.47       528
           3       0.78      0.88      0.83       879
           4       0.46      0.60      0.52       594
           5       0.81      0.75      0.78       416
           6       0.62      0.60      0.61       626

    accuracy                           0.64      3589
   macro avg       0.55      0.54      0.54      3589
weighted avg       0.63      0.64      0.63      3589

correct 0.6388966285873502 
f1 0.5371723935682191 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:38,  1.44it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.34it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.83it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.28it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.47it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.71it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.77it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.93it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.92it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.03it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.99it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.08it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  4.02it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.10it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.03it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.11it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  3.97it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.03it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  3.99it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.09it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.02it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.11it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.05it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.12it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  3.96it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.00it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  3.92it/s] 50%|████▉     | 28/56.072265625 [00:07<00:07,  3.97it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  3.90it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  3.97it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  3.88it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:06,  3.95it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  3.89it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  3.95it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  3.92it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.03it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.00it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.02it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  3.92it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:04,  3.98it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  3.89it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  3.95it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  3.87it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:03,  3.96it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  3.92it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  3.97it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  3.94it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.05it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.02it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.07it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  4.03it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.11it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  3.93it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.04it/s] 98%|█████████▊| 55/56.072265625 [00:14<00:00,  4.01it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.06it/s]57it [00:14,  4.85it/s]                                  57it [00:14,  3.90it/s]epoch 92 iter 0 sum correct 0.91015625 loss 1.8851326704025269
epoch 92 iter 1 sum correct 0.9111328125 loss 1.8850629329681396
epoch 92 iter 2 sum correct 0.91796875 loss 1.8844680786132812
epoch 92 iter 3 sum correct 0.916015625 loss 1.8845889568328857
epoch 92 iter 4 sum correct 0.916015625 loss 1.8845964670181274
epoch 92 iter 5 sum correct 0.9130859375 loss 1.8847965002059937
epoch 92 iter 6 sum correct 0.9123883928571429 loss 1.884861946105957
epoch 92 iter 7 sum correct 0.916015625 loss 1.8845967054367065
epoch 92 iter 8 sum correct 0.9147135416666666 loss 1.884708046913147
epoch 92 iter 9 sum correct 0.912890625 loss 1.8848412036895752
epoch 92 iter 10 sum correct 0.9110440340909091 loss 1.884963035583496
epoch 92 iter 11 sum correct 0.9111328125 loss 1.8849424123764038
epoch 92 iter 12 sum correct 0.9110576923076923 loss 1.8849661350250244
epoch 92 iter 13 sum correct 0.9104352678571429 loss 1.885016679763794
epoch 92 iter 14 sum correct 0.91015625 loss 1.8850388526916504
epoch 92 iter 15 sum correct 0.91064453125 loss 1.8849948644638062
epoch 92 iter 16 sum correct 0.9116498161764706 loss 1.8849090337753296
epoch 92 iter 17 sum correct 0.9113498263888888 loss 1.884926676750183
epoch 92 iter 18 sum correct 0.9102590460526315 loss 1.8850126266479492
epoch 92 iter 19 sum correct 0.91015625 loss 1.885024905204773
epoch 92 iter 20 sum correct 0.9100632440476191 loss 1.885029911994934
epoch 92 iter 21 sum correct 0.9086470170454546 loss 1.8851306438446045
epoch 92 iter 22 sum correct 0.908797554347826 loss 1.885121464729309
epoch 92 iter 23 sum correct 0.9081217447916666 loss 1.8851741552352905
epoch 92 iter 24 sum correct 0.908125 loss 1.8851830959320068
epoch 92 iter 25 sum correct 0.9081280048076923 loss 1.885183572769165
epoch 92 iter 26 sum correct 0.9074074074074074 loss 1.8852461576461792
epoch 92 iter 27 sum correct 0.9078543526785714 loss 1.885208249092102
epoch 92 iter 28 sum correct 0.9074622844827587 loss 1.8852405548095703
epoch 92 iter 29 sum correct 0.9067057291666667 loss 1.885300636291504
epoch 92 iter 30 sum correct 0.9071320564516129 loss 1.8852711915969849
epoch 92 iter 31 sum correct 0.906494140625 loss 1.8853263854980469
epoch 92 iter 32 sum correct 0.9057173295454546 loss 1.885384202003479
epoch 92 iter 33 sum correct 0.9056181066176471 loss 1.885393500328064
epoch 92 iter 34 sum correct 0.90546875 loss 1.885409951210022
epoch 92 iter 35 sum correct 0.9056532118055556 loss 1.8853936195373535
epoch 92 iter 36 sum correct 0.9054054054054054 loss 1.8854146003723145
epoch 92 iter 37 sum correct 0.9053248355263158 loss 1.8854209184646606
epoch 92 iter 38 sum correct 0.9051482371794872 loss 1.885439157485962
epoch 92 iter 39 sum correct 0.905615234375 loss 1.8854010105133057
epoch 92 iter 40 sum correct 0.9060118140243902 loss 1.885369062423706
epoch 92 iter 41 sum correct 0.9057849702380952 loss 1.8853874206542969
epoch 92 iter 42 sum correct 0.9059774709302325 loss 1.8853760957717896
epoch 92 iter 43 sum correct 0.90576171875 loss 1.8853893280029297
epoch 92 iter 44 sum correct 0.9056857638888889 loss 1.8853946924209595
epoch 92 iter 45 sum correct 0.9055706521739131 loss 1.8854035139083862
epoch 92 iter 46 sum correct 0.9056682180851063 loss 1.8853987455368042
epoch 92 iter 47 sum correct 0.9059244791666666 loss 1.8853744268417358
epoch 92 iter 48 sum correct 0.9060507015306123 loss 1.8853648900985718
epoch 92 iter 49 sum correct 0.9061328125 loss 1.885357141494751
epoch 92 iter 50 sum correct 0.9057521446078431 loss 1.8853874206542969
epoch 92 iter 51 sum correct 0.9057992788461539 loss 1.8853809833526611
epoch 92 iter 52 sum correct 0.9055866745283019 loss 1.8853946924209595
epoch 92 iter 53 sum correct 0.9054904513888888 loss 1.8854010105133057
epoch 92 iter 54 sum correct 0.9054332386363636 loss 1.885405421257019
epoch 92 iter 55 sum correct 0.9050990513392857 loss 1.8854326009750366
epoch 92 iter 56 sum correct 0.8900767543859649 loss 1.88575279712677
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.68it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.17it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.75it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.88it/s]8it [00:01,  5.78it/s]                                 macro  0.5223490345363223
micro  0.6219002507662301
[[216   0  57  26  89  20  59]
 [ 14   0   9   6  19   1   7]
 [ 39   0 219  19 105  54  60]
 [ 20   0  10 737  38  22  68]
 [ 54   0  74  28 334  13 150]
 [  8   0  24  17  13 341  12]
 [ 31   0  39  45  95  12 385]]
              precision    recall  f1-score   support

           0       0.57      0.46      0.51       467
           1       0.00      0.00      0.00        56
           2       0.51      0.44      0.47       496
           3       0.84      0.82      0.83       895
           4       0.48      0.51      0.50       653
           5       0.74      0.82      0.78       415
           6       0.52      0.63      0.57       607

    accuracy                           0.62      3589
   macro avg       0.52      0.53      0.52      3589
weighted avg       0.61      0.62      0.62      3589

correct 0.6219002507662301 
f1 0.5223490345363223 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.83it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.52it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.92it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.19it/s]8it [00:01,  6.13it/s]                                 macro  0.5423919170135664
micro  0.6450264697687378
[[244   0  66  19  91  16  55]
 [ 13   0  14   3  19   2   4]
 [ 31   0 256  24 104  53  60]
 [ 15   0  19 767  33  13  32]
 [ 29   0  75  28 311  11 140]
 [  7   0  30  24   8 327  20]
 [ 35   0  41  39  86  15 410]]
              precision    recall  f1-score   support

           0       0.65      0.50      0.56       491
           1       0.00      0.00      0.00        55
           2       0.51      0.48      0.50       528
           3       0.85      0.87      0.86       879
           4       0.48      0.52      0.50       594
           5       0.75      0.79      0.77       416
           6       0.57      0.65      0.61       626

    accuracy                           0.65      3589
   macro avg       0.54      0.55      0.54      3589
weighted avg       0.64      0.65      0.64      3589

correct 0.6450264697687378 
f1 0.5423919170135664 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:36,  1.50it/s]  4%|▎         | 2/56.072265625 [00:00<00:22,  2.41it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.89it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.32it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.49it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.73it/s] 12%|█▏        | 7/56.072265625 [00:02<00:12,  3.79it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.93it/s] 16%|█▌        | 9/56.072265625 [00:02<00:11,  3.92it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.04it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.99it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.08it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  4.04it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.12it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.06it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.12it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.04it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.12it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.03it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.11it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.04it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.12it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.04it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.12it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.07it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.13it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.05it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.13it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.06it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.13it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.05it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.13it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.04it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.12it/s] 62%|██████▏   | 35/56.072265625 [00:08<00:05,  4.06it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.11it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.03it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.11it/s] 70%|██████▉   | 39/56.072265625 [00:09<00:04,  4.04it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.12it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.06it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.13it/s] 77%|███████▋  | 43/56.072265625 [00:10<00:03,  4.05it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.13it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.05it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.13it/s] 84%|████████▍ | 47/56.072265625 [00:11<00:02,  4.07it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.14it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.05it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.13it/s] 91%|█████████ | 51/56.072265625 [00:12<00:01,  4.07it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.15it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.09it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.16it/s] 98%|█████████▊| 55/56.072265625 [00:13<00:00,  4.09it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.15it/s]57it [00:14,  5.01it/s]                                  57it [00:14,  3.99it/s]epoch 93 iter 0 sum correct 0.92578125 loss 1.8838856220245361
epoch 93 iter 1 sum correct 0.916015625 loss 1.8846054077148438
epoch 93 iter 2 sum correct 0.91796875 loss 1.8844159841537476
epoch 93 iter 3 sum correct 0.9189453125 loss 1.8843410015106201
epoch 93 iter 4 sum correct 0.922265625 loss 1.8840750455856323
epoch 93 iter 5 sum correct 0.91796875 loss 1.8844327926635742
epoch 93 iter 6 sum correct 0.9154575892857143 loss 1.8846378326416016
epoch 93 iter 7 sum correct 0.914306640625 loss 1.8847185373306274
epoch 93 iter 8 sum correct 0.9140625 loss 1.8847304582595825
epoch 93 iter 9 sum correct 0.9140625 loss 1.8847366571426392
epoch 93 iter 10 sum correct 0.912109375 loss 1.884909987449646
epoch 93 iter 11 sum correct 0.9124348958333334 loss 1.8848915100097656
epoch 93 iter 12 sum correct 0.9118088942307693 loss 1.8849461078643799
epoch 93 iter 13 sum correct 0.9111328125 loss 1.8849936723709106
epoch 93 iter 14 sum correct 0.91171875 loss 1.8849478960037231
epoch 93 iter 15 sum correct 0.911376953125 loss 1.8849787712097168
epoch 93 iter 16 sum correct 0.9113051470588235 loss 1.8849835395812988
epoch 93 iter 17 sum correct 0.9109157986111112 loss 1.8850103616714478
epoch 93 iter 18 sum correct 0.9102590460526315 loss 1.8850566148757935
epoch 93 iter 19 sum correct 0.91103515625 loss 1.8849941492080688
epoch 93 iter 20 sum correct 0.9117373511904762 loss 1.8849431276321411
epoch 93 iter 21 sum correct 0.912109375 loss 1.8849050998687744
epoch 93 iter 22 sum correct 0.9116847826086957 loss 1.8849362134933472
epoch 93 iter 23 sum correct 0.910888671875 loss 1.885000467300415
epoch 93 iter 24 sum correct 0.910703125 loss 1.8850144147872925
epoch 93 iter 25 sum correct 0.9103064903846154 loss 1.8850418329238892
epoch 93 iter 26 sum correct 0.9103732638888888 loss 1.8850407600402832
epoch 93 iter 27 sum correct 0.90966796875 loss 1.8850922584533691
epoch 93 iter 28 sum correct 0.9093480603448276 loss 1.8851152658462524
epoch 93 iter 29 sum correct 0.9095703125 loss 1.8850982189178467
epoch 93 iter 30 sum correct 0.9089591733870968 loss 1.8851394653320312
epoch 93 iter 31 sum correct 0.90887451171875 loss 1.8851486444473267
epoch 93 iter 32 sum correct 0.9084398674242424 loss 1.8851796388626099
epoch 93 iter 33 sum correct 0.9079159007352942 loss 1.8852216005325317
epoch 93 iter 34 sum correct 0.9080357142857143 loss 1.8852131366729736
epoch 93 iter 35 sum correct 0.9083116319444444 loss 1.8851912021636963
epoch 93 iter 36 sum correct 0.9079919763513513 loss 1.8852190971374512
epoch 93 iter 37 sum correct 0.9081003289473685 loss 1.8852109909057617
epoch 93 iter 38 sum correct 0.9083032852564102 loss 1.8851940631866455
epoch 93 iter 39 sum correct 0.90830078125 loss 1.8851938247680664
epoch 93 iter 40 sum correct 0.9083460365853658 loss 1.8851885795593262
epoch 93 iter 41 sum correct 0.9084821428571429 loss 1.8851741552352905
epoch 93 iter 42 sum correct 0.9086573401162791 loss 1.8851609230041504
epoch 93 iter 43 sum correct 0.9084250710227273 loss 1.8851776123046875
epoch 93 iter 44 sum correct 0.9086805555555556 loss 1.8851547241210938
epoch 93 iter 45 sum correct 0.908203125 loss 1.8851909637451172
epoch 93 iter 46 sum correct 0.9080369015957447 loss 1.8852012157440186
epoch 93 iter 47 sum correct 0.9080403645833334 loss 1.8852019309997559
epoch 93 iter 48 sum correct 0.9075255102040817 loss 1.885242223739624
epoch 93 iter 49 sum correct 0.907734375 loss 1.8852293491363525
epoch 93 iter 50 sum correct 0.9073988970588235 loss 1.8852559328079224
epoch 93 iter 51 sum correct 0.9073768028846154 loss 1.8852593898773193
epoch 93 iter 52 sum correct 0.9073555424528302 loss 1.8852579593658447
epoch 93 iter 53 sum correct 0.9077329282407407 loss 1.8852248191833496
epoch 93 iter 54 sum correct 0.9079545454545455 loss 1.8852062225341797
epoch 93 iter 55 sum correct 0.9076102120535714 loss 1.8852314949035645
epoch 93 iter 56 sum correct 0.892578125 loss 1.8855133056640625
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.66it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.28it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.32it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.61it/s]8it [00:01,  5.65it/s]                                 macro  0.524528727571683
micro  0.6182780718863193
[[274   0  74  14  44  20  41]
 [ 36   0   8   3   5   0   4]
 [ 54   0 266  10  81  37  48]
 [ 50   0  32 707  33  25  48]
 [ 93   0 116  15 313  18  98]
 [ 11   0  34  14  13 334   9]
 [ 70   0  58  42  98  14 325]]
              precision    recall  f1-score   support

           0       0.47      0.59      0.52       467
           1       0.00      0.00      0.00        56
           2       0.45      0.54      0.49       496
           3       0.88      0.79      0.83       895
           4       0.53      0.48      0.50       653
           5       0.75      0.80      0.77       415
           6       0.57      0.54      0.55       607

    accuracy                           0.62      3589
   macro avg       0.52      0.53      0.52      3589
weighted avg       0.62      0.62      0.62      3589

correct 0.6182780718863193 
f1 0.524528727571683 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.74it/s] 29%|██▊       | 2/7.009765625 [00:00<00:01,  3.36it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.51it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  6.26it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.12it/s]8it [00:01,  5.76it/s]                                 macro  0.5359088724346446
micro  0.6336026748397883
[[288   0  87  13  49  11  43]
 [ 30   0  15   1   5   2   2]
 [ 62   0 311  17  71  41  26]
 [ 39   0  32 728  36  24  20]
 [ 84   0 110  22 271   9  98]
 [ 11   0  45  18  10 320  12]
 [ 71   0  52  32  89  26 356]]
              precision    recall  f1-score   support

           0       0.49      0.59      0.54       491
           1       0.00      0.00      0.00        55
           2       0.48      0.59      0.53       528
           3       0.88      0.83      0.85       879
           4       0.51      0.46      0.48       594
           5       0.74      0.77      0.75       416
           6       0.64      0.57      0.60       626

    accuracy                           0.63      3589
   macro avg       0.53      0.54      0.54      3589
weighted avg       0.63      0.63      0.63      3589

correct 0.6336026748397883 
f1 0.5359088724346446 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:38,  1.43it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.33it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.82it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.27it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.44it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.68it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.75it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.92it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.90it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.02it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  4.00it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.09it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  4.03it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.11it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.06it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.14it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.06it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.13it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.07it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.13it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.05it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  3.91it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  3.92it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.03it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.01it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.09it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.04it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.12it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.05it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.12it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.07it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.14it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.08it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.15it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.09it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.16it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.07it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.14it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  4.08it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.14it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.08it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.15it/s] 77%|███████▋  | 43/56.072265625 [00:10<00:03,  4.09it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.15it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.09it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.16it/s] 84%|████████▍ | 47/56.072265625 [00:11<00:02,  4.09it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.16it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.06it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.14it/s] 91%|█████████ | 51/56.072265625 [00:12<00:01,  4.10it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.17it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.11it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.15it/s] 98%|█████████▊| 55/56.072265625 [00:13<00:00,  4.09it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.16it/s]57it [00:14,  3.98it/s]                                  epoch 94 iter 0 sum correct 0.896484375 loss 1.885934591293335
epoch 94 iter 1 sum correct 0.8818359375 loss 1.8872663974761963
epoch 94 iter 2 sum correct 0.8795572916666666 loss 1.8874391317367554
epoch 94 iter 3 sum correct 0.8818359375 loss 1.8873251676559448
epoch 94 iter 4 sum correct 0.87421875 loss 1.887955904006958
epoch 94 iter 5 sum correct 0.8717447916666666 loss 1.8881443738937378
epoch 94 iter 6 sum correct 0.8702566964285714 loss 1.888256311416626
epoch 94 iter 7 sum correct 0.87353515625 loss 1.8880494832992554
epoch 94 iter 8 sum correct 0.8734809027777778 loss 1.8880441188812256
epoch 94 iter 9 sum correct 0.8728515625 loss 1.888079047203064
epoch 94 iter 10 sum correct 0.8741122159090909 loss 1.8879814147949219
epoch 94 iter 11 sum correct 0.8741861979166666 loss 1.8879649639129639
epoch 94 iter 12 sum correct 0.8743990384615384 loss 1.8879437446594238
epoch 94 iter 13 sum correct 0.8747209821428571 loss 1.8879128694534302
epoch 94 iter 14 sum correct 0.87421875 loss 1.8879796266555786
epoch 94 iter 15 sum correct 0.8760986328125 loss 1.887839674949646
epoch 94 iter 16 sum correct 0.8770680147058824 loss 1.8877555131912231
epoch 94 iter 17 sum correct 0.8773871527777778 loss 1.8877243995666504
epoch 94 iter 18 sum correct 0.8778782894736842 loss 1.8876779079437256
epoch 94 iter 19 sum correct 0.8783203125 loss 1.887642502784729
epoch 94 iter 20 sum correct 0.8804873511904762 loss 1.8874611854553223
epoch 94 iter 21 sum correct 0.8813032670454546 loss 1.8874067068099976
epoch 94 iter 22 sum correct 0.8815387228260869 loss 1.887387990951538
epoch 94 iter 23 sum correct 0.881103515625 loss 1.887431025505066
epoch 94 iter 24 sum correct 0.880390625 loss 1.8874894380569458
epoch 94 iter 25 sum correct 0.8809344951923077 loss 1.8874462842941284
epoch 94 iter 26 sum correct 0.8809317129629629 loss 1.8874478340148926
epoch 94 iter 27 sum correct 0.8812779017857143 loss 1.8874170780181885
epoch 94 iter 28 sum correct 0.8819369612068966 loss 1.8873575925827026
epoch 94 iter 29 sum correct 0.8821614583333334 loss 1.8873376846313477
epoch 94 iter 30 sum correct 0.8827494959677419 loss 1.8872876167297363
epoch 94 iter 31 sum correct 0.8829345703125 loss 1.8872694969177246
epoch 94 iter 32 sum correct 0.8831676136363636 loss 1.8872528076171875
epoch 94 iter 33 sum correct 0.8833869485294118 loss 1.887231469154358
epoch 94 iter 34 sum correct 0.88359375 loss 1.8872148990631104
epoch 94 iter 35 sum correct 0.8837890625 loss 1.8871971368789673
epoch 94 iter 36 sum correct 0.8838682432432432 loss 1.8871885538101196
epoch 94 iter 37 sum correct 0.884046052631579 loss 1.8871715068817139
epoch 94 iter 38 sum correct 0.8847155448717948 loss 1.8871158361434937
epoch 94 iter 39 sum correct 0.88525390625 loss 1.887069582939148
epoch 94 iter 40 sum correct 0.8850038109756098 loss 1.8870878219604492
epoch 94 iter 41 sum correct 0.8850911458333334 loss 1.8870810270309448
epoch 94 iter 42 sum correct 0.8849473110465116 loss 1.8870930671691895
epoch 94 iter 43 sum correct 0.8852095170454546 loss 1.8870675563812256
epoch 94 iter 44 sum correct 0.88515625 loss 1.8870656490325928
epoch 94 iter 45 sum correct 0.8850628396739131 loss 1.887073040008545
epoch 94 iter 46 sum correct 0.8856798537234043 loss 1.8870244026184082
epoch 94 iter 47 sum correct 0.8863118489583334 loss 1.8869751691818237
epoch 94 iter 48 sum correct 0.8871572066326531 loss 1.8869112730026245
epoch 94 iter 49 sum correct 0.88734375 loss 1.886901617050171
epoch 94 iter 50 sum correct 0.8877144607843137 loss 1.8868685960769653
epoch 94 iter 51 sum correct 0.8881460336538461 loss 1.8868354558944702
epoch 94 iter 52 sum correct 0.8879716981132075 loss 1.8868471384048462
epoch 94 iter 53 sum correct 0.888671875 loss 1.8867912292480469
epoch 94 iter 54 sum correct 0.8886363636363637 loss 1.8867939710617065
epoch 94 iter 55 sum correct 0.8888113839285714 loss 1.8867803812026978
epoch 94 iter 56 sum correct 0.874280427631579 loss 1.8868643045425415
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.50it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.01it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.59it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.83it/s]8it [00:01,  5.68it/s]                                 macro  0.5276198500240691
micro  0.6224575090554472
[[243   0  47  23  57  12  85]
 [ 21   0  11   4  10   0  10]
 [ 44   0 221  11  91  48  81]
 [ 22   0  22 697  27  21 106]
 [ 59   0  72  19 315   9 179]
 [ 11   0  29  12  11 326  26]
 [ 37   0  35  35  64   4 432]]
              precision    recall  f1-score   support

           0       0.56      0.52      0.54       467
           1       0.00      0.00      0.00        56
           2       0.51      0.45      0.47       496
           3       0.87      0.78      0.82       895
           4       0.55      0.48      0.51       653
           5       0.78      0.79      0.78       415
           6       0.47      0.71      0.57       607

    accuracy                           0.62      3589
   macro avg       0.53      0.53      0.53      3589
weighted avg       0.63      0.62      0.62      3589

correct 0.6224575090554472 
f1 0.5276198500240691 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.75it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.45it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.85it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.05it/s]8it [00:01,  6.01it/s]                                 macro  0.5406622893051737
micro  0.6397325160211759
[[256   0  50  16  69  12  88]
 [ 20   0   8   5  11   2   9]
 [ 44   0 259  14  88  43  80]
 [ 18   0  13 722  32  16  78]
 [ 56   0  61  24 268   5 180]
 [  7   0  26  21  12 315  35]
 [ 33   0  30  13  67   7 476]]
              precision    recall  f1-score   support

           0       0.59      0.52      0.55       491
           1       0.00      0.00      0.00        55
           2       0.58      0.49      0.53       528
           3       0.89      0.82      0.85       879
           4       0.49      0.45      0.47       594
           5       0.79      0.76      0.77       416
           6       0.50      0.76      0.61       626

    accuracy                           0.64      3589
   macro avg       0.55      0.54      0.54      3589
weighted avg       0.64      0.64      0.64      3589

correct 0.6397325160211759 
f1 0.5406622893051737 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:38,  1.44it/s]  4%|▎         | 2/56.072265625 [00:00<00:22,  2.37it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.87it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.26it/s]  9%|▉         | 5/56.072265625 [00:01<00:15,  3.40it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.66it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.74it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.91it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.80it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  3.92it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.91it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.01it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  3.93it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.01it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  3.98it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.06it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.00it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.09it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.05it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.13it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.04it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.12it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.05it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.05it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  3.91it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  3.97it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  3.91it/s] 50%|████▉     | 28/56.072265625 [00:07<00:07,  3.97it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  3.94it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.05it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.01it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.03it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  3.94it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.04it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.01it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.10it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  3.96it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  3.98it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  3.87it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:04,  3.97it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  3.93it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.04it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  3.99it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.08it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.01it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.10it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  4.03it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:02,  4.03it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  3.91it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  3.98it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  3.94it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:01,  4.04it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.00it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.09it/s] 98%|█████████▊| 55/56.072265625 [00:14<00:00,  4.06it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.12it/s]57it [00:14,  4.96it/s]                                  57it [00:14,  3.91it/s]epoch 95 iter 0 sum correct 0.908203125 loss 1.8850955963134766
epoch 95 iter 1 sum correct 0.904296875 loss 1.885363221168518
epoch 95 iter 2 sum correct 0.9075520833333334 loss 1.8851451873779297
epoch 95 iter 3 sum correct 0.90478515625 loss 1.8853764533996582
epoch 95 iter 4 sum correct 0.90625 loss 1.8852932453155518
epoch 95 iter 5 sum correct 0.9036458333333334 loss 1.8854918479919434
epoch 95 iter 6 sum correct 0.9012276785714286 loss 1.8857088088989258
epoch 95 iter 7 sum correct 0.899169921875 loss 1.8858760595321655
epoch 95 iter 8 sum correct 0.8936631944444444 loss 1.8863160610198975
epoch 95 iter 9 sum correct 0.891796875 loss 1.8864681720733643
epoch 95 iter 10 sum correct 0.8927556818181818 loss 1.8864059448242188
epoch 95 iter 11 sum correct 0.8919270833333334 loss 1.8864753246307373
epoch 95 iter 12 sum correct 0.8904747596153846 loss 1.8865970373153687
epoch 95 iter 13 sum correct 0.8892299107142857 loss 1.8866844177246094
epoch 95 iter 14 sum correct 0.8911458333333333 loss 1.8865253925323486
epoch 95 iter 15 sum correct 0.8902587890625 loss 1.8865984678268433
epoch 95 iter 16 sum correct 0.8902803308823529 loss 1.8865901231765747
epoch 95 iter 17 sum correct 0.8900824652777778 loss 1.8866127729415894
epoch 95 iter 18 sum correct 0.8896998355263158 loss 1.8866312503814697
epoch 95 iter 19 sum correct 0.89091796875 loss 1.8865432739257812
epoch 95 iter 20 sum correct 0.8922061011904762 loss 1.8864439725875854
epoch 95 iter 21 sum correct 0.8918678977272727 loss 1.8864729404449463
epoch 95 iter 22 sum correct 0.8902004076086957 loss 1.8866058588027954
epoch 95 iter 23 sum correct 0.8899739583333334 loss 1.8866280317306519
epoch 95 iter 24 sum correct 0.8896875 loss 1.886644721031189
epoch 95 iter 25 sum correct 0.8900240384615384 loss 1.886622428894043
epoch 95 iter 26 sum correct 0.8904079861111112 loss 1.886594295501709
epoch 95 iter 27 sum correct 0.8906947544642857 loss 1.8865747451782227
epoch 95 iter 28 sum correct 0.890692349137931 loss 1.886579155921936
epoch 95 iter 29 sum correct 0.8904947916666667 loss 1.8865975141525269
epoch 95 iter 30 sum correct 0.8903099798387096 loss 1.8866119384765625
epoch 95 iter 31 sum correct 0.889892578125 loss 1.8866519927978516
epoch 95 iter 32 sum correct 0.8896780303030303 loss 1.8866654634475708
epoch 95 iter 33 sum correct 0.8897058823529411 loss 1.8866626024246216
epoch 95 iter 34 sum correct 0.8892299107142857 loss 1.8866993188858032
epoch 95 iter 35 sum correct 0.8892686631944444 loss 1.8866969347000122
epoch 95 iter 36 sum correct 0.889674831081081 loss 1.886662244796753
epoch 95 iter 37 sum correct 0.8894942434210527 loss 1.8866764307022095
epoch 95 iter 38 sum correct 0.8907251602564102 loss 1.88657808303833
epoch 95 iter 39 sum correct 0.88994140625 loss 1.886638879776001
epoch 95 iter 40 sum correct 0.8903391768292683 loss 1.8866097927093506
epoch 95 iter 41 sum correct 0.8907645089285714 loss 1.886576533317566
epoch 95 iter 42 sum correct 0.8911700581395349 loss 1.8865448236465454
epoch 95 iter 43 sum correct 0.8908913352272727 loss 1.8865630626678467
epoch 95 iter 44 sum correct 0.8905815972222222 loss 1.8865909576416016
epoch 95 iter 45 sum correct 0.8908372961956522 loss 1.8865816593170166
epoch 95 iter 46 sum correct 0.8904172207446809 loss 1.8866139650344849
epoch 95 iter 47 sum correct 0.8904215494791666 loss 1.886611819267273
epoch 95 iter 48 sum correct 0.8907445790816326 loss 1.8865927457809448
epoch 95 iter 49 sum correct 0.890703125 loss 1.8865940570831299
epoch 95 iter 50 sum correct 0.8909696691176471 loss 1.8865735530853271
epoch 95 iter 51 sum correct 0.8910757211538461 loss 1.8865571022033691
epoch 95 iter 52 sum correct 0.8913988797169812 loss 1.8865329027175903
epoch 95 iter 53 sum correct 0.8914207175925926 loss 1.8865303993225098
epoch 95 iter 54 sum correct 0.89140625 loss 1.8865309953689575
epoch 95 iter 55 sum correct 0.8915318080357143 loss 1.886522650718689
epoch 95 iter 56 sum correct 0.876953125 loss 1.8866181373596191
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.73it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.40it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.82it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.11it/s]8it [00:01,  6.05it/s]                                 macro  0.526122994640749
micro  0.6157704095848425
[[237   0  61  15 104   2  48]
 [ 24   0  10   2  14   0   6]
 [ 38   0 238  14 129  30  47]
 [ 38   0  30 692  54  15  66]
 [ 54   0  90  15 372   3 119]
 [ 18   0  46  12  18 300  21]
 [ 29   0  46  30 129   2 371]]
              precision    recall  f1-score   support

           0       0.54      0.51      0.52       467
           1       0.00      0.00      0.00        56
           2       0.46      0.48      0.47       496
           3       0.89      0.77      0.83       895
           4       0.45      0.57      0.51       653
           5       0.85      0.72      0.78       415
           6       0.55      0.61      0.58       607

    accuracy                           0.62      3589
   macro avg       0.53      0.52      0.53      3589
weighted avg       0.63      0.62      0.62      3589

correct 0.6157704095848425 
f1 0.526122994640749 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.76it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.41it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.92it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.14it/s]8it [00:01,  6.05it/s]                                 macro  0.5449588937416866
micro  0.6394538868765672
[[270   0  61  12  92   5  51]
 [ 24   0   7   3  11   2   8]
 [ 54   0 272  15 118  27  42]
 [ 29   0  30 714  43  15  48]
 [ 40   0  80  21 346   3 104]
 [ 13   0  50  15  21 292  25]
 [ 32   0  41  19 126   7 401]]
              precision    recall  f1-score   support

           0       0.58      0.55      0.57       491
           1       0.00      0.00      0.00        55
           2       0.50      0.52      0.51       528
           3       0.89      0.81      0.85       879
           4       0.46      0.58      0.51       594
           5       0.83      0.70      0.76       416
           6       0.59      0.64      0.61       626

    accuracy                           0.64      3589
   macro avg       0.55      0.54      0.54      3589
weighted avg       0.65      0.64      0.64      3589

correct 0.6394538868765672 
f1 0.5449588937416866 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:38,  1.42it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.34it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.84it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.28it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.44it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.70it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.75it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.90it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.89it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.01it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.95it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.05it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  3.99it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.07it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.01it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.10it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.02it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.10it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.03it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.10it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.01it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.09it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.04it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.11it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.01it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.10it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  3.99it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.08it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  3.99it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.08it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.04it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.11it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.03it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.11it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.06it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.13it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.04it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.12it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  4.05it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.12it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.04it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.11it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  4.05it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.12it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.04it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.11it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  4.04it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:02,  3.86it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  3.86it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  3.99it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  3.98it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.08it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.04it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.12it/s] 98%|█████████▊| 55/56.072265625 [00:14<00:00,  4.06it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.13it/s]57it [00:14,  3.95it/s]                                  epoch 96 iter 0 sum correct 0.888671875 loss 1.8866232633590698
epoch 96 iter 1 sum correct 0.8876953125 loss 1.8867197036743164
epoch 96 iter 2 sum correct 0.9029947916666666 loss 1.8856338262557983
epoch 96 iter 3 sum correct 0.90283203125 loss 1.885655164718628
epoch 96 iter 4 sum correct 0.90078125 loss 1.8858360052108765
epoch 96 iter 5 sum correct 0.8977864583333334 loss 1.8860501050949097
epoch 96 iter 6 sum correct 0.896484375 loss 1.8861675262451172
epoch 96 iter 7 sum correct 0.893798828125 loss 1.8863738775253296
epoch 96 iter 8 sum correct 0.8938802083333334 loss 1.886331558227539
epoch 96 iter 9 sum correct 0.891796875 loss 1.8865092992782593
epoch 96 iter 10 sum correct 0.8900923295454546 loss 1.886643409729004
epoch 96 iter 11 sum correct 0.8873697916666666 loss 1.8868634700775146
epoch 96 iter 12 sum correct 0.8861177884615384 loss 1.8869740962982178
epoch 96 iter 13 sum correct 0.8835100446428571 loss 1.8871923685073853
epoch 96 iter 14 sum correct 0.8837239583333333 loss 1.8871687650680542
epoch 96 iter 15 sum correct 0.88134765625 loss 1.887336015701294
epoch 96 iter 16 sum correct 0.8800551470588235 loss 1.8874269723892212
epoch 96 iter 17 sum correct 0.8798828125 loss 1.8874441385269165
epoch 96 iter 18 sum correct 0.8805509868421053 loss 1.8873690366744995
epoch 96 iter 19 sum correct 0.88125 loss 1.8873084783554077
epoch 96 iter 20 sum correct 0.8812313988095238 loss 1.8873008489608765
epoch 96 iter 21 sum correct 0.8807705965909091 loss 1.887330412864685
epoch 96 iter 22 sum correct 0.8813688858695652 loss 1.887291669845581
epoch 96 iter 23 sum correct 0.8810221354166666 loss 1.8873189687728882
epoch 96 iter 24 sum correct 0.8803125 loss 1.8873822689056396
epoch 96 iter 25 sum correct 0.8801081730769231 loss 1.8874138593673706
epoch 96 iter 26 sum correct 0.8814380787037037 loss 1.88730788230896
epoch 96 iter 27 sum correct 0.8809988839285714 loss 1.8873313665390015
epoch 96 iter 28 sum correct 0.8810614224137931 loss 1.887330174446106
epoch 96 iter 29 sum correct 0.8817057291666667 loss 1.8872772455215454
epoch 96 iter 30 sum correct 0.8826234879032258 loss 1.8871990442276
epoch 96 iter 31 sum correct 0.88311767578125 loss 1.88715660572052
epoch 96 iter 32 sum correct 0.8835227272727273 loss 1.8871303796768188
epoch 96 iter 33 sum correct 0.8833295036764706 loss 1.8871490955352783
epoch 96 iter 34 sum correct 0.8837053571428571 loss 1.8871233463287354
epoch 96 iter 35 sum correct 0.8842230902777778 loss 1.8870809078216553
epoch 96 iter 36 sum correct 0.8841849662162162 loss 1.8870817422866821
epoch 96 iter 37 sum correct 0.884508634868421 loss 1.8870594501495361
epoch 96 iter 38 sum correct 0.8848157051282052 loss 1.887040615081787
epoch 96 iter 39 sum correct 0.885498046875 loss 1.8869835138320923
epoch 96 iter 40 sum correct 0.885813643292683 loss 1.8869619369506836
epoch 96 iter 41 sum correct 0.8858351934523809 loss 1.8869600296020508
epoch 96 iter 42 sum correct 0.8858557412790697 loss 1.8869541883468628
epoch 96 iter 43 sum correct 0.8857865767045454 loss 1.8869571685791016
epoch 96 iter 44 sum correct 0.8856336805555556 loss 1.8869673013687134
epoch 96 iter 45 sum correct 0.8856148097826086 loss 1.8869645595550537
epoch 96 iter 46 sum correct 0.8854720744680851 loss 1.886974811553955
epoch 96 iter 47 sum correct 0.8859456380208334 loss 1.8869351148605347
epoch 96 iter 48 sum correct 0.886360012755102 loss 1.886903166770935
epoch 96 iter 49 sum correct 0.8865625 loss 1.8868839740753174
epoch 96 iter 50 sum correct 0.8866421568627451 loss 1.886878252029419
epoch 96 iter 51 sum correct 0.8868689903846154 loss 1.8868569135665894
epoch 96 iter 52 sum correct 0.8871241155660378 loss 1.8868374824523926
epoch 96 iter 53 sum correct 0.8874421296296297 loss 1.8868123292922974
epoch 96 iter 54 sum correct 0.887997159090909 loss 1.8867673873901367
epoch 96 iter 55 sum correct 0.8877999441964286 loss 1.8867812156677246
epoch 96 iter 56 sum correct 0.8733209978070176 loss 1.8868132829666138
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.67it/s] 29%|██▊       | 2/7.009765625 [00:00<00:01,  3.26it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.45it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  6.19it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.31it/s]8it [00:01,  5.73it/s]                                 macro  0.5254704253707324
micro  0.6263583170799666
[[252   0  40  29  92  11  43]
 [ 20   0  11   5  17   0   3]
 [ 58   0 185  19 134  51  49]
 [ 19   0   2 753  45  26  50]
 [ 62   0  60  42 388  18  83]
 [ 14   0  20  19  16 337   9]
 [ 31   0  23  50 162   8 333]]
              precision    recall  f1-score   support

           0       0.55      0.54      0.55       467
           1       0.00      0.00      0.00        56
           2       0.54      0.37      0.44       496
           3       0.82      0.84      0.83       895
           4       0.45      0.59      0.51       653
           5       0.75      0.81      0.78       415
           6       0.58      0.55      0.57       607

    accuracy                           0.63      3589
   macro avg       0.53      0.53      0.53      3589
weighted avg       0.62      0.63      0.62      3589

correct 0.6263583170799666 
f1 0.5254704253707324 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.74it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.40it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.83it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.90it/s]8it [00:01,  5.87it/s]                                 macro  0.5441363901246267
micro  0.6475341320702146
[[276   0  52  22  88  11  42]
 [ 26   0   3   5  16   3   2]
 [ 56   0 216  31 126  57  42]
 [ 16   0  11 763  35  23  31]
 [ 44   0  41  39 368  13  89]
 [ 10   0  24  23  11 331  17]
 [ 39   0  32  25 148  12 370]]
              precision    recall  f1-score   support

           0       0.59      0.56      0.58       491
           1       0.00      0.00      0.00        55
           2       0.57      0.41      0.48       528
           3       0.84      0.87      0.85       879
           4       0.46      0.62      0.53       594
           5       0.74      0.80      0.76       416
           6       0.62      0.59      0.61       626

    accuracy                           0.65      3589
   macro avg       0.55      0.55      0.54      3589
weighted avg       0.64      0.65      0.64      3589

correct 0.6475341320702146 
f1 0.5441363901246267 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:36,  1.49it/s]  4%|▎         | 2/56.072265625 [00:00<00:22,  2.39it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.89it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.31it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.48it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.69it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.75it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.90it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.88it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.00it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.90it/s] 21%|██▏       | 12/56.072265625 [00:03<00:11,  4.00it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  3.95it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.03it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  3.97it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.04it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  3.93it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.02it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  3.98it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.07it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.03it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.11it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  4.06it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.13it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.08it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.14it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.08it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.15it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.07it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.14it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.07it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.15it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.09it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.15it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.07it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.14it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.08it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.15it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  4.09it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.16it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  4.09it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.16it/s] 77%|███████▋  | 43/56.072265625 [00:10<00:03,  4.09it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:02,  4.16it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  4.09it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.15it/s] 84%|████████▍ | 47/56.072265625 [00:11<00:02,  4.04it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.12it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.06it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.14it/s] 91%|█████████ | 51/56.072265625 [00:12<00:01,  4.08it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.15it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.08it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.15it/s] 98%|█████████▊| 55/56.072265625 [00:13<00:00,  4.08it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.15it/s]57it [00:14,  4.99it/s]                                  57it [00:14,  3.98it/s]epoch 97 iter 0 sum correct 0.904296875 loss 1.885469675064087
epoch 97 iter 1 sum correct 0.8984375 loss 1.8858261108398438
epoch 97 iter 2 sum correct 0.8997395833333334 loss 1.8858442306518555
epoch 97 iter 3 sum correct 0.89599609375 loss 1.8861134052276611
epoch 97 iter 4 sum correct 0.898046875 loss 1.8859666585922241
epoch 97 iter 5 sum correct 0.8942057291666666 loss 1.886308193206787
epoch 97 iter 6 sum correct 0.8967633928571429 loss 1.8860989809036255
epoch 97 iter 7 sum correct 0.899658203125 loss 1.8858965635299683
epoch 97 iter 8 sum correct 0.9016927083333334 loss 1.8857152462005615
epoch 97 iter 9 sum correct 0.901953125 loss 1.885703682899475
epoch 97 iter 10 sum correct 0.9016335227272727 loss 1.8857096433639526
epoch 97 iter 11 sum correct 0.90185546875 loss 1.8856801986694336
epoch 97 iter 12 sum correct 0.9012920673076923 loss 1.8857295513153076
epoch 97 iter 13 sum correct 0.9009486607142857 loss 1.8857427835464478
epoch 97 iter 14 sum correct 0.9009114583333333 loss 1.8857364654541016
epoch 97 iter 15 sum correct 0.899658203125 loss 1.885832667350769
epoch 97 iter 16 sum correct 0.8998161764705882 loss 1.885813593864441
epoch 97 iter 17 sum correct 0.8991970486111112 loss 1.8858598470687866
epoch 97 iter 18 sum correct 0.8989514802631579 loss 1.8858833312988281
epoch 97 iter 19 sum correct 0.89970703125 loss 1.8858263492584229
epoch 97 iter 20 sum correct 0.9007626488095238 loss 1.8857403993606567
epoch 97 iter 21 sum correct 0.900390625 loss 1.885765790939331
epoch 97 iter 22 sum correct 0.9009001358695652 loss 1.8857272863388062
epoch 97 iter 23 sum correct 0.9016927083333334 loss 1.8856700658798218
epoch 97 iter 24 sum correct 0.90125 loss 1.8857043981552124
epoch 97 iter 25 sum correct 0.9007662259615384 loss 1.885735034942627
epoch 97 iter 26 sum correct 0.9008969907407407 loss 1.8857312202453613
epoch 97 iter 27 sum correct 0.9006696428571429 loss 1.8857524394989014
epoch 97 iter 28 sum correct 0.9011314655172413 loss 1.8857271671295166
epoch 97 iter 29 sum correct 0.9013671875 loss 1.8857104778289795
epoch 97 iter 30 sum correct 0.9015877016129032 loss 1.8856921195983887
epoch 97 iter 31 sum correct 0.902099609375 loss 1.8856616020202637
epoch 97 iter 32 sum correct 0.9024621212121212 loss 1.8856313228607178
epoch 97 iter 33 sum correct 0.9032054227941176 loss 1.8855750560760498
epoch 97 iter 34 sum correct 0.9029575892857142 loss 1.8855990171432495
epoch 97 iter 35 sum correct 0.9027235243055556 loss 1.8856167793273926
epoch 97 iter 36 sum correct 0.9029771959459459 loss 1.8856030702590942
epoch 97 iter 37 sum correct 0.9030119243421053 loss 1.8856010437011719
epoch 97 iter 38 sum correct 0.9032451923076923 loss 1.8855786323547363
epoch 97 iter 39 sum correct 0.9025390625 loss 1.8856302499771118
epoch 97 iter 40 sum correct 0.9028201219512195 loss 1.8856106996536255
epoch 97 iter 41 sum correct 0.9025762648809523 loss 1.8856297731399536
epoch 97 iter 42 sum correct 0.9024345930232558 loss 1.8856420516967773
epoch 97 iter 43 sum correct 0.9022549715909091 loss 1.8856595754623413
epoch 97 iter 44 sum correct 0.9020399305555555 loss 1.885677456855774
epoch 97 iter 45 sum correct 0.9015794836956522 loss 1.8857176303863525
epoch 97 iter 46 sum correct 0.9020528590425532 loss 1.8856804370880127
epoch 97 iter 47 sum correct 0.9019775390625 loss 1.8856866359710693
epoch 97 iter 48 sum correct 0.9018654336734694 loss 1.885693073272705
epoch 97 iter 49 sum correct 0.9022265625 loss 1.8856638669967651
epoch 97 iter 50 sum correct 0.9022288602941176 loss 1.8856602907180786
epoch 97 iter 51 sum correct 0.9019681490384616 loss 1.8856794834136963
epoch 97 iter 52 sum correct 0.9017172759433962 loss 1.8856956958770752
epoch 97 iter 53 sum correct 0.9014033564814815 loss 1.885720133781433
epoch 97 iter 54 sum correct 0.9013494318181818 loss 1.8857252597808838
epoch 97 iter 55 sum correct 0.9012276785714286 loss 1.885732650756836
epoch 97 iter 56 sum correct 0.8866159539473685 loss 1.885669231414795
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.74it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.42it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.87it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.14it/s]8it [00:01,  6.02it/s]                                 macro  0.5169447437276286
micro  0.620228475898579
[[292   0  51  29  39  23  33]
 [ 27   0   9   6  10   1   3]
 [ 65   0 208  20  93  63  47]
 [ 32   0   7 755  20  38  43]
 [103   0  83  43 276  30 118]
 [  9   0  18  18   9 351  10]
 [ 80   0  29  54  78  22 344]]
              precision    recall  f1-score   support

           0       0.48      0.63      0.54       467
           1       0.00      0.00      0.00        56
           2       0.51      0.42      0.46       496
           3       0.82      0.84      0.83       895
           4       0.53      0.42      0.47       653
           5       0.66      0.85      0.74       415
           6       0.58      0.57      0.57       607

    accuracy                           0.62      3589
   macro avg       0.51      0.53      0.52      3589
weighted avg       0.61      0.62      0.61      3589

correct 0.620228475898579 
f1 0.5169447437276286 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.80it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.53it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.83it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.00it/s]8it [00:01,  6.04it/s]                                 macro  0.5242721889947782
micro  0.6297018668152689
[[318   0  48  25  47  22  31]
 [ 27   0  11   6   5   3   3]
 [ 72   0 227  28 100  70  31]
 [ 23   0  12 766  18  37  23]
 [ 96   0  67  48 251  19 113]
 [ 12   0  27  19   9 342   7]
 [ 85   0  38  46  76  25 356]]
              precision    recall  f1-score   support

           0       0.50      0.65      0.57       491
           1       0.00      0.00      0.00        55
           2       0.53      0.43      0.47       528
           3       0.82      0.87      0.84       879
           4       0.50      0.42      0.46       594
           5       0.66      0.82      0.73       416
           6       0.63      0.57      0.60       626

    accuracy                           0.63      3589
   macro avg       0.52      0.54      0.52      3589
weighted avg       0.62      0.63      0.62      3589

correct 0.6297018668152689 
f1 0.5242721889947782 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:38,  1.42it/s]  4%|▎         | 2/56.072265625 [00:00<00:23,  2.34it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.84it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.29it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.47it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.72it/s] 12%|█▏        | 7/56.072265625 [00:02<00:13,  3.77it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.93it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.91it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  4.03it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.97it/s] 21%|██▏       | 12/56.072265625 [00:03<00:10,  4.07it/s] 23%|██▎       | 13/56.072265625 [00:03<00:10,  4.03it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  4.11it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  4.06it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.14it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.07it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.08it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  3.95it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.05it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  4.01it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.06it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  3.98it/s] 43%|████▎     | 24/56.072265625 [00:06<00:07,  4.07it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  4.03it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.12it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  4.06it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.14it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.08it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.14it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.09it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.15it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.09it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.15it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.09it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.13it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  3.97it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.01it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  3.91it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:04,  3.97it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  3.89it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  3.95it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  3.89it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:03,  4.00it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  3.97it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.07it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  4.03it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.11it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.06it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.14it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  4.07it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.14it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.08it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.15it/s] 98%|█████████▊| 55/56.072265625 [00:13<00:00,  4.10it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.16it/s]57it [00:14,  3.96it/s]                                  epoch 98 iter 0 sum correct 0.900390625 loss 1.885938048362732
epoch 98 iter 1 sum correct 0.9130859375 loss 1.8847683668136597
epoch 98 iter 2 sum correct 0.9088541666666666 loss 1.8850630521774292
epoch 98 iter 3 sum correct 0.90185546875 loss 1.8856077194213867
epoch 98 iter 4 sum correct 0.903515625 loss 1.885500192642212
epoch 98 iter 5 sum correct 0.9046223958333334 loss 1.8853998184204102
epoch 98 iter 6 sum correct 0.9048549107142857 loss 1.8854275941848755
epoch 98 iter 7 sum correct 0.90380859375 loss 1.8854986429214478
epoch 98 iter 8 sum correct 0.90234375 loss 1.8856230974197388
epoch 98 iter 9 sum correct 0.9037109375 loss 1.8855081796646118
epoch 98 iter 10 sum correct 0.9026988636363636 loss 1.885594129562378
epoch 98 iter 11 sum correct 0.90185546875 loss 1.8856686353683472
epoch 98 iter 12 sum correct 0.9011418269230769 loss 1.88572096824646
epoch 98 iter 13 sum correct 0.9015066964285714 loss 1.8856884241104126
epoch 98 iter 14 sum correct 0.901171875 loss 1.8857203722000122
epoch 98 iter 15 sum correct 0.901123046875 loss 1.8857136964797974
epoch 98 iter 16 sum correct 0.9014246323529411 loss 1.8856834173202515
epoch 98 iter 17 sum correct 0.900390625 loss 1.8857699632644653
epoch 98 iter 18 sum correct 0.900390625 loss 1.8857730627059937
epoch 98 iter 19 sum correct 0.89951171875 loss 1.885837197303772
epoch 98 iter 20 sum correct 0.9000186011904762 loss 1.8857917785644531
epoch 98 iter 21 sum correct 0.9001242897727273 loss 1.8857783079147339
epoch 98 iter 22 sum correct 0.8998811141304348 loss 1.8857966661453247
epoch 98 iter 23 sum correct 0.9005533854166666 loss 1.8857457637786865
epoch 98 iter 24 sum correct 0.900625 loss 1.8857386112213135
epoch 98 iter 25 sum correct 0.9006159855769231 loss 1.8857448101043701
epoch 98 iter 26 sum correct 0.9013310185185185 loss 1.8856892585754395
epoch 98 iter 27 sum correct 0.9017857142857143 loss 1.885664463043213
epoch 98 iter 28 sum correct 0.9025457974137931 loss 1.8856064081192017
epoch 98 iter 29 sum correct 0.9033203125 loss 1.8855502605438232
epoch 98 iter 30 sum correct 0.9034148185483871 loss 1.8855419158935547
epoch 98 iter 31 sum correct 0.90325927734375 loss 1.8855578899383545
epoch 98 iter 32 sum correct 0.9037642045454546 loss 1.8855146169662476
epoch 98 iter 33 sum correct 0.9041819852941176 loss 1.8854832649230957
epoch 98 iter 34 sum correct 0.90390625 loss 1.885501503944397
epoch 98 iter 35 sum correct 0.9040256076388888 loss 1.8854972124099731
epoch 98 iter 36 sum correct 0.904296875 loss 1.8854762315750122
epoch 98 iter 37 sum correct 0.9046052631578947 loss 1.8854548931121826
epoch 98 iter 38 sum correct 0.904296875 loss 1.8854788541793823
epoch 98 iter 39 sum correct 0.904248046875 loss 1.8854843378067017
epoch 98 iter 40 sum correct 0.9046303353658537 loss 1.8854557275772095
epoch 98 iter 41 sum correct 0.9038318452380952 loss 1.8855191469192505
epoch 98 iter 42 sum correct 0.9034338662790697 loss 1.8855522871017456
epoch 98 iter 43 sum correct 0.9038529829545454 loss 1.8855184316635132
epoch 98 iter 44 sum correct 0.9036458333333334 loss 1.8855313062667847
epoch 98 iter 45 sum correct 0.903702445652174 loss 1.8855258226394653
epoch 98 iter 46 sum correct 0.9042137632978723 loss 1.8854846954345703
epoch 98 iter 47 sum correct 0.9041341145833334 loss 1.8854889869689941
epoch 98 iter 48 sum correct 0.9044164540816326 loss 1.885467529296875
epoch 98 iter 49 sum correct 0.9043359375 loss 1.8854705095291138
epoch 98 iter 50 sum correct 0.9044117647058824 loss 1.8854649066925049
epoch 98 iter 51 sum correct 0.9046349158653846 loss 1.8854445219039917
epoch 98 iter 52 sum correct 0.9042600235849056 loss 1.885475516319275
epoch 98 iter 53 sum correct 0.9040436921296297 loss 1.885491967201233
epoch 98 iter 54 sum correct 0.9037642045454546 loss 1.8855094909667969
epoch 98 iter 55 sum correct 0.9039132254464286 loss 1.8854987621307373
epoch 98 iter 56 sum correct 0.8892201206140351 loss 1.8854776620864868
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.80it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.45it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.93it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.12it/s]8it [00:01,  6.12it/s]                                 macro  0.5321984790429323
micro  0.6316522708275285
[[250   0  76  33  51  12  45]
 [ 20   0  13   6  13   1   3]
 [ 50   0 248  25  72  37  64]
 [ 26   0  31 765  24  12  37]
 [ 67   0 112  42 296   9 127]
 [ 10   0  40  20   8 322  15]
 [ 41   0  48  61  66   5 386]]
              precision    recall  f1-score   support

           0       0.54      0.54      0.54       467
           1       0.00      0.00      0.00        56
           2       0.44      0.50      0.47       496
           3       0.80      0.85      0.83       895
           4       0.56      0.45      0.50       653
           5       0.81      0.78      0.79       415
           6       0.57      0.64      0.60       607

    accuracy                           0.63      3589
   macro avg       0.53      0.54      0.53      3589
weighted avg       0.62      0.63      0.63      3589

correct 0.6316522708275285 
f1 0.5321984790429323 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.84it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.59it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.60it/s] 86%|████████▌ | 6/7.009765625 [00:01<00:00,  6.33it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  6.61it/s]8it [00:01,  5.74it/s]                                 macro  0.5434305934923449
micro  0.6458623572025634
[[272   0  84  31  56   7  41]
 [ 18   0  21   5   6   2   3]
 [ 49   0 286  26  84  36  47]
 [ 18   0  25 781  19  14  22]
 [ 46   0 104  55 265   7 117]
 [ 16   0  45  21  12 306  16]
 [ 39   0  48  51  72   8 408]]
              precision    recall  f1-score   support

           0       0.59      0.55      0.57       491
           1       0.00      0.00      0.00        55
           2       0.47      0.54      0.50       528
           3       0.81      0.89      0.84       879
           4       0.52      0.45      0.48       594
           5       0.81      0.74      0.77       416
           6       0.62      0.65      0.64       626

    accuracy                           0.65      3589
   macro avg       0.54      0.55      0.54      3589
weighted avg       0.63      0.65      0.64      3589

correct 0.6458623572025634 
f1 0.5434305934923449 

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/56.072265625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
  2%|▏         | 1/56.072265625 [00:00<00:37,  1.45it/s]  4%|▎         | 2/56.072265625 [00:00<00:22,  2.39it/s]  5%|▌         | 3/56.072265625 [00:01<00:18,  2.86it/s]  7%|▋         | 4/56.072265625 [00:01<00:15,  3.31it/s]  9%|▉         | 5/56.072265625 [00:01<00:14,  3.49it/s] 11%|█         | 6/56.072265625 [00:01<00:13,  3.74it/s] 12%|█▏        | 7/56.072265625 [00:02<00:12,  3.79it/s] 14%|█▍        | 8/56.072265625 [00:02<00:12,  3.90it/s] 16%|█▌        | 9/56.072265625 [00:02<00:12,  3.82it/s] 18%|█▊        | 10/56.072265625 [00:02<00:11,  3.91it/s] 20%|█▉        | 11/56.072265625 [00:03<00:11,  3.84it/s] 21%|██▏       | 12/56.072265625 [00:03<00:11,  3.74it/s] 23%|██▎       | 13/56.072265625 [00:03<00:11,  3.79it/s] 25%|██▍       | 14/56.072265625 [00:03<00:10,  3.94it/s] 27%|██▋       | 15/56.072265625 [00:04<00:10,  3.93it/s] 29%|██▊       | 16/56.072265625 [00:04<00:09,  4.04it/s] 30%|███       | 17/56.072265625 [00:04<00:09,  4.01it/s] 32%|███▏      | 18/56.072265625 [00:04<00:09,  4.10it/s] 34%|███▍      | 19/56.072265625 [00:05<00:09,  4.07it/s] 36%|███▌      | 20/56.072265625 [00:05<00:08,  4.08it/s] 37%|███▋      | 21/56.072265625 [00:05<00:08,  3.98it/s] 39%|███▉      | 22/56.072265625 [00:05<00:08,  4.02it/s] 41%|████      | 23/56.072265625 [00:06<00:08,  3.92it/s] 43%|████▎     | 24/56.072265625 [00:06<00:08,  3.98it/s] 45%|████▍     | 25/56.072265625 [00:06<00:07,  3.91it/s] 46%|████▋     | 26/56.072265625 [00:06<00:07,  4.02it/s] 48%|████▊     | 27/56.072265625 [00:07<00:07,  3.98it/s] 50%|████▉     | 28/56.072265625 [00:07<00:06,  4.07it/s] 52%|█████▏    | 29/56.072265625 [00:07<00:06,  4.01it/s] 54%|█████▎    | 30/56.072265625 [00:07<00:06,  4.10it/s] 55%|█████▌    | 31/56.072265625 [00:08<00:06,  4.04it/s] 57%|█████▋    | 32/56.072265625 [00:08<00:05,  4.12it/s] 59%|█████▉    | 33/56.072265625 [00:08<00:05,  4.07it/s] 61%|██████    | 34/56.072265625 [00:08<00:05,  4.14it/s] 62%|██████▏   | 35/56.072265625 [00:09<00:05,  4.07it/s] 64%|██████▍   | 36/56.072265625 [00:09<00:04,  4.14it/s] 66%|██████▌   | 37/56.072265625 [00:09<00:04,  4.06it/s] 68%|██████▊   | 38/56.072265625 [00:09<00:04,  4.13it/s] 70%|██████▉   | 39/56.072265625 [00:10<00:04,  4.05it/s] 71%|███████▏  | 40/56.072265625 [00:10<00:03,  4.07it/s] 73%|███████▎  | 41/56.072265625 [00:10<00:03,  3.97it/s] 75%|███████▍  | 42/56.072265625 [00:10<00:03,  4.01it/s] 77%|███████▋  | 43/56.072265625 [00:11<00:03,  3.91it/s] 78%|███████▊  | 44/56.072265625 [00:11<00:03,  3.97it/s] 80%|████████  | 45/56.072265625 [00:11<00:02,  3.91it/s] 82%|████████▏ | 46/56.072265625 [00:11<00:02,  4.02it/s] 84%|████████▍ | 47/56.072265625 [00:12<00:02,  3.99it/s] 86%|████████▌ | 48/56.072265625 [00:12<00:01,  4.08it/s] 87%|████████▋ | 49/56.072265625 [00:12<00:01,  4.03it/s] 89%|████████▉ | 50/56.072265625 [00:12<00:01,  4.12it/s] 91%|█████████ | 51/56.072265625 [00:13<00:01,  4.07it/s] 93%|█████████▎| 52/56.072265625 [00:13<00:00,  4.14it/s] 95%|█████████▍| 53/56.072265625 [00:13<00:00,  4.08it/s] 96%|█████████▋| 54/56.072265625 [00:13<00:00,  4.16it/s] 98%|█████████▊| 55/56.072265625 [00:14<00:00,  4.09it/s]100%|█████████▉| 56/56.072265625 [00:14<00:00,  4.16it/s]57it [00:14,  5.01it/s]                                  57it [00:14,  3.93it/s]epoch 99 iter 0 sum correct 0.900390625 loss 1.885576605796814
epoch 99 iter 1 sum correct 0.9072265625 loss 1.8851468563079834
epoch 99 iter 2 sum correct 0.908203125 loss 1.8851358890533447
epoch 99 iter 3 sum correct 0.9111328125 loss 1.8848600387573242
epoch 99 iter 4 sum correct 0.910546875 loss 1.8848904371261597
epoch 99 iter 5 sum correct 0.9065755208333334 loss 1.8852299451828003
epoch 99 iter 6 sum correct 0.9065290178571429 loss 1.8852134943008423
epoch 99 iter 7 sum correct 0.90673828125 loss 1.8852077722549438
epoch 99 iter 8 sum correct 0.9099392361111112 loss 1.884971261024475
epoch 99 iter 9 sum correct 0.9107421875 loss 1.8849296569824219
epoch 99 iter 10 sum correct 0.9113991477272727 loss 1.884877324104309
epoch 99 iter 11 sum correct 0.91259765625 loss 1.8847888708114624
epoch 99 iter 12 sum correct 0.9124098557692307 loss 1.8848191499710083
epoch 99 iter 13 sum correct 0.9128069196428571 loss 1.8847793340682983
epoch 99 iter 14 sum correct 0.9123697916666667 loss 1.8848178386688232
epoch 99 iter 15 sum correct 0.9122314453125 loss 1.8848367929458618
epoch 99 iter 16 sum correct 0.912109375 loss 1.8848506212234497
epoch 99 iter 17 sum correct 0.9112413194444444 loss 1.8849198818206787
epoch 99 iter 18 sum correct 0.911389802631579 loss 1.884912371635437
epoch 99 iter 19 sum correct 0.912890625 loss 1.884796380996704
epoch 99 iter 20 sum correct 0.9131324404761905 loss 1.884782314300537
epoch 99 iter 21 sum correct 0.9135298295454546 loss 1.884753942489624
epoch 99 iter 22 sum correct 0.9137228260869565 loss 1.8847413063049316
epoch 99 iter 23 sum correct 0.9131673177083334 loss 1.8847856521606445
epoch 99 iter 24 sum correct 0.912578125 loss 1.8848302364349365
epoch 99 iter 25 sum correct 0.9120342548076923 loss 1.8848706483840942
epoch 99 iter 26 sum correct 0.9125434027777778 loss 1.8848260641098022
epoch 99 iter 27 sum correct 0.9127371651785714 loss 1.8848118782043457
epoch 99 iter 28 sum correct 0.912042025862069 loss 1.8848648071289062
epoch 99 iter 29 sum correct 0.912109375 loss 1.8848594427108765
epoch 99 iter 30 sum correct 0.9118573588709677 loss 1.884879231452942
epoch 99 iter 31 sum correct 0.91229248046875 loss 1.884848952293396
epoch 99 iter 32 sum correct 0.9116358901515151 loss 1.8848997354507446
epoch 99 iter 33 sum correct 0.9118795955882353 loss 1.8848803043365479
epoch 99 iter 34 sum correct 0.9118861607142857 loss 1.8848779201507568
epoch 99 iter 35 sum correct 0.9119466145833334 loss 1.8848748207092285
epoch 99 iter 36 sum correct 0.911792652027027 loss 1.8848881721496582
epoch 99 iter 37 sum correct 0.9111328125 loss 1.88493812084198
epoch 99 iter 38 sum correct 0.9111077724358975 loss 1.884936809539795
epoch 99 iter 39 sum correct 0.91162109375 loss 1.8848990201950073
epoch 99 iter 40 sum correct 0.9115377286585366 loss 1.8849073648452759
epoch 99 iter 41 sum correct 0.9109468005952381 loss 1.884955883026123
epoch 99 iter 42 sum correct 0.910655886627907 loss 1.8849773406982422
epoch 99 iter 43 sum correct 0.9102450284090909 loss 1.8850091695785522
epoch 99 iter 44 sum correct 0.9104166666666667 loss 1.8849942684173584
epoch 99 iter 45 sum correct 0.9107931385869565 loss 1.8849653005599976
epoch 99 iter 46 sum correct 0.9103640292553191 loss 1.8849999904632568
epoch 99 iter 47 sum correct 0.9101969401041666 loss 1.885009765625
epoch 99 iter 48 sum correct 0.9101163903061225 loss 1.8850185871124268
epoch 99 iter 49 sum correct 0.910546875 loss 1.8849858045578003
epoch 99 iter 50 sum correct 0.9104626225490197 loss 1.88499116897583
epoch 99 iter 51 sum correct 0.9104942908653846 loss 1.8849904537200928
epoch 99 iter 52 sum correct 0.9104510613207547 loss 1.8849928379058838
epoch 99 iter 53 sum correct 0.9103370949074074 loss 1.8849968910217285
epoch 99 iter 54 sum correct 0.91015625 loss 1.8850117921829224
epoch 99 iter 55 sum correct 0.9105747767857143 loss 1.8849778175354004
epoch 99 iter 56 sum correct 0.895764802631579 loss 1.8849815130233765
=================================val======================================

  0%|          | 0/7.009765625 [00:00<?, ?it/s] 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.70it/s] 29%|██▊       | 2/7.009765625 [00:00<00:01,  3.30it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.58it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  6.32it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.48it/s]8it [00:01,  5.94it/s]                                 macro  0.530826812396602
micro  0.6319308999721371
[[264   0  37  28  50  15  73]
 [ 19   0  10   4  14   0   9]
 [ 62   0 205  14  97  44  74]
 [ 26   0  15 748  21  20  65]
 [ 77   0  68  36 298  12 162]
 [ 14   0  21  16   8 338  18]
 [ 51   0  22  40  70   9 415]]
              precision    recall  f1-score   support

           0       0.51      0.57      0.54       467
           1       0.00      0.00      0.00        56
           2       0.54      0.41      0.47       496
           3       0.84      0.84      0.84       895
           4       0.53      0.46      0.49       653
           5       0.77      0.81      0.79       415
           6       0.51      0.68      0.58       607

    accuracy                           0.63      3589
   macro avg       0.53      0.54      0.53      3589
weighted avg       0.62      0.63      0.62      3589

correct 0.6319308999721371 
f1 0.530826812396602 
=================================test======================================

/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
  0%|          | 0/7.009765625 [00:00<?, ?it/s]/home/ubuntu/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/home/ubuntu/caolei/FER/models/FineTuneModel.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  result = F.softmax(output) #(batch,7)
/home/ubuntu/caolei/FER/models/FineTuneModel.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.
  return F.softmax(output)
 14%|█▍        | 1/7.009765625 [00:00<00:03,  1.72it/s] 43%|████▎     | 3/7.009765625 [00:00<00:00,  4.38it/s] 71%|███████▏  | 5/7.009765625 [00:01<00:00,  5.91it/s]100%|█████████▉| 7/7.009765625 [00:01<00:00,  7.22it/s]8it [00:01,  6.12it/s]                                 
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, msg_start, len(result))
macro  0.5413497016855798
micro  0.6450264697687378
[[294   0  42  20  56  13  66]
 [ 30   0  10   4   6   1   4]
 [ 57   0 238  23  87  48  75]
 [ 22   0  13 747  24  22  51]
 [ 69   0  51  42 262  13 157]
 [ 11   0  29  22  11 327  16]
 [ 45   0  28  28  61  17 447]]
              precision    recall  f1-score   support

           0       0.56      0.60      0.58       491
           1       0.00      0.00      0.00        55
           2       0.58      0.45      0.51       528
           3       0.84      0.85      0.85       879
           4       0.52      0.44      0.48       594
           5       0.74      0.79      0.76       416
           6       0.55      0.71      0.62       626

    accuracy                           0.65      3589
   macro avg       0.54      0.55      0.54      3589
weighted avg       0.63      0.65      0.64      3589

correct 0.6450264697687378 
f1 0.5413497016855798 
End
